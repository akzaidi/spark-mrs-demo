# (PART) Provisioning and Ingesting Data {-}

# Provisioning Instructions

This module provides a walkthrough of how to provision a Spark cluster on Azure HDInsight Premium with Microsoft R Server, and how to add an edge node with RStudio Server.

## Provision Cluster from Azure Portal

The [Azure documentation](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hadoop-r-server-overview/) page provides details on how to provision a Spark cluster with Microsoft R Server.

The first steps are outlined here: [Get started using R Server on HDInsight (preview)](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hadoop-r-server-get-started/)

I have summarized the steps here to help you get started quickly:

* Login to [portal.azure.com](ms.portal.azure.com) with your Azure subscription
* New -> Data + Analytics -> HDInsight
* Choose Premium cluster: R Server on Spark
* Create an sshkey, using putty or openSSH, and include the public key in the credentials tab
* [Install RStudio Server on the Edge Node](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hadoop-r-server-install-r-studio/)
* Tunnel into your RStudio Server instance, and start your ML pipeline!

## Installing Packages

For packages you only need to run on the edge node, you can continue using `install.packages`. For packages you need installed on the edge node as well as all the worker nodes, you'll need to use a [script action](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hadoop-r-server-get-started/#install-r-packages)

### todo - install packages demo

# Ingesting Data into Azure Blob Storage - todo {#ingestion}

Azure HDInsight utilizes low-cost Blob storage as it's data store. Through a Hadoop distributed file system (HDFS) interface, the full set of components in HDInsight can operate directly on structured or unstructured data in Blob storage. The storage accounts containing your blob storage is separated from your compute environment, allowing you to delete your HDInsight cluster for computation without losing your data, or pointing multiple compute systems to the same data store.

## AzCopy

## Azure Storage Explorer

# Setting Your R Profile

When developing on the RStudio server instance on your Spark HDInsight Cluster, it might be useful to configure your profile so that your R environment can find the SparkR library. This can save some tedious operations that can easily be missed, or mistyped.

Here is an example *.Rprofile*, specifying the location of the `SparkR` library. If you prefer not to load the SparkR library by default, or change your user `Rprofile`, you can load the package directly from it's directory before conducting your analysis. Details of doing this are provided in the following chapter.

```{r, eval = FALSE}

.First <- function() {
  
  .libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"), .libPaths()))
  pkgs_to_load <- c(getOption("defaultPackages"), "SparkR")
  options(defaultPackages = pkgs_to_load)

}

.Last <- function() {
    if (interactive()) {
        hist_file <- Sys.getenv("R_HISTFILE")
        if (hist_file == "")
            hist_file <- "~/.RHistory"
        savehistory(hist_file)
    }
}


```


For more information about your user Rprofile, see the R documentation on startup configurations: `help(Startup)`. 
