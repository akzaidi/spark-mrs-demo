# Spark Backend for dplyr

The last section described the limitations with the `SparkR` API and provided some solutions through the `SparkRext` package. The team at RStudio has developed an alternative and more comprehensive solution for accessing Spark through an R API. This chapter showcases these extensions. For more information you should visit the documentation at the [RStudio Spark Homepage](http://spark.rstudio.com/)

## sparkrapi

Underpinning the Spark API developed by RStudio is the package [sparkapi](github.com/rstudio/sparkapi), which provides access to the SQLContext and HiveContext needed for creating DataFrames, but also provides access to calling the full Spark Scala API. By providing a low-level API like this, it is possible to create [extensions](http://spark.rstudio.com/extensions.html) that access parts of the Spark codebase that are not currently exposed to the R API. As an example extension, the RStudio team immediately wrote a package called [sparklyr](github.com/rstudio/sparklyr) which provides an _actual_ backend for `dplyr`. 

### A Proper `dplyr` backend for Spark DataFrames

While `SparkR` attempted to mimick the `dplyr` syntax, the actual functions exported by the package masked the `dplyr` functions, making it impossible to interoperate between the two packages (loading one would cause calls from the second package to fail). While the `SparkRext` attempted to resolve this issue by properly defining classes and methods for the `dplyr` functions that interacted specifically with Spark DataFrames, the `sparklyr` package took the process one step further (lower?). Since `dplyr` supports [SQL backends](https://cran.r-project.org/web/packages/dplyr/vignettes/new-sql-backend.html) as a data source, and since Spark DataFrames use Spark SQL to create and query its DataFrames, `sparklyr` allows complete access to `dplyr` by invoking SQL commands directly to `src_sql` in `dplyr`. 

In order to get started with the `sparklyr` package, we need to create a Spark connection. We specify the connection directly through `sparklyr`, by running `spark_connect` and specifying our cluster's URL. Since we are using a HDInsight Spark cluster, our cluster is managed through YARN.

```{r sparkapi, eval = FALSE}
library(sparklyr)
sc <- spark_connect("yarn-client")

```

Using the `sparklyr` package, importing data immediately an object of class `tbl_spark`, which inherits from the more general `dplyr` class, `tbl_sql` and `tbl` (see the [`tibble`](github.com/hadley/tibble) for more information about the `tibble` format), and allows access to all the functions exported by the `dplyr` package with methods for the `tbl_sql` class.

In this section, we will examine the data from [Fannie Mae Single Family Loan Performance Data](http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html). Our data is located in an Azure Storage Account, and is delimited by a pipe and does not have a header. The function `spark_read_csv` simply invokes the `read.df` function we had used earlier, so you can think of it is a wrapper function. It is useful mainly for the additional attributes it defines on it's return value: the `tbl_spark` class.

```{r renamefannie, eval = FALSE}

fannie <- spark_read_csv(sc, name = "fannie", 
                         path = "wasb://mrs-spark@alizaidi.blob.core.windows.net/user/RevoShare/alizaidi/Acquisition/",
                         header = F,
                         delimiter = "|")

class(fannie)

library(dplyr)

fannie <- fannie %>% rename(loan_id = C0,
                            orig_chn = C1,
                            seller_name = C2,
                            orig_rt = C3,
                            orig_amt = C4,
                            orig_trm = C5,
                            orig_dte = C6,
                            frst_dte = C7,
                            oltv = C8,
                            ocltv = C9,
                            num_bo = C10,
                            dti = C11,
                            cscore_b = C12,
                            fthb_flg = C13,
                            purpose = C14,
                            prop_typ = C15,
                            num_unit  = C16,
                            occ_stat  = C17,
                            state  = C18,
                            zip_3 = C19,
                            mi_pct  = C20,
                            product_type = C21,
                            cscore_co  = C22)

fannie %>% head

```

### Aggregating Data


Now that we have a proper `tbl` class that works with the `dplyr` package, any function we write that is desgined to work with `dplyr` will automatically work here as well. The only caveat is that not all methods available for the `tbl_df` class are available for the `tbl_spark` or `tbl_sql` class. For example, you can't use arbitrary R functions with `tbl_sql`, just those can be easily understood by SQL.


```{r agg-zip}

fannie <- fannie %>% mutate(orig_year = substr(orig_dte, 4, 7))

zip_summary <- fannie %>% 
  group_by(zip_3, orig_year) %>% 
  summarize(ave_amnt = mean(orig_amt), ave_ltv = mean(oltv), 
            ave_dti = mean(dti), ave_cscore = mean(cscore_b))

zip_summary %>% head
```

