# Spark Backend for dplyr

The last section described the limitations with the `SparkR` API and provided some solutions through the `SparkRext` package. The team at RStudio has developed an alternative and more comprehensive solution for accessing Spark through an R API. This chapter showcases these extensions. For more information you should visit the documentation at the [RStudio Spark Homepage](http://spark.rstudio.com/)

## sparkrapi

Underpinning the Spark API developed by RStudio is the package [sparkapi](github.com/rstudio/sparkapi), which provides access to the SQLContext and HiveContext needed for creating DataFrames, but also provides access to calling the full Spark Scala API. By providing a low-level API like this, it is possible to create [extensions](http://spark.rstudio.com/extensions.html) that access parts of the Spark codebase that are not currently exposed to the R API. As an example extension, the RStudio team immediately wrote a package called [sparklyr](github.com/rstudio/sparklyr) which provides an _actual_ backend for `dplyr`. 

### A Proper `dplyr` backend for Spark DataFrames

While `SparkR` attempted to mimick the `dplyr` syntax, the actual functions exported by the package masked the `dplyr` functions, making it impossible to interoperate between the two packages (loading one would cause calls from the second package to fail). While the `SparkRext` attempted to resolve this issue by properly defining classes and methods for the `dplyr` functions that interacted specifically with Spark DataFrames, the `sparklyr` package took the process one step further (lower?). Since `dplyr` supports [SQL backends](https://cran.r-project.org/web/packages/dplyr/vignettes/new-sql-backend.html) as a data source, and since Spark DataFrames use Spark SQL to create and query its DataFrames, `sparklyr` allows complete access to `dplyr` by invoking SQL commands directly to `src_sql` in `dplyr`. 

In order to get started with the `sparklyr` package, we need to create a Spark connection. We specify the connection directly through `sparklyr`, by running `spark_connect` and specifying our cluster's URL. Since we are using a HDInsight Spark cluster, our cluster is managed through YARN.

```{r sparkapi, eval = FALSE}
library(sparklyr)
sc <- spark_connect("yarn-client")

```

Using the `sparklyr` package, importing data immediately an object of class `tbl_spark`, which inherits from the more general `dplyr` class, `tbl_sql` and `tbl` (see the [`tibble`](github.com/hadley/tibble) for more information about the `tibble` format), and allows access to all the functions exported by the `dplyr` package with methods for the `tbl_sql` class.

In this section, we will examine the data from [Fannie Mae Single Family Loan Performance Data](http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html). Our data is located in an Azure Storage Account, and is delimited by a pipe and does not have a header. The function `spark_read_csv` simply invokes the `read.df` function we had used earlier, so you can think of it is a wrapper function. It is useful mainly for the additional attributes it defines on it's return value: the `tbl_spark` class.

```{r renamefannie, eval = FALSE}

fannie <- spark_read_csv(sc, name = "fannie", 
                         path = "wasb://mrs-spark@alizaidi.blob.core.windows.net/user/RevoShare/alizaidi/Acquisition/",
                         header = F,
                         delimiter = "|")

class(fannie)
head(fannie)

library(dplyr)

fannie <- fannie %>% rename(loan_id = V1,
                            orig_chn = V2,
                            seller_name = V3,
                            orig_rt = V4,
                            orig_amt = V5,
                            orig_trm = V6,
                            orig_dte = V7,
                            frst_dte = V8,
                            oltv = V9,
                            ocltv = V10,
                            num_bo = V11,
                            dti = V12,
                            cscore_b = V13,
                            fthb_flg = V14,
                            purpose = V15,
                            prop_typ = V16,
                            num_unit  = V17,
                            occ_stat  = V18,
                            state  = V19,
                            zip_3 = V20,
                            mi_pct  = V21,
                            product_type = V22,
                            cscore_co  = V23)

fannie %>% head
fannie %>% count



```

### Importing Performance Data

Importing the entire performance data seems to lead to storage overfill.

```{r perf_import, eval = FALSE}

fannie_perf <- spark_read_csv(sc, name = "performance",
                              path = "wasb://mrs-spark@alizaidi.blob.core.windows.net/user/RevoShare/alizaidi/Performance/",
                              header = F,
                              delimiter = "|")

fannie_perf <- fannie_perf %>% rename(loan_id = C0,
                    monthly_rpd_prd  = C1,
                    servicer  = C2,
                    last_rt  = C3,
                    last_upb  = C4,
                    loan_age  = C5,
                    months_to_maturity = C6,
                    adj_months_to_maturity  = C7,
                    maturity_date  = C8,
                    msa  = C9,
                    deliq_status  = C10,
                    mod_flag  = C11,
                    zero_bal_code = C12,
                    zb_date  = C13,
                    lpi_date  = C14,
                    fcc_dte = C15,
                    disp_dt  = C16,
                    fcc_cost  = C17,
                    pp_cost  = C18,
                    ar_cost  = C19,
                    ie_cost  = C20,
                    tax_cost  = C21,
                    ns_procs = C22,
                    ce_procs  = C23,
                    rmw_procs  = C24,
                    o_procs  = C25,
                    non_int_upb  = C26,
                    prin_forg_upb  = C27)

```


### Aggregating Data


Now that we have a proper `tbl` class that works with the `dplyr` package, any function we write that is desgined to work with `dplyr` will automatically work here as well. The only caveat is that not all methods available for the `tbl_df` class are available for the `tbl_spark` or `tbl_sql` class. For example, you can't use arbitrary R functions with `tbl_sql`, just those can be easily understood by SQL.


```{r agg-zip, eval = FALSE}

fannie <- fannie %>% mutate(orig_year = substr(orig_dte, 4, 7))

zip_summary <- fannie %>% 
  group_by(zip_3, orig_year) %>% 
  summarize(ave_amnt = mean(orig_amt), ave_ltv = mean(oltv), 
            ave_dti = mean(dti), ave_cscore = mean(cscore_b))

zip_summary %>% head
zip_summary %>% count
```

### Plotting Results 

#### By State

```{r choropleth, eval = FALSE}

state_summary <- fannie %>% 
  group_by(state, orig_year) %>% 
  summarize(ave_amnt = mean(orig_amt), ave_ltv = mean(oltv), 
            ave_dti = mean(dti), ave_cscore = mean(cscore_b))

state_summary %>% collect %>% 
  mutate(year = as.numeric(orig_year)) %>% 
    rMaps::ichoropleth(ave_cscore ~ state, data = ., 
                     animate = "year",
                     geographyConfig = list(popupTemplate = "#!function(geo, data) {
                                         return '<div class=\"hoverinfo\"><strong>'+
                                         data.state+ '<br>' + 'Average Credit Score for Loans Originated in '+
                                         data.year + ': ' +
                                         data.ave_cscore.toFixed(0) +
                                         '</strong></div>';}!#")) -> state_fico

state_fico

```

