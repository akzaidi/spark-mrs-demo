<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Scalable Machine Learning and Data Science with R and Spark</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This book is a guide for doing data analysis with Spark and R. It is organized as a collection of examples of data science problems analyzed with these two technologies, and tries to highlight programming paradigms that make the best of both worlds. It starts with a discussion of using R for data science and machine learning, and describes how to create functional programming pipelines that are data source invariant, easy to write, easy to debug, and easy to deploy! In order to scale out typical R workflows, we showcase how to use Spark and Microsoft R Server to bring your favorite R functions to large datasets.">
  <meta name="generator" content="bookdown 0.0.79 and GitBook 2.6.7">

  <meta property="og:title" content="Scalable Machine Learning and Data Science with R and Spark" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/scalemlcover.png" />
  <meta property="og:description" content="This book is a guide for doing data analysis with Spark and R. It is organized as a collection of examples of data science problems analyzed with these two technologies, and tries to highlight programming paradigms that make the best of both worlds. It starts with a discussion of using R for data science and machine learning, and describes how to create functional programming pipelines that are data source invariant, easy to write, easy to debug, and easy to deploy! In order to scale out typical R workflows, we showcase how to use Spark and Microsoft R Server to bring your favorite R functions to large datasets." />
  <meta name="github-repo" content="akzaidi/spark-mrs-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Scalable Machine Learning and Data Science with R and Spark" />
  
  <meta name="twitter:description" content="This book is a guide for doing data analysis with Spark and R. It is organized as a collection of examples of data science problems analyzed with these two technologies, and tries to highlight programming paradigms that make the best of both worlds. It starts with a discussion of using R for data science and machine learning, and describes how to create functional programming pipelines that are data source invariant, easy to write, easy to debug, and easy to deploy! In order to scale out typical R workflows, we showcase how to use Spark and Microsoft R Server to bring your favorite R functions to large datasets." />
  <meta name="twitter:image" content="images/scalemlcover.png" />

<meta name="author" content="Ali Zaidi">

<meta name="date" content="2016-07-10">


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="rprofile.html">
<link rel="next" href="exploratory-data-analysis-with-sparkr.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.6/htmlwidgets.js"></script>
<link href="libs/plotlyjs-1.10.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.10.1/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-3.6.5/plotly.js"></script>
<script src="libs/d3-3.5.5/d3.min.js"></script>
<script src="libs/d3-grid-0.1.0/d3-grid.js"></script>
<script src="libs/dimple-2.1.6/dimple.min.js"></script>
<script src="libs/dimple-binding-0.1/dimple.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#scalable-machine-learning-and-data-science"><i class="fa fa-check"></i>Scalable Machine Learning and Data Science</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#useful-resources"><i class="fa fa-check"></i>Useful Resources</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#spark"><i class="fa fa-check"></i>Spark</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#microsoft-r-server"><i class="fa fa-check"></i>Microsoft R Server</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#azure-hdinsight"><i class="fa fa-check"></i>Azure HDInsight</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Overview</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#why-r"><i class="fa fa-check"></i><b>1.1</b> Why R?</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#microsoft-r-server-ftw"><i class="fa fa-check"></i><b>1.2</b> Microsoft R Server FTW</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#apache-spark"><i class="fa fa-check"></i><b>1.3</b> Apache Spark</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#sparkr"><i class="fa fa-check"></i><b>1.4</b> SparkR</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#azure-hdinsight-1"><i class="fa fa-check"></i><b>1.5</b> Azure HDInsight</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#prerequisites---what-youll-need"><i class="fa fa-check"></i><b>1.6</b> Prerequisites - What You’ll Need</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#data-used"><i class="fa fa-check"></i><b>1.7</b> Data Used</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#versioning"><i class="fa fa-check"></i><b>1.8</b> Versioning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="apache-spark-1.html"><a href="apache-spark-1.html"><i class="fa fa-check"></i><b>2</b> Apache Spark</a><ul>
<li class="chapter" data-level="2.1" data-path="apache-spark-1.html"><a href="apache-spark-1.html#lazyevalfp"><i class="fa fa-check"></i><b>2.1</b> Functional Programming and Lazy Evaluation</a></li>
<li class="chapter" data-level="2.2" data-path="apache-spark-1.html"><a href="apache-spark-1.html#distributed-programming-abstractions"><i class="fa fa-check"></i><b>2.2</b> Distributed Programming Abstractions</a></li>
<li class="chapter" data-level="2.3" data-path="apache-spark-1.html"><a href="apache-spark-1.html#rdds"><i class="fa fa-check"></i><b>2.3</b> RDDs</a><ul>
<li class="chapter" data-level="2.3.1" data-path="apache-spark-1.html"><a href="apache-spark-1.html#common-transformations-and-actions"><i class="fa fa-check"></i><b>2.3.1</b> Common Transformations and Actions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="apache-spark-1.html"><a href="apache-spark-1.html#dataframes"><i class="fa fa-check"></i><b>2.4</b> DataFrames</a></li>
<li class="chapter" data-level="2.5" data-path="apache-spark-1.html"><a href="apache-spark-1.html#datasets"><i class="fa fa-check"></i><b>2.5</b> Datasets</a></li>
<li class="chapter" data-level="2.6" data-path="apache-spark-1.html"><a href="apache-spark-1.html#mllib"><i class="fa fa-check"></i><b>2.6</b> MLlib</a></li>
<li class="chapter" data-level="2.7" data-path="apache-spark-1.html"><a href="apache-spark-1.html#spark-apis"><i class="fa fa-check"></i><b>2.7</b> Spark APIs</a><ul>
<li class="chapter" data-level="2.7.1" data-path="apache-spark-1.html"><a href="apache-spark-1.html#scala"><i class="fa fa-check"></i><b>2.7.1</b> Scala</a></li>
<li class="chapter" data-level="2.7.2" data-path="apache-spark-1.html"><a href="apache-spark-1.html#pyspark"><i class="fa fa-check"></i><b>2.7.2</b> <a href="https://spark.apache.org/docs/latest/api/python/index.html">PySpark</a></a></li>
<li class="chapter" data-level="2.7.3" data-path="apache-spark-1.html"><a href="apache-spark-1.html#sparkr-1"><i class="fa fa-check"></i><b>2.7.3</b> SparkR</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="rserver.html"><a href="rserver.html"><i class="fa fa-check"></i><b>3</b> R &amp; Microsoft R Server - todo</a><ul>
<li class="chapter" data-level="3.1" data-path="rserver.html"><a href="rserver.html#functional-programming-and-lazy-evaluation-in-r"><i class="fa fa-check"></i><b>3.1</b> Functional Programming and Lazy Evaluation in R</a></li>
<li class="chapter" data-level="3.2" data-path="rserver.html"><a href="rserver.html#pema"><i class="fa fa-check"></i><b>3.2</b> PEMA Algorithms and the RevoScaleR Package</a></li>
<li class="chapter" data-level="3.3" data-path="rserver.html"><a href="rserver.html#xdf"><i class="fa fa-check"></i><b>3.3</b> eXternal Data Frames (XDFs)</a></li>
<li class="chapter" data-level="3.4" data-path="rserver.html"><a href="rserver.html#computecontext"><i class="fa fa-check"></i><b>3.4</b> Compute Contexts</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><i class="fa fa-check"></i><b>4</b> Azure HDInsight – Managed Hadoop in the Cloud - todo</a><ul>
<li class="chapter" data-level="4.1" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html#hdinsight-premium-spark-clusters-with-r-server"><i class="fa fa-check"></i><b>4.1</b> HDInsight Premium Spark Clusters with R Server</a></li>
<li class="chapter" data-level="4.2" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html#dashboards-for-management"><i class="fa fa-check"></i><b>4.2</b> Dashboards for Management</a></li>
<li class="chapter" data-level="4.3" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html#jupyter-and-rstudio-server"><i class="fa fa-check"></i><b>4.3</b> Jupyter and RStudio Server</a></li>
</ul></li>
<li class="part"><span><b>Provisioning and Ingesting Data</b></span></li>
<li class="chapter" data-level="5" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html"><i class="fa fa-check"></i><b>5</b> Provisioning Instructions</a><ul>
<li class="chapter" data-level="5.1" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html#provision-cluster-from-azure-portal"><i class="fa fa-check"></i><b>5.1</b> Provision Cluster from Azure Portal</a></li>
<li class="chapter" data-level="5.2" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html#installing-packages"><i class="fa fa-check"></i><b>5.2</b> Installing Packages</a><ul>
<li class="chapter" data-level="5.2.1" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html#todo---install-packages-demo"><i class="fa fa-check"></i><b>5.2.1</b> todo - install packages demo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ingestion.html"><a href="ingestion.html"><i class="fa fa-check"></i><b>6</b> Ingesting Data into Azure Blob Storage - todo</a><ul>
<li class="chapter" data-level="6.1" data-path="ingestion.html"><a href="ingestion.html#azcopy"><i class="fa fa-check"></i><b>6.1</b> AzCopy</a></li>
<li class="chapter" data-level="6.2" data-path="ingestion.html"><a href="ingestion.html#azure-storage-explorer"><i class="fa fa-check"></i><b>6.2</b> Azure Storage Explorer</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="rprofile.html"><a href="rprofile.html"><i class="fa fa-check"></i><b>7</b> Setting Your R Profile</a></li>
<li class="part"><span><b>Data Manipulation and Data Aggregation</b></span></li>
<li class="chapter" data-level="8" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html"><i class="fa fa-check"></i><b>8</b> Starting Your Machine Learning Pipeline</a><ul>
<li class="chapter" data-level="8.1" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#finding-the-sparkr-library"><i class="fa fa-check"></i><b>8.1</b> Finding the SparkR Library</a></li>
<li class="chapter" data-level="8.2" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#creating-a-spark-context"><i class="fa fa-check"></i><b>8.2</b> Creating a Spark Context</a></li>
<li class="chapter" data-level="8.3" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#creating-dataframes"><i class="fa fa-check"></i><b>8.3</b> Creating DataFrames</a><ul>
<li class="chapter" data-level="8.3.1" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#from-local-r-data.frames"><i class="fa fa-check"></i><b>8.3.1</b> From Local R data.frames</a></li>
<li class="chapter" data-level="8.3.2" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#code-reusability-and-non-standard-evaluation"><i class="fa fa-check"></i><b>8.3.2</b> Code Reusability and Non-Standard Evaluation</a></li>
<li class="chapter" data-level="8.3.3" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#creating-dataframes-from-csv-files"><i class="fa fa-check"></i><b>8.3.3</b> Creating DataFrames from CSV Files</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#caching-dataframes"><i class="fa fa-check"></i><b>8.4</b> Caching DataFrames</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html"><i class="fa fa-check"></i><b>9</b> Exploratory Data Analysis with SparkR</a><ul>
<li class="chapter" data-level="9.1" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#sparkr-the-explorer"><i class="fa fa-check"></i><b>9.1</b> SparkR the Explorer</a></li>
<li class="chapter" data-level="9.2" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#doing-data-aggregations-with-sparkr-efficiently"><i class="fa fa-check"></i><b>9.2</b> Doing Data Aggregations with SparkR Efficiently</a></li>
<li class="chapter" data-level="9.3" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#from-spark-dataframes-to-local-dataframes"><i class="fa fa-check"></i><b>9.3</b> From Spark DataFrames to Local Dataframes</a></li>
<li class="chapter" data-level="9.4" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#plotting-results"><i class="fa fa-check"></i><b>9.4</b> Plotting Results</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html"><i class="fa fa-check"></i><b>10</b> Data Manipulation with SparkR</a><ul>
<li class="chapter" data-level="10.1" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#data-aggregations"><i class="fa fa-check"></i><b>10.1</b> Data Aggregations</a></li>
<li class="chapter" data-level="10.2" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#collecting-results-to-local-dataframes"><i class="fa fa-check"></i><b>10.2</b> Collecting Results to Local Dataframes</a></li>
<li class="chapter" data-level="10.3" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#dimple-bar-charts"><i class="fa fa-check"></i><b>10.3</b> Dimple Bar Charts</a></li>
<li class="chapter" data-level="10.4" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#merging-data"><i class="fa fa-check"></i><b>10.4</b> Merging Data</a></li>
<li class="chapter" data-level="10.5" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#exploring-credit-scores-for-mortgage-borrowers"><i class="fa fa-check"></i><b>10.5</b> Exploring Credit Scores for Mortgage Borrowers</a><ul>
<li class="chapter" data-level="10.5.1" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#ingesting-originations-data-into-spark-dataframes"><i class="fa fa-check"></i><b>10.5.1</b> Ingesting Originations Data into Spark DataFrames</a></li>
<li class="chapter" data-level="10.5.2" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#calculting-state-level-credit-attributes"><i class="fa fa-check"></i><b>10.5.2</b> Calculting State Level Credit Attributes</a></li>
<li class="chapter" data-level="10.5.3" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#credit-attribute-choropleths"><i class="fa fa-check"></i><b>10.5.3</b> Credit Attribute Choropleths</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Modeling and Prediction with Microsoft R Server</b></span></li>
<li class="chapter" data-level="11" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html"><i class="fa fa-check"></i><b>11</b> Modeling with Microsoft R Server</a><ul>
<li class="chapter" data-level="11.1" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#import-csv-to-xdf"><i class="fa fa-check"></i><b>11.1</b> Import CSV to XDF</a></li>
<li class="chapter" data-level="11.2" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#splitting-xdf-into-train-and-test-tests"><i class="fa fa-check"></i><b>11.2</b> Splitting XDF into Train and Test Tests</a></li>
<li class="chapter" data-level="11.3" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#training-binary-classification-models"><i class="fa fa-check"></i><b>11.3</b> Training Binary Classification Models</a><ul>
<li class="chapter" data-level="11.3.1" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#logistic-regression-models"><i class="fa fa-check"></i><b>11.3.1</b> Logistic Regression Models</a></li>
<li class="chapter" data-level="11.3.2" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#tree-and-ensemble-classifiers"><i class="fa fa-check"></i><b>11.3.2</b> Tree and Ensemble Classifiers</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#testing-models"><i class="fa fa-check"></i><b>11.4</b> Testing Models</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html"><i class="fa fa-check"></i><b>12</b> Spark Backend for dplyr</a><ul>
<li class="chapter" data-level="12.1" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html#sparkrapi"><i class="fa fa-check"></i><b>12.1</b> sparkrapi</a><ul>
<li class="chapter" data-level="12.1.1" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html#a-proper-dplyr-backend-for-spark-dataframes"><i class="fa fa-check"></i><b>12.1.1</b> A Proper <code>dplyr</code> backend for Spark DataFrames</a></li>
<li class="chapter" data-level="12.1.2" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html#aggregating-data"><i class="fa fa-check"></i><b>12.1.2</b> Aggregating Data</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Scalable Machine Learning and Data Science with R and Spark</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="starting-your-machine-learning-pipeline" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Starting Your Machine Learning Pipeline</h1>
<p>The first steps to start your machine learning and data science pipeline is to set your compute environment, and point to your data.</p>
<p>In Spark, you’ll need to create a SparkContext. This constructor provides Spark with the details of the cluster: how and where to access it, additional package directories, etc. You’ll use this constructor to create new RDDs or DataFrames.</p>
<div id="finding-the-sparkr-library" class="section level2">
<h2><span class="header-section-number">8.1</span> Finding the SparkR Library</h2>
<p>In order to create a Spark Context from your RStudio Server environment, you’ll need to access the SparkR library. Since Spark 1.4, SparkR has shipped the R API directly with its core implementation. Therefore, since 1.4 you do not need to install Spark from CRAN or a development version from github, but you need to add the SparkR library to your library paths in order to access it.</p>
<p>A system variable called “SPARK_HOME” has been set that points to the Spark installation directory, and in it you’ll find subdirectories, “R/lib”, containing the SparkR library. If you prefer, you can add the location of the SparkR package to your library paths, as we did in section <a href="rprofile.html#rprofile">7</a>. If you prefer not edit your Rprofile, you can add the location of the SparkR package to your library paths at the start of your R session.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">list.files</span>(<span class="kw">file.path</span>(<span class="kw">Sys.getenv</span>(<span class="st">&quot;SPARK_HOME&quot;</span>), <span class="st">&quot;R&quot;</span>, <span class="st">&quot;lib&quot;</span>))</code></pre></div>
<pre><code>## [1] &quot;SparkR&quot;     &quot;sparkr.zip&quot;</code></pre>
<p>To add the SparkR library to your library paths, use the <code>.libPaths</code> function to include the directory in the search path for R’s library tree. The library paths could also be changed from in the <code>Rprofile</code>, either for the user or system wide. See the help on <code>?StartUp</code> for more details on R’s startup mechanism.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">.libPaths</span>(<span class="kw">c</span>(<span class="kw">file.path</span>(<span class="kw">Sys.getenv</span>(<span class="st">&quot;SPARK_HOME&quot;</span>), <span class="st">&quot;R&quot;</span>, <span class="st">&quot;lib&quot;</span>), <span class="kw">.libPaths</span>()))</code></pre></div>
</div>
<div id="creating-a-spark-context" class="section level2">
<h2><span class="header-section-number">8.2</span> Creating a Spark Context</h2>
<p>To create a SparkContext, you should use the <code>spark.init</code> function and pass in options for the environment parameters, application properties, any spark packages depended on, and any other additional Spark parameters. In order to create and manipulate DataFrames, we will need a SQLContext or the HiveContext, which can be created from the SparkContext using the <code>sparkRSQL.init</code> and <code>sparkRHive.init</code> functions respectively.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
<span class="kw">library</span>(SparkR)

sparkEnvir &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">spark.executor.instance =</span> <span class="st">&#39;10&#39;</span>,
                   <span class="dt">spark.yarn.executor.memoryOverhead =</span> <span class="st">&#39;1000&#39;</span>)

sc &lt;-<span class="st"> </span><span class="kw">sparkR.init</span>(
  <span class="dt">sparkEnvir =</span> sparkEnvir,
  <span class="dt">sparkPackages =</span> <span class="st">&quot;com.databricks:spark-csv_2.10:1.3.0&quot;</span>
)</code></pre></div>
<pre><code>## Launching java with spark-submit command /usr/hdp/current/spark-client/bin/spark-submit  --packages com.databricks:spark-csv_2.10:1.3.0 sparkr-shell /tmp/RtmpY6KWOB/backend_port17fb706622b6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sqlContext &lt;-<span class="st"> </span><span class="kw">sparkRHive.init</span>(sc)</code></pre></div>
<p>We added the <code>sparkPackages</code> argument and set it to the value of <code>spark-csv</code> package provided by databricks. This will allow us to read csv objects into Spark DataFrames. Databricks are a prominent vendor for Spark, and are developers of many Spark packages (think of them as the RStudio of the Spark ecosystem).</p>
<p>After you are done using your Spark session, you can terminate your backend to Spark by running <code>sparkR.stop()</code>.</p>
</div>
<div id="creating-dataframes" class="section level2">
<h2><span class="header-section-number">8.3</span> Creating DataFrames</h2>
<p>Using our <code>sqlContext</code> variable, we can create DataFrames from local R data.frames, from Hive tables, or from other data sources. You could create a Spark DataFrame from a local R data.frame using the <code>createDataFrame</code> function,</p>
<div id="from-local-r-data.frames" class="section level3">
<h3><span class="header-section-number">8.3.1</span> From Local R data.frames</h3>
<p>Creating Spark DataFrames from local R <code>data.frames</code> might not seem like a great idea. After all, if it can fit in an R data.frame, that means it can fit in a single node’s memory, so why distribute it? It might be that you are testing your methods out on a sample dataset, and eventually you plan to scale out your analysis to larger datasets when your tests have passed. For expository purposes, this is a useful exercise, as it’ll expose you to the fundamentals of Spark DataFrames, and the SparkR syntax.</p>
<p>As a first example, we’ll import data from the <code>nycflight13</code> package into a Spark DataFrame, and use it’s data aggregation functions to tabulate the number of flights to each destination of flights originating from JFK.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nycflights13)
flights &lt;-<span class="st"> </span><span class="kw">createDataFrame</span>(sqlContext, nycflights13::flights)
jfk_flights &lt;-<span class="st"> </span><span class="kw">filter</span>(flights, flights$origin ==<span class="st"> &quot;JFK&quot;</span>)
<span class="co"># Group the flights by destination and aggregate by the number of flights</span>
dest_flights &lt;-<span class="st"> </span><span class="kw">summarize</span>(
  <span class="kw">group_by</span>(jkf_flights, jfk_flights$dest), 
  <span class="dt">count =</span> <span class="kw">n</span>(jfk_flights$dest)
  )
<span class="co"># Now sort by the `count` column and print the first few rows</span>
<span class="kw">head</span>(<span class="kw">arrange</span>(dest_flights, <span class="kw">desc</span>(dest_flights$count)))</code></pre></div>
<p>This same analysis could be streamlined using the <code>%&gt;%</code> operator exposed by the magrittr package to improve the readability of the pipeline:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(magrittr)
dest_flights &lt;-<span class="st"> </span><span class="kw">filter</span>(flights, flights$origin ==<span class="st"> &quot;JFK&quot;</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(flights$dest) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">count =</span> <span class="kw">n</span>(flights$dest))
dest_flights %&gt;%<span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(dest_flights$count)) %&gt;%<span class="st"> </span>head</code></pre></div>
</div>
<div id="code-reusability-and-non-standard-evaluation" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Code Reusability and Non-Standard Evaluation</h3>
<p>Many R users are familiar with the <a href="http://cran.r-project.org/web/packages/dplyr/"><code>dplyr</code> package</a> for data manipulation. The SparkR API has intentionally tried to imitate the syntax of the <code>dplyr</code> package because of it’s popularity and ease of use. Unfortunately, the <code>SparkR</code> lacks the <a href="https://cran.r-project.org/web/packages/dplyr/vignettes/nse.html">non-standard evaluation</a> procedures that <code>dplyr</code> utilizes, which lead to less-typing and, the ability to translate R code into other backends (such as SQL for databases). In particular, a typical <code>dplyr</code>allows you to do the following to create new columns for a <code>data.frame</code> of nyctaxi:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nycflights13)
flights %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(year, carrier) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">average_delay =</span> <span class="kw">mean</span>(arr_delay, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))</code></pre></div>
<pre><code>## Source: local data frame [16 x 3]
## Groups: year [?]
## 
##     year carrier average_delay
##    &lt;int&gt;   &lt;chr&gt;         &lt;dbl&gt;
## 1   2013      9E     7.3796692
## 2   2013      AA     0.3642909
## 3   2013      AS    -9.9308886
## 4   2013      B6     9.4579733
## 5   2013      DL     1.6443409
## 6   2013      EV    15.7964311
## 7   2013      F9    21.9207048
## 8   2013      FL    20.1159055
## 9   2013      HA    -6.9152047
## 10  2013      MQ    10.7747334
## 11  2013      OO    11.9310345
## 12  2013      UA     3.5580111
## 13  2013      US     2.1295951
## 14  2013      VX     1.7644644
## 15  2013      WN     9.6491199
## 16  2013      YV    15.5569853</code></pre>
<p>That the above code works as easily as it does is pretty incredible. The only object that actually exists (in an explicity sense, a pointer to a object living in memory) is the <code>flights</code> data.frame (which is lazily loaded from the <code>nycflights13</code> package). The objects <code>year</code>, <code>carrier</code> and <code>arr_delay</code> don’t actually exist outside of the <code>data.frame</code> <code>flights</code>. For example, if you tried <code>summary(year)</code> you will get the error “object not found”. This makes sense, in that we don’t actually have <code>year</code> as an object in our current global environment.</p>
<p>In order to make the code work, <code>dplyr</code> relies on <a href="http://adv-r.had.co.nz/Computing-on-the-language.html">non-standard evaluation</a>. In particular, Hadley recreated the functions <code>substitute</code> and <code>deparse</code> from the <code>base</code> R package in a new package called <a href="https://cran.r-project.org/web/packages/lazyeval/vignettes/lazyeval-old.html"><code>lazyeval</code></a>, which makes it very easy to use non-standard evaluation. In NSE, the arguments of the function are captured as expressions, and are evaluated later. This is all possible due to R’s lazy evaluation procedures.</p>
<p>Unfortunately, the SparkR API does not currently take advantage of this procedure, and therefore you have to explicitly call on the columns that you want to evaluate. However, there is a very handy package <a href="github.com/akzaidi/SparkRext" class="uri">github.com/akzaidi/SparkRext</a> that provides lazy evaluation support and therefore, it is possible to write functions and functional pipelines that work with <code>data.frames</code> and can automatically be applied to Spark DataFrames without having to rewrite the code. This is described more fully in the section <em>Functional Pipelines</em>.</p>
<p>By loading the <code>SparkRext</code> package, we automatically get all that NSE goodness (at least for the main <code>dplyr</code> verbs).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(SparkRext)
flights &lt;-<span class="st"> </span><span class="kw">createDataFrame</span>(sqlContext, nycflights13::flights)
dest_flights &lt;-<span class="st"> </span><span class="kw">filter</span>(flights, origin ==<span class="st"> &quot;JFK&quot;</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(dest) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">ave_delay =</span> <span class="kw">mean</span>(arr_delay))
dest_flights %&gt;%<span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(ave_delay)) %&gt;%<span class="st"> </span>head</code></pre></div>
</div>
<div id="creating-dataframes-from-csv-files" class="section level3">
<h3><span class="header-section-number">8.3.3</span> Creating DataFrames from CSV Files</h3>
<p>Since we imported the spark-csv package, we can import CSV files into DataFrames. We will be using the full airlines dataset, which I assume you have already ingested into Blob storage by following the steps in Section <a href="ingestion.html#ingestion">6</a>.</p>
<p>We have saved in our data directory a couple of data directories (virtually, everything in Azure Storage is a simple blob!). Let’s see what we have in our directory using the <code>rxHadoopListFiles</code> command, which is simply a wrapper to hadoop shell command <code>hadoop fs -ls</code></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data_dir &lt;-<span class="st"> &quot;/user/RevoShare/alizaidi&quot;</span>
<span class="kw">rxHadoopListFiles</span>(data_dir)</code></pre></div>
<p>Taking a look at the individual directories, we can see how many files there are for the Airlines directory and Weather directory</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rxHadoopCommand</span>(<span class="st">&quot;fs -ls /user/RevoShare/alizaidi/AirOnTimeCSV | head&quot;</span>)
<span class="kw">rxHadoopCommand</span>(<span class="st">&quot;fs -ls /user/RevoShare/alizaidi/delayDataLarge/WeatherRaw | head&quot;</span>)</code></pre></div>
<p>Let’s read the airlines directory and the weather directory to Spark DataFrames. We will use the <code>read.df</code> function from the <code>spark.csv</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">airPath &lt;-<span class="st"> </span><span class="kw">file.path</span>(<span class="st">&quot;/user/RevoShare/alizaidi&quot;</span>, <span class="st">&quot;AirOnTimeCSV&quot;</span>)
weatherPath &lt;-<span class="st"> </span><span class="kw">file.path</span>(fullDataDir, <span class="st">&quot;Weather&quot;</span>) <span class="co"># pre-processed weather data</span>

<span class="co"># create a SparkR DataFrame for the airline data</span>

airDF &lt;-<span class="st"> </span><span class="kw">read.df</span>(sqlContext, airPath, <span class="dt">source =</span> <span class="st">&quot;com.databricks.spark.csv&quot;</span>, 
                 <span class="dt">header =</span> <span class="st">&quot;true&quot;</span>, <span class="dt">inferSchema =</span> <span class="st">&quot;true&quot;</span>)

<span class="co"># Create a SparkR DataFrame for the weather data</span>

weatherDF &lt;-<span class="st"> </span><span class="kw">read.df</span>(sqlContext, weatherPath, <span class="dt">source =</span> <span class="st">&quot;com.databricks.spark.csv&quot;</span>, 
                     <span class="dt">header =</span> <span class="st">&quot;true&quot;</span>, <span class="dt">inferSchema =</span> <span class="st">&quot;true&quot;</span>)</code></pre></div>
<p>Note that it took more than 6 minutes to load our airlines data into Spark DataFrames. However, subsequent operations on the <code>airDF</code> object will occur in-memory, and should be very fast.</p>
<p>Let’s count the number of rows in each of our DataFrames and print the first few rows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(SparkR)
<span class="kw">lapply</span>(<span class="kw">list</span>(airDF, weatherDF), count)
<span class="co"># [[1]]</span>
<span class="co"># [1] 148619655</span>
<span class="co"># </span>
<span class="co"># [[2]]</span>
<span class="co"># [1] 14829028</span>

<span class="kw">lapply</span>(<span class="kw">list</span>(airDF, weatherDF), head)
<span class="co"># [[1]]</span>
<span class="co">#   YEAR MONTH DAY_OF_MONTH DAY_OF_WEEK    FL_DATE UNIQUE_CARRIER TAIL_NUM FL_NUM</span>
<span class="co"># 1 1987    10            1           4 1987-10-01             AA               1</span>
<span class="co"># 2 1987    10            2           5 1987-10-02             AA               1</span>
<span class="co"># 3 1987    10            3           6 1987-10-03             AA               1</span>
<span class="co"># 4 1987    10            4           7 1987-10-04             AA               1</span>
<span class="co"># 5 1987    10            5           1 1987-10-05             AA               1</span>
<span class="co"># 6 1987    10            6           2 1987-10-06             AA               1</span>
<span class="co">#   ORIGIN_AIRPORT_ID ORIGIN ORIGIN_STATE_ABR DEST_AIRPORT_ID DEST DEST_STATE_ABR</span>
<span class="co"># 1             12478    JFK               NY           12892  LAX             CA</span>
<span class="co"># 2             12478    JFK               NY           12892  LAX             CA</span>
<span class="co"># 3             12478    JFK               NY           12892  LAX             CA</span>
<span class="co"># 4             12478    JFK               NY           12892  LAX             CA</span>
<span class="co"># 5             12478    JFK               NY           12892  LAX             CA</span>
<span class="co"># 6             12478    JFK               NY           12892  LAX             CA</span>
<span class="co">#   CRS_DEP_TIME DEP_TIME DEP_DELAY DEP_DELAY_NEW DEP_DEL15 DEP_DELAY_GROUP TAXI_OUT</span>
<span class="co"># 1          900      901         1             1         0               0         </span>
<span class="co"># 2          900      901         1             1         0               0         </span>
<span class="co"># 3          900      859        -1             0         0              -1         </span>
<span class="co"># 4          900      900         0             0         0               0         </span>
<span class="co"># 5          900      902         2             2         0               0         </span>
<span class="co"># 6          900      900         0             0         0               0         </span>
<span class="co">#   WHEELS_OFF WHEELS_ON TAXI_IN CRS_ARR_TIME ARR_TIME ARR_DELAY ARR_DELAY_NEW</span>
<span class="co"># 1                                      1152     1117       -35             0</span>
<span class="co"># 2                                      1152     1137       -15             0</span>
<span class="co"># 3                                      1152     1111       -41             0</span>
<span class="co"># 4                                      1152     1116       -36             0</span>
<span class="co"># 5                                      1152     1119       -33             0</span>
<span class="co"># 6                                      1152       NA        NA            NA</span>
<span class="co">#   ARR_DEL15 ARR_DELAY_GROUP CANCELLED CANCELLATION_CODE DIVERTED CRS_ELAPSED_TIME</span>
<span class="co"># 1         0              -2         0                          0              352</span>
<span class="co"># 2         0              -1         0                          0              352</span>
<span class="co"># 3         0              -2         0                          0              352</span>
<span class="co"># 4         0              -2         0                          0              352</span>
<span class="co"># 5         0              -2         0                          0              352</span>
<span class="co"># 6        NA              NA         0                          1              352</span>
<span class="co">#   ACTUAL_ELAPSED_TIME AIR_TIME FLIGHTS DISTANCE DISTANCE_GROUP CARRIER_DELAY</span>
<span class="co"># 1                 316                1     2475             10              </span>
<span class="co"># 2                 336                1     2475             10              </span>
<span class="co"># 3                 312                1     2475             10              </span>
<span class="co"># 4                 316                1     2475             10              </span>
<span class="co"># 5                 317                1     2475             10              </span>
<span class="co"># 6                  NA                1     2475             10              </span>
<span class="co">#   WEATHER_DELAY NAS_DELAY SECURITY_DELAY LATE_AIRCRAFT_DELAY </span>
<span class="co"># 1                                                            </span>
<span class="co"># 2                                                            </span>
<span class="co"># 3                                                            </span>
<span class="co"># 4                                                            </span>
<span class="co"># 5                                                            </span>
<span class="co"># 6                                                            </span>
<span class="co"># </span>
<span class="co"># [[2]]</span>
<span class="co">#   Visibility DryBulbCelsius DewPointCelsius RelativeHumidity WindSpeed Altimeter</span>
<span class="co"># 1   4.000000       0.000000       -1.000000        92.000000  0.000000 29.690000</span>
<span class="co"># 2  10.000000       7.000000       -3.000000        49.000000 11.000000 29.790000</span>
<span class="co"># 3   3.000000       1.000000        0.000000        92.000000  3.000000 29.710000</span>
<span class="co"># 4  10.000000       7.000000       -5.000000        42.000000 18.000000 29.710000</span>
<span class="co"># 5   1.250000       3.000000        0.000000        82.000000  6.000000 29.720000</span>
<span class="co"># 6  10.000000       8.000000       -4.000000        44.000000 14.000000 29.770000</span>
<span class="co">#   AdjustedYear AdjustedMonth AdjustedDay AdjustedHour AirportID</span>
<span class="co"># 1         2007             5           4           18     15177</span>
<span class="co"># 2         2007             5           4            6     15177</span>
<span class="co"># 3         2007             5           4           17     15177</span>
<span class="co"># 4         2007             5           4            9     15177</span>
<span class="co"># 5         2007             5           4           10     15177</span>
<span class="co"># 6         2007             5           4            7     15177</span></code></pre></div>
</div>
</div>
<div id="caching-dataframes" class="section level2">
<h2><span class="header-section-number">8.4</span> Caching DataFrames</h2>
<p>Spark provides a few different options for caching DataFrames.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="rprofile.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="exploratory-data-analysis-with-sparkr.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
