<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Scalable Machine Learning and Data Science with R and Spark</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This book is a guide for doing data analysis with Spark and R. It is organized as a collection of examples of data science problems analyzed with these two technologies, and tries to highlight programming paradigms that make the best of both worlds. It starts with a discussion of using R for data science and machine learning, and describes how to create functional programming pipelines that are data source invariant, easy to write, easy to debug, and easy to deploy! In order to scale out typical R workflows, we showcase how to use Spark and Microsoft R Server to bring your favorite R functions to large datasets.">
  <meta name="generator" content="bookdown 0.0.79 and GitBook 2.6.7">

  <meta property="og:title" content="Scalable Machine Learning and Data Science with R and Spark" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/scalemlcover.png" />
  <meta property="og:description" content="This book is a guide for doing data analysis with Spark and R. It is organized as a collection of examples of data science problems analyzed with these two technologies, and tries to highlight programming paradigms that make the best of both worlds. It starts with a discussion of using R for data science and machine learning, and describes how to create functional programming pipelines that are data source invariant, easy to write, easy to debug, and easy to deploy! In order to scale out typical R workflows, we showcase how to use Spark and Microsoft R Server to bring your favorite R functions to large datasets." />
  <meta name="github-repo" content="akzaidi/spark-mrs-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Scalable Machine Learning and Data Science with R and Spark" />
  
  <meta name="twitter:description" content="This book is a guide for doing data analysis with Spark and R. It is organized as a collection of examples of data science problems analyzed with these two technologies, and tries to highlight programming paradigms that make the best of both worlds. It starts with a discussion of using R for data science and machine learning, and describes how to create functional programming pipelines that are data source invariant, easy to write, easy to debug, and easy to deploy! In order to scale out typical R workflows, we showcase how to use Spark and Microsoft R Server to bring your favorite R functions to large datasets." />
  <meta name="twitter:image" content="images/scalemlcover.png" />

<meta name="author" content="Ali Zaidi">

<meta name="date" content="2016-07-10">


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="modeling-with-microsoft-r-server.html">


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.6/htmlwidgets.js"></script>
<link href="libs/plotlyjs-1.10.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.10.1/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-3.6.5/plotly.js"></script>
<script src="libs/d3-3.5.5/d3.min.js"></script>
<script src="libs/d3-grid-0.1.0/d3-grid.js"></script>
<script src="libs/dimple-2.1.6/dimple.min.js"></script>
<script src="libs/dimple-binding-0.1/dimple.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#scalable-machine-learning-and-data-science"><i class="fa fa-check"></i>Scalable Machine Learning and Data Science</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#useful-resources"><i class="fa fa-check"></i>Useful Resources</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#spark"><i class="fa fa-check"></i>Spark</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#microsoft-r-server"><i class="fa fa-check"></i>Microsoft R Server</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#azure-hdinsight"><i class="fa fa-check"></i>Azure HDInsight</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Overview</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#why-r"><i class="fa fa-check"></i><b>1.1</b> Why R?</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#microsoft-r-server-ftw"><i class="fa fa-check"></i><b>1.2</b> Microsoft R Server FTW</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#apache-spark"><i class="fa fa-check"></i><b>1.3</b> Apache Spark</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#sparkr"><i class="fa fa-check"></i><b>1.4</b> SparkR</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#azure-hdinsight-1"><i class="fa fa-check"></i><b>1.5</b> Azure HDInsight</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#prerequisites---what-youll-need"><i class="fa fa-check"></i><b>1.6</b> Prerequisites - What You’ll Need</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#data-used"><i class="fa fa-check"></i><b>1.7</b> Data Used</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#versioning"><i class="fa fa-check"></i><b>1.8</b> Versioning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="apache-spark-1.html"><a href="apache-spark-1.html"><i class="fa fa-check"></i><b>2</b> Apache Spark</a><ul>
<li class="chapter" data-level="2.1" data-path="apache-spark-1.html"><a href="apache-spark-1.html#lazyevalfp"><i class="fa fa-check"></i><b>2.1</b> Functional Programming and Lazy Evaluation</a></li>
<li class="chapter" data-level="2.2" data-path="apache-spark-1.html"><a href="apache-spark-1.html#distributed-programming-abstractions"><i class="fa fa-check"></i><b>2.2</b> Distributed Programming Abstractions</a></li>
<li class="chapter" data-level="2.3" data-path="apache-spark-1.html"><a href="apache-spark-1.html#rdds"><i class="fa fa-check"></i><b>2.3</b> RDDs</a><ul>
<li class="chapter" data-level="2.3.1" data-path="apache-spark-1.html"><a href="apache-spark-1.html#common-transformations-and-actions"><i class="fa fa-check"></i><b>2.3.1</b> Common Transformations and Actions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="apache-spark-1.html"><a href="apache-spark-1.html#dataframes"><i class="fa fa-check"></i><b>2.4</b> DataFrames</a></li>
<li class="chapter" data-level="2.5" data-path="apache-spark-1.html"><a href="apache-spark-1.html#datasets"><i class="fa fa-check"></i><b>2.5</b> Datasets</a></li>
<li class="chapter" data-level="2.6" data-path="apache-spark-1.html"><a href="apache-spark-1.html#mllib"><i class="fa fa-check"></i><b>2.6</b> MLlib</a></li>
<li class="chapter" data-level="2.7" data-path="apache-spark-1.html"><a href="apache-spark-1.html#spark-apis"><i class="fa fa-check"></i><b>2.7</b> Spark APIs</a><ul>
<li class="chapter" data-level="2.7.1" data-path="apache-spark-1.html"><a href="apache-spark-1.html#scala"><i class="fa fa-check"></i><b>2.7.1</b> Scala</a></li>
<li class="chapter" data-level="2.7.2" data-path="apache-spark-1.html"><a href="apache-spark-1.html#pyspark"><i class="fa fa-check"></i><b>2.7.2</b> <a href="https://spark.apache.org/docs/latest/api/python/index.html">PySpark</a></a></li>
<li class="chapter" data-level="2.7.3" data-path="apache-spark-1.html"><a href="apache-spark-1.html#sparkr-1"><i class="fa fa-check"></i><b>2.7.3</b> SparkR</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="rserver.html"><a href="rserver.html"><i class="fa fa-check"></i><b>3</b> R &amp; Microsoft R Server - todo</a><ul>
<li class="chapter" data-level="3.1" data-path="rserver.html"><a href="rserver.html#functional-programming-and-lazy-evaluation-in-r"><i class="fa fa-check"></i><b>3.1</b> Functional Programming and Lazy Evaluation in R</a></li>
<li class="chapter" data-level="3.2" data-path="rserver.html"><a href="rserver.html#pema"><i class="fa fa-check"></i><b>3.2</b> PEMA Algorithms and the RevoScaleR Package</a></li>
<li class="chapter" data-level="3.3" data-path="rserver.html"><a href="rserver.html#xdf"><i class="fa fa-check"></i><b>3.3</b> eXternal Data Frames (XDFs)</a></li>
<li class="chapter" data-level="3.4" data-path="rserver.html"><a href="rserver.html#computecontext"><i class="fa fa-check"></i><b>3.4</b> Compute Contexts</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><i class="fa fa-check"></i><b>4</b> Azure HDInsight – Managed Hadoop in the Cloud - todo</a><ul>
<li class="chapter" data-level="4.1" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html#hdinsight-premium-spark-clusters-with-r-server"><i class="fa fa-check"></i><b>4.1</b> HDInsight Premium Spark Clusters with R Server</a></li>
<li class="chapter" data-level="4.2" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html#dashboards-for-management"><i class="fa fa-check"></i><b>4.2</b> Dashboards for Management</a></li>
<li class="chapter" data-level="4.3" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html#jupyter-and-rstudio-server"><i class="fa fa-check"></i><b>4.3</b> Jupyter and RStudio Server</a></li>
</ul></li>
<li class="part"><span><b>Provisioning and Ingesting Data</b></span></li>
<li class="chapter" data-level="5" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html"><i class="fa fa-check"></i><b>5</b> Provisioning Instructions</a><ul>
<li class="chapter" data-level="5.1" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html#provision-cluster-from-azure-portal"><i class="fa fa-check"></i><b>5.1</b> Provision Cluster from Azure Portal</a></li>
<li class="chapter" data-level="5.2" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html#installing-packages"><i class="fa fa-check"></i><b>5.2</b> Installing Packages</a><ul>
<li class="chapter" data-level="5.2.1" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html#todo---install-packages-demo"><i class="fa fa-check"></i><b>5.2.1</b> todo - install packages demo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ingestion.html"><a href="ingestion.html"><i class="fa fa-check"></i><b>6</b> Ingesting Data into Azure Blob Storage - todo</a><ul>
<li class="chapter" data-level="6.1" data-path="ingestion.html"><a href="ingestion.html#azcopy"><i class="fa fa-check"></i><b>6.1</b> AzCopy</a></li>
<li class="chapter" data-level="6.2" data-path="ingestion.html"><a href="ingestion.html#azure-storage-explorer"><i class="fa fa-check"></i><b>6.2</b> Azure Storage Explorer</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="rprofile.html"><a href="rprofile.html"><i class="fa fa-check"></i><b>7</b> Setting Your R Profile</a></li>
<li class="part"><span><b>Data Manipulation and Data Aggregation</b></span></li>
<li class="chapter" data-level="8" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html"><i class="fa fa-check"></i><b>8</b> Starting Your Machine Learning Pipeline</a><ul>
<li class="chapter" data-level="8.1" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#finding-the-sparkr-library"><i class="fa fa-check"></i><b>8.1</b> Finding the SparkR Library</a></li>
<li class="chapter" data-level="8.2" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#creating-a-spark-context"><i class="fa fa-check"></i><b>8.2</b> Creating a Spark Context</a></li>
<li class="chapter" data-level="8.3" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#creating-dataframes"><i class="fa fa-check"></i><b>8.3</b> Creating DataFrames</a><ul>
<li class="chapter" data-level="8.3.1" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#from-local-r-data.frames"><i class="fa fa-check"></i><b>8.3.1</b> From Local R data.frames</a></li>
<li class="chapter" data-level="8.3.2" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#code-reusability-and-non-standard-evaluation"><i class="fa fa-check"></i><b>8.3.2</b> Code Reusability and Non-Standard Evaluation</a></li>
<li class="chapter" data-level="8.3.3" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#creating-dataframes-from-csv-files"><i class="fa fa-check"></i><b>8.3.3</b> Creating DataFrames from CSV Files</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#caching-dataframes"><i class="fa fa-check"></i><b>8.4</b> Caching DataFrames</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html"><i class="fa fa-check"></i><b>9</b> Exploratory Data Analysis with SparkR</a><ul>
<li class="chapter" data-level="9.1" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#sparkr-the-explorer"><i class="fa fa-check"></i><b>9.1</b> SparkR the Explorer</a></li>
<li class="chapter" data-level="9.2" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#doing-data-aggregations-with-sparkr-efficiently"><i class="fa fa-check"></i><b>9.2</b> Doing Data Aggregations with SparkR Efficiently</a></li>
<li class="chapter" data-level="9.3" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#from-spark-dataframes-to-local-dataframes"><i class="fa fa-check"></i><b>9.3</b> From Spark DataFrames to Local Dataframes</a></li>
<li class="chapter" data-level="9.4" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#plotting-results"><i class="fa fa-check"></i><b>9.4</b> Plotting Results</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html"><i class="fa fa-check"></i><b>10</b> Data Manipulation with SparkR</a><ul>
<li class="chapter" data-level="10.1" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#data-aggregations"><i class="fa fa-check"></i><b>10.1</b> Data Aggregations</a></li>
<li class="chapter" data-level="10.2" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#collecting-results-to-local-dataframes"><i class="fa fa-check"></i><b>10.2</b> Collecting Results to Local Dataframes</a></li>
<li class="chapter" data-level="10.3" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#dimple-bar-charts"><i class="fa fa-check"></i><b>10.3</b> Dimple Bar Charts</a></li>
<li class="chapter" data-level="10.4" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#merging-data"><i class="fa fa-check"></i><b>10.4</b> Merging Data</a></li>
<li class="chapter" data-level="10.5" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#exploring-credit-scores-for-mortgage-borrowers"><i class="fa fa-check"></i><b>10.5</b> Exploring Credit Scores for Mortgage Borrowers</a><ul>
<li class="chapter" data-level="10.5.1" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#ingesting-originations-data-into-spark-dataframes"><i class="fa fa-check"></i><b>10.5.1</b> Ingesting Originations Data into Spark DataFrames</a></li>
<li class="chapter" data-level="10.5.2" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#calculting-state-level-credit-attributes"><i class="fa fa-check"></i><b>10.5.2</b> Calculting State Level Credit Attributes</a></li>
<li class="chapter" data-level="10.5.3" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#credit-attribute-choropleths"><i class="fa fa-check"></i><b>10.5.3</b> Credit Attribute Choropleths</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Modeling and Prediction with Microsoft R Server</b></span></li>
<li class="chapter" data-level="11" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html"><i class="fa fa-check"></i><b>11</b> Modeling with Microsoft R Server</a><ul>
<li class="chapter" data-level="11.1" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#import-csv-to-xdf"><i class="fa fa-check"></i><b>11.1</b> Import CSV to XDF</a></li>
<li class="chapter" data-level="11.2" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#splitting-xdf-into-train-and-test-tests"><i class="fa fa-check"></i><b>11.2</b> Splitting XDF into Train and Test Tests</a></li>
<li class="chapter" data-level="11.3" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#training-binary-classification-models"><i class="fa fa-check"></i><b>11.3</b> Training Binary Classification Models</a><ul>
<li class="chapter" data-level="11.3.1" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#logistic-regression-models"><i class="fa fa-check"></i><b>11.3.1</b> Logistic Regression Models</a></li>
<li class="chapter" data-level="11.3.2" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#tree-and-ensemble-classifiers"><i class="fa fa-check"></i><b>11.3.2</b> Tree and Ensemble Classifiers</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#testing-models"><i class="fa fa-check"></i><b>11.4</b> Testing Models</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html"><i class="fa fa-check"></i><b>12</b> Spark Backend for dplyr</a><ul>
<li class="chapter" data-level="12.1" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html#sparkrapi"><i class="fa fa-check"></i><b>12.1</b> sparkrapi</a><ul>
<li class="chapter" data-level="12.1.1" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html#a-proper-dplyr-backend-for-spark-dataframes"><i class="fa fa-check"></i><b>12.1.1</b> A Proper <code>dplyr</code> backend for Spark DataFrames</a></li>
<li class="chapter" data-level="12.1.2" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html#aggregating-data"><i class="fa fa-check"></i><b>12.1.2</b> Aggregating Data</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Scalable Machine Learning and Data Science with R and Spark</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="spark-backend-for-dplyr" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Spark Backend for dplyr</h1>
<p>The last section described the limitations with the <code>SparkR</code> API and provided some solutions through the <code>SparkRext</code> package. The team at RStudio has developed an alternative and more comprehensive solution for accessing Spark through an R API. This chapter showcases these extensions. For more information you should visit the documentation at the <a href="http://spark.rstudio.com/">RStudio Spark Homepage</a></p>
<div id="sparkrapi" class="section level2">
<h2><span class="header-section-number">12.1</span> sparkrapi</h2>
<p>Underpinning the Spark API developed by RStudio is the package <a href="github.com/rstudio/sparkapi">sparkapi</a>, which provides access to the SQLContext and HiveContext needed for creating DataFrames, but also provides access to calling the full Spark Scala API. By providing a low-level API like this, it is possible to create <a href="http://spark.rstudio.com/extensions.html">extensions</a> that access parts of the Spark codebase that are not currently exposed to the R API. As an example extension, the RStudio team immediately wrote a package called <a href="github.com/rstudio/sparklyr">sparklyr</a> which provides an <em>actual</em> backend for <code>dplyr</code>.</p>
<div id="a-proper-dplyr-backend-for-spark-dataframes" class="section level3">
<h3><span class="header-section-number">12.1.1</span> A Proper <code>dplyr</code> backend for Spark DataFrames</h3>
<p>While <code>SparkR</code> attempted to mimick the <code>dplyr</code> syntax, the actual functions exported by the package masked the <code>dplyr</code> functions, making it impossible to interoperate between the two packages (loading one would cause calls from the second package to fail). While the <code>SparkRext</code> attempted to resolve this issue by properly defining classes and methods for the <code>dplyr</code> functions that interacted specifically with Spark DataFrames, the <code>sparklyr</code> package took the process one step further (lower?). Since <code>dplyr</code> supports <a href="https://cran.r-project.org/web/packages/dplyr/vignettes/new-sql-backend.html">SQL backends</a> as a data source, and since Spark DataFrames use Spark SQL to create and query its DataFrames, <code>sparklyr</code> allows complete access to <code>dplyr</code> by invoking SQL commands directly to <code>src_sql</code> in <code>dplyr</code>.</p>
<p>In order to get started with the <code>sparklyr</code> package, we need to create a Spark connection. We specify the connection directly through <code>sparklyr</code>, by running <code>spark_connect</code> and specifying our cluster’s URL. Since we are using a HDInsight Spark cluster, our cluster is managed through YARN.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="st">&quot;yarn-client&quot;</span>)</code></pre></div>
<p>Using the <code>sparklyr</code> package, importing data immediately an object of class <code>tbl_spark</code>, which inherits from the more general <code>dplyr</code> class, <code>tbl_sql</code> and <code>tbl</code> (see the <a href="github.com/hadley/tibble"><code>tibble</code></a> for more information about the <code>tibble</code> format), and allows access to all the functions exported by the <code>dplyr</code> package with methods for the <code>tbl_sql</code> class.</p>
<p>In this section, we will examine the data from <a href="http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html">Fannie Mae Single Family Loan Performance Data</a>. Our data is located in an Azure Storage Account, and is delimited by a pipe and does not have a header. The function <code>spark_read_csv</code> simply invokes the <code>read.df</code> function we had used earlier, so you can think of it is a wrapper function. It is useful mainly for the additional attributes it defines on it’s return value: the <code>tbl_spark</code> class.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fannie &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="dt">name =</span> <span class="st">&quot;fannie&quot;</span>, 
                         <span class="dt">path =</span> <span class="st">&quot;wasb://mrs-spark@alizaidi.blob.core.windows.net/user/RevoShare/alizaidi/Acquisition/&quot;</span>,
                         <span class="dt">header =</span> F,
                         <span class="dt">delimiter =</span> <span class="st">&quot;|&quot;</span>)

<span class="kw">class</span>(fannie)

<span class="kw">library</span>(dplyr)

fannie &lt;-<span class="st"> </span>fannie %&gt;%<span class="st"> </span><span class="kw">rename</span>(<span class="dt">loan_id =</span> C0,
                            <span class="dt">orig_chn =</span> C1,
                            <span class="dt">seller_name =</span> C2,
                            <span class="dt">orig_rt =</span> C3,
                            <span class="dt">orig_amt =</span> C4,
                            <span class="dt">orig_trm =</span> C5,
                            <span class="dt">orig_dte =</span> C6,
                            <span class="dt">frst_dte =</span> C7,
                            <span class="dt">oltv =</span> C8,
                            <span class="dt">ocltv =</span> C9,
                            <span class="dt">num_bo =</span> C10,
                            <span class="dt">dti =</span> C11,
                            <span class="dt">cscore_b =</span> C12,
                            <span class="dt">fthb_flg =</span> C13,
                            <span class="dt">purpose =</span> C14,
                            <span class="dt">prop_typ =</span> C15,
                            <span class="dt">num_unit  =</span> C16,
                            <span class="dt">occ_stat  =</span> C17,
                            <span class="dt">state  =</span> C18,
                            <span class="dt">zip_3 =</span> C19,
                            <span class="dt">mi_pct  =</span> C20,
                            <span class="dt">product_type =</span> C21,
                            <span class="dt">cscore_co  =</span> C22)

fannie %&gt;%<span class="st"> </span>head</code></pre></div>
</div>
<div id="aggregating-data" class="section level3">
<h3><span class="header-section-number">12.1.2</span> Aggregating Data</h3>
<p>Now that we have a proper <code>tbl</code> class that works with the <code>dplyr</code> package, any function we write that is desgined to work with <code>dplyr</code> will automatically work here as well. The only caveat is that not all methods available for the <code>tbl_df</code> class are available for the <code>tbl_spark</code> or <code>tbl_sql</code> class. For example, you can’t use arbitrary R functions with <code>tbl_sql</code>, just those can be easily understood by SQL.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fannie &lt;-<span class="st"> </span>fannie %&gt;%<span class="st"> </span><span class="kw">mutate</span>(<span class="dt">orig_year =</span> <span class="kw">substr</span>(orig_dte, <span class="dv">4</span>, <span class="dv">7</span>))

zip_summary &lt;-<span class="st"> </span>fannie %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(zip_3, orig_year) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">ave_amnt =</span> <span class="kw">mean</span>(orig_amt), <span class="dt">ave_ltv =</span> <span class="kw">mean</span>(oltv), 
            <span class="dt">ave_dti =</span> <span class="kw">mean</span>(dti), <span class="dt">ave_cscore =</span> <span class="kw">mean</span>(cscore_b))

zip_summary %&gt;%<span class="st"> </span>head</code></pre></div>

<div id="refs" class="references">

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modeling-with-microsoft-r-server.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>


<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
