<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Scalable Machine Learning and Data Science with Microsoft R Server and Spark</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="These are (tentatively) rough notes showcasing some tips on conducting large scale data analysis with R, Spark, and Microsoft R Server. The focus is primarily on machine learning with Azure HDInsight platform, but review other in-memory, large-scale data analysis platforms, such as R Services with SQL Server 2016, and discuss how to utilize BI tools such as PowerBI and Shiny for dynamic reporting, and report generation.">
  <meta name="generator" content="bookdown 0.0.66 and GitBook 2.6.7">

  <meta property="og:title" content="Scalable Machine Learning and Data Science with Microsoft R Server and Spark" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are (tentatively) rough notes showcasing some tips on conducting large scale data analysis with R, Spark, and Microsoft R Server. The focus is primarily on machine learning with Azure HDInsight platform, but review other in-memory, large-scale data analysis platforms, such as R Services with SQL Server 2016, and discuss how to utilize BI tools such as PowerBI and Shiny for dynamic reporting, and report generation." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Scalable Machine Learning and Data Science with Microsoft R Server and Spark" />
  
  <meta name="twitter:description" content="These are (tentatively) rough notes showcasing some tips on conducting large scale data analysis with R, Spark, and Microsoft R Server. The focus is primarily on machine learning with Azure HDInsight platform, but review other in-memory, large-scale data analysis platforms, such as R Services with SQL Server 2016, and discuss how to utilize BI tools such as PowerBI and Shiny for dynamic reporting, and report generation." />
  

<meta name="author" content="Ali Zaidi, Machine Learning and Data Science, Microsoft">

<meta name="date" content="2016-04-22">


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="r-microsoft-r-server-todo.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Scalable Machine Learning with MRS and Spark</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="part"><b><a href="#">Overview</a></b></li>
<li class="chapter" data-level="1" data-path="apache-spark-todo.html"><a href="apache-spark-todo.html"><i class="fa fa-check"></i><b>1</b> Apache Spark - todo</a><ul>
<li class="chapter" data-level="1.1" data-path="apache-spark-todo.html"><a href="apache-spark-todo.html#lazyevalfp"><i class="fa fa-check"></i><b>1.1</b> Functional Programming and Lazy Evaluation</a></li>
<li class="chapter" data-level="1.2" data-path="apache-spark-todo.html"><a href="apache-spark-todo.html#distributed-programming-abstractions"><i class="fa fa-check"></i><b>1.2</b> Distributed Programming Abstractions</a></li>
<li class="chapter" data-level="1.3" data-path="apache-spark-todo.html"><a href="apache-spark-todo.html#rdds"><i class="fa fa-check"></i><b>1.3</b> RDDs</a></li>
<li class="chapter" data-level="1.4" data-path="apache-spark-todo.html"><a href="apache-spark-todo.html#dataframes"><i class="fa fa-check"></i><b>1.4</b> DataFrames</a></li>
<li class="chapter" data-level="1.5" data-path="apache-spark-todo.html"><a href="apache-spark-todo.html#mllib"><i class="fa fa-check"></i><b>1.5</b> MLlib</a></li>
<li class="chapter" data-level="1.6" data-path="apache-spark-todo.html"><a href="apache-spark-todo.html#spark-apis"><i class="fa fa-check"></i><b>1.6</b> Spark APIs</a><ul>
<li class="chapter" data-level="1.6.1" data-path="apache-spark-todo.html"><a href="apache-spark-todo.html#scala"><i class="fa fa-check"></i><b>1.6.1</b> Scala</a></li>
<li class="chapter" data-level="1.6.2" data-path="apache-spark-todo.html"><a href="apache-spark-todo.html#pyspark"><i class="fa fa-check"></i><b>1.6.2</b> <a href="https://spark.apache.org/docs/latest/api/python/index.html">PySpark</a></a></li>
<li class="chapter" data-level="1.6.3" data-path="apache-spark-todo.html"><a href="apache-spark-todo.html#sparkr"><i class="fa fa-check"></i><b>1.6.3</b> SparkR</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="r-microsoft-r-server-todo.html"><a href="r-microsoft-r-server-todo.html"><i class="fa fa-check"></i><b>2</b> R &amp; Microsoft R Server - todo</a></li>
<li class="chapter" data-level="3" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><i class="fa fa-check"></i><b>3</b> Azure HDInsight – Managed Hadoop in the Cloud - todo</a></li>
<li class="part"><b><a href="#">Provisioning and Ingesting Data</a></b></li>
<li class="part"><b><a href="#">Data Manipulation and Data Aggregation</a></b></li>
<li class="chapter" data-level="4" data-path="placeholder.html"><a href="placeholder.html"><i class="fa fa-check"></i><b>4</b> Placeholder</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Scalable Machine Learning and Data Science with Microsoft R Server and Spark</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="apache-spark---todo" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Apache Spark - todo</h1>
<p>The core of Apache Spark consists of four components:</p>
<ol style="list-style-type: decimal">
<li><a href="http://spark.apache.org/sql/">Spark SQL</a></li>
<li><a href="http://spark.apache.org/streaming/">Spark Streaming</a></li>
<li><a href="http://spark.apache.org/mllib/">MLlib - Machine Learning Library</a></li>
<li><a href="http://spark.apache.org/graphx/">GraphX - Graphical Computing Library</a></li>
</ol>
<p>The Spark project is constantly under development, and the AMPLab is frequently adding new packages and implementing new ideas. An overview of the current state of the ecosystem can be found here: <a href="https://amplab.cs.berkeley.edu/software/">Berkeley Data Analytics Stack</a></p>
<div id="lazyevalfp" class="section level2">
<h2><span class="header-section-number">1.1</span> Functional Programming and Lazy Evaluation</h2>
<p>In functional programming, all constructs are functions, and all computations are evaluated as function calls (higher-order functions being functions that call upon other functions, think of it as function composition). Pure functional languages (such as Haskell), aim to achieve purity in their evaluation, and therefore rarely change-state or mutate objects, and don’t cause any side-effects.</p>
<p>Scala, which composes ~80% of the code in Spark, has full support for functional programming. Although not a pure functional progrmaming langauge like Haskell, it is actually a hybrid of object oriented languages like Java and functional languages like Haskell.</p>
<p>Lazy evaluation allows you defer evaluation of a function until it is necessary. This makes it easy to create higher order functions, and then optimize their computation by only evaluation what is necessary. This is particularly useful for data science, where one is often manipulating large amounts of data, filtering through the noise to find the signal, which is where you would want most of your computation to focus on.</p>
</div>
<div id="distributed-programming-abstractions" class="section level2">
<h2><span class="header-section-number">1.2</span> Distributed Programming Abstractions</h2>
<p>A Spark program consists of two programs, a driver program and a worker program. The driver programs run on the driver machine, whereas the worker program runs on cluster nodes (or in local threads when running on a single machine).</p>
</div>
<div id="rdds" class="section level2">
<h2><span class="header-section-number">1.3</span> RDDs</h2>
<p>RDDs, short for Resilient Distributed Datasets</p>
</div>
<div id="dataframes" class="section level2">
<h2><span class="header-section-number">1.4</span> DataFrames</h2>
<p>When working with relational data for structured data processing, most data scientists will think of using SQL, due to it’s highly efficient relational algebra. Spark provides a SQL interface with Spark SQL and a SQL context, SQLContext.</p>
<p>Spark SQL is a Spark module for structured data processing. While the RDD API is great for generic data storage, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL, the DataFrames API and the Datasets API. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between the various APIs based on which provides the most natural way to express a given transformation.</p>
<p>DataFrames use the catalyst query optimizer to make querying more efficient. The catalyst query optimizer leverages advanced Scala features, such as pattern matching and quasiquotes to build an extenisble query optimizer.</p>
</div>
<div id="mllib" class="section level2">
<h2><span class="header-section-number">1.5</span> MLlib</h2>
</div>
<div id="spark-apis" class="section level2">
<h2><span class="header-section-number">1.6</span> Spark APIs</h2>
<div id="scala" class="section level3">
<h3><span class="header-section-number">1.6.1</span> Scala</h3>
</div>
<div id="pyspark" class="section level3">
<h3><span class="header-section-number">1.6.2</span> <a href="https://spark.apache.org/docs/latest/api/python/index.html">PySpark</a></h3>
<p>The <a href="https://spark.apache.org/docs/latest/api/python/index.html">PySpark</a> API might be the most commonly used API, due to Python’s ubiquity in data science. It is perhaps second in maturity to the Scala API, and when working with DataFrames, provides nearly the same performance as what you would get when writing Scala.</p>
</div>
<div id="sparkr" class="section level3">
<h3><span class="header-section-number">1.6.3</span> SparkR</h3>
<p>The SparkR API provides two ways of interacting with Spark through R: a package called SparkR for creating the necessary abstractions in your R session to access Spark’s functionality, and a sparkR shell which loads the library on top of the interactive R shell and creates the necessary SparkContexts automatically. To use the shell, see this <a href="https://github.com/amplab-extras/SparkR-pkg/wiki/SparkR-Quick-Start">quickstart</a>. The examples in these notes use the SparkR library from RStudio, but you could use the SparkR library from any R environment of your choice.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="r-microsoft-r-server-todo.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-spark.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
