[
["index.html", "Scalable Machine Learning and Data Science with Microsoft R Server and Spark Abstract 0.1 Scalable Machine Learning with Microsoft R Server and Spark 0.2 Useful Resources", " Scalable Machine Learning and Data Science with Microsoft R Server and Spark Ali Zaidi, Machine Learning and Data Science, Microsoft 2016-04-21   Abstract  0.1 Scalable Machine Learning with Microsoft R Server and Spark This collection of notes in progress will help you get started on using Microsoft R Server and Spark with the Azure HDInsight platform.   0.2 Useful Resources There are a number of useful resources on Spark, R, and Azure HDInsight. I have listed a few that I found particularly useful.  0.2.1 Spark  Spark Documentation Home Page The Founding Paper Origination of RDDs MLlib – Machine Learning in Spark Spark Programming Guide Spark Packages Spark Summit Jacek Laskowski - Mastering Apache Spark     0.2.2 Microsoft R Server  Landing Page for R Server Documentation     0.2.3 Azure HDInsight  HDInsight Documentation Home Page Machine Learning with HDInsight Spark R Server on HDInsight        "],
["introduction.html", "Chapter 1 Introduction 1.1 Why R? 1.2 Microsoft R Server FTW 1.3 Apache Spark 1.4 SparkR 1.5 Azure HDInsight 1.6 Prerequisites - What You’ll Need", " Chapter 1 Introduction This book is organized into modules, each of which provide a motivated example of doing data science with R and Spark. The modules are based on notes I created while learning how to make scalable machine learning pipelines, focusing on the tools provided by Microsoft R Server, Azure HDInsight, and Spark.  1.1 Why R? R is a tool of choice for many data scientists. The abundance of available packages for statistical modeling, visualization, and machine learning, coupled with the deep interactivity baked into it’s very foundation, push it to the top of the stack for off-the-shelf languages for data science. Unfortunately, in order to maintain the level of interactivity R provides, it must sacrifice on performance relative to low-level, statically typed languages, which makes it inherantly difficult for R to scale, and inhibits it’s adoption in enterprise.   1.2 Microsoft R Server FTW Microsoft R Server (formerly known as Revolution R Enterprise) was developed to tackle R’s scalability challenges and increase the adoption of the R project in industry. The MRS distribution includes R packages designed specifically for scalability, exposing new parallel external memory algorithms that interact with data residing in disk or distributed data stores, and a new highly optimized columnar data object, called xdf (short for eXternal Data Frame), that is chunked and especially amenable for parallelization. A data scientist’s coding and debugging time is the most important resource in data science applications, and MRS makes it possible for the data scientist to execute highly performant distributed algorithms on huge amounts of data without ever having to leave their favorite programming environment!   1.3 Apache Spark Developed at the AMPLab at Berkeley, Spark was designed to tackle scalability. Data is growing much faster than Moore’s law for CPUs, so creating commodity computers were not feasible or scalable for the type of data we face today. However, the cost of memory is dropping at a rate that is comparable to the growth of data. While Hadoop revolutionized computing by reintroducing distributed computing through the MapReduce framework, and distributed storage through HDFS, Spark spurred the revolution further by utilizing memory for in-data sharing during interactive Map Reduce jobs. Spark can be 10 - 100 orders of magnitude faster than traditional Map Reduce.   1.4 SparkR Spark has a number of APIs, allowing you to write code in your favorite language to be executed in Spark. The most popular APIs for Spark are Scala and Python. The SparkR API is less mature than the Python and Scala APIs, but provides R with an abstraction to interact with data residing in Spark DataFrames in a manner that looks a lot like manipulating R data.frames, and has a syntax that will be familiar to many R users of the dplyr package.   1.5 Azure HDInsight [todo] add overview of azure HDInsight…   1.6 Prerequisites - What You’ll Need While much of the material in these notes will generalize to other implementations of Spark and R, in order to take complete advantage of everything here you’ll need an Azure subscription, and enough credit in your subscription to provision a Premium Spark HDInsight Cluster. More details on provisioning are provided in the HDInsight chapter. The complete prerequisites (in order of importance):  An Azure subscription A terminal emulator with openSSH or bash, e.g., bash on Linux, Mac Terminal, iTerm2, Putty, or Cygwin/MobaXterm PowerBI Desktop Azure Storage explorer Visual Studio 2015  These notes will be most useful to those that have been programming with R, have a solid knowledge of statistics and machine learning, but have limited exposure to Spark. I don’t assume any Spark background for these notes, and try to explain the Spark concepts from the ground up. I also do not presume that you have used the Microsoft R Server implementation of R, or have used the RevoScaleR package that MRS ships with.   "],
["apache-spark-todo.html", "Chapter 2 Apache Spark - todo 2.1 Functional Programming and Lazy Evaluation 2.2 Distributed Programming Abstractions 2.3 RDDs 2.4 DataFrames 2.5 MLlib 2.6 Spark APIs", " Chapter 2 Apache Spark - todo The core of Apache Spark consists of four components:  Spark SQL Spark Streaming MLlib - Machine Learning Library GraphX - Graphical Computing Library  The Spark project is constantly under development, and the AMPLab is frequently adding new packages and implementing new ideas. An overview of the current state of the ecosystem can be found here: Berkeley Data Analytics Stack  2.1 Functional Programming and Lazy Evaluation In functional programming, all constructs are functions, and all computations are evaluated as function calls (higher-order functions being functions that call upon other functions, think of it as function composition). Pure functional languages (such as Haskell), aim to achieve purity in their evaluation, and therefore rarely change-state or mutate objects, and don’t cause any side-effects. Scala, which composes ~80% of the code in Spark, has full support for functional programming. Although not a pure functional progrmaming langauge like Haskell, it is actually a hybrid of object oriented languages like Java and functional languages like Haskell. Lazy evaluation allows you defer evaluation of a function until it is necessary. This makes it easy to create higher order functions, and then optimize their computation by only evaluation what is necessary. This is particularly useful for data science, where one is often manipulating large amounts of data, filtering through the noise to find the signal, which is where you would want most of your computation to focus on.   2.2 Distributed Programming Abstractions A Spark program consists of two programs, a driver program and a worker program. The driver programs run on the driver machine, whereas the worker program runs on cluster nodes (or in local threads when running on a single machine).   2.3 RDDs RDDs, short for Resilient Distributed Datasets   2.4 DataFrames   2.5 MLlib   2.6 Spark APIs  2.6.1 Scala   2.6.2 PySpark The PySpark API might be the most commonly used API, due to Python’s ubiquity in data science. It is perhaps second in maturity to the Scala API, and when working with DataFrames, provides nearly the same performance as what you would get when writing Scala.   2.6.3 SparkR The SparkR API provides two ways of interacting with Spark through R: a package called SparkR for creating the necessary abstractions in your R session to access Spark’s functionality, and a sparkR shell which loads the library on top of the interactive R shell and creates the necessary SparkContexts automatically. To use the shell, see this quickstart. The examples in these notes use the SparkR library from RStudio, but you could use the SparkR library from any R environment of your choice.    "],
["r-microsoft-r-server-todo.html", "Chapter 3 R &amp; Microsoft R Server - todo 3.1 Functional Programming and Lazy Evaluation in R 3.2 PEMA Algorithms and the RevoScaleR Package 3.3 eXternal Data Frames (XDFs) 3.4 Compute Contexts", " Chapter 3 R &amp; Microsoft R Server - todo  3.1 Functional Programming and Lazy Evaluation in R As we mentioned in Section 2.1, Spark takes advantage of the functional programming paradigm and lazy evaluation to optimize it’s operations and improve upon algorithmic complexity. R is also at it’s heart a functional programming langauge. Moreover, the arguments in a function are evaluated lazily by R: only evaluated if they’re actually used, and only when they’re needed. This allows R to be highly expressive, capable of doing many intricate things with few lines of code, but also causes R to have a rather heavy memory footprint. Many packages for R have been written to take advantage of it’s lazy, and non-standard evaluation procedures. Most famously, the dplyr package utlizes R’s NSE mechanism to have it’s functions connect to backends in different databases, translating R into syntax that can be understood and evaluated by those backends. Thee RevoScaleR similarly reimagines R’s algorithms as distributable C++ code, taht can be optimized and compiled in various compute contexts.   3.2 PEMA Algorithms and the RevoScaleR Package   3.3 eXternal Data Frames (XDFs)   3.4 Compute Contexts   "],
["azure-hdinsight-managed-hadoop-in-the-cloud-todo.html", "Chapter 4 Azure HDInsight – Managed Hadoop in the Cloud - todo 4.1 HDInsight Premium Spark Clusters with R Server 4.2 Dashboards for Management 4.3 Jupyter and RStudio Server", " Chapter 4 Azure HDInsight – Managed Hadoop in the Cloud - todo  4.1 HDInsight Premium Spark Clusters with R Server   4.2 Dashboards for Management   4.3 Jupyter and RStudio Server      "],
["provisioning-instructions.html", "Chapter 5 Provisioning Instructions 5.1 Provision Cluster from Azure Portal 5.2 Installing Packages", " Chapter 5 Provisioning Instructions This module provides a walkthrough of how to provision a Spark cluster on Azure HDInsight Premium with Microsoft R Server, and how to add an edge node with RStudio Server.  5.1 Provision Cluster from Azure Portal The Azure documentation page provides details on how to provision a Spark cluster with Microsoft R Server. The first steps are outlined here: Get started using R Server on HDInsight (preview) I have summarized the steps here to help you get started quickly:  Login to portal.azure.com with your Azure subscription New -&gt; Data + Analytics -&gt; HDInsight Choose Premium cluster: R Server on Spark Create an sshkey, using putty or openSSH, and include the public key in the credentials tab Install RStudio Server on the Edge Node Tunnel into your RStudio Server instance, and start your ML pipeline!    5.2 Installing Packages For packages you only need to run on the edge node, you can continue using install.packages. For packages you need installed on the edge node as well as all the worker nodes, you’ll need to use a script action  5.2.1 todo - install packages demo   "],
["ingestion.html", "Chapter 6 Ingesting Data into Azure Blob Storage - todo 6.1 AzCopy 6.2 Azure Storage Explorer", " Chapter 6 Ingesting Data into Azure Blob Storage - todo Azure HDInsight utilizes low-cost Blob storage as it’s data store. Through a Hadoop distributed file system (HDFS) interface, the full set of components in HDInsight can operate directly on structured or unstructured data in Blob storage. The storage accounts containing your blob storage is separated from your compute environment, allowing you to delete your HDInsight cluster for computation without losing your data, or pointing multiple compute systems to the same data store.  6.1 AzCopy   6.2 Azure Storage Explorer      "],
["starting-your-machine-learning-pipeline.html", "Chapter 7 Starting Your Machine Learning Pipeline 7.1 Finding the SparkR Library 7.2 Creating a Spark Context 7.3 Creating DataFrames", " Chapter 7 Starting Your Machine Learning Pipeline The first steps to start your machine learning and data science pipeline is to set your compute environment, and point to your data. In Spark, you’ll need to create a SparkContext. This constructor provides Spark with the details of the cluster: how and where to access it, additional package directories, etc. You’ll use this constructor to create new RDDs or DataFrames.  7.1 Finding the SparkR Library In order to create a Spark Context from your RStudio Server environment, you’ll need to access the SparkR library. Since Spark 1.4, SparkR has shipped the R API directly with its core implementation. Therefore, since 1.4 you do not need to install Spark from CRAN or a development version from github, but you need to add the SparkR library to your library paths in order to access it. A system variable called “SPARK_HOME” has been set that points to the Spark installation directory, and in it you’ll find subdirectories, “R/lib”, containing the SparkR library. list.files(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;)) ## [1] &quot;SparkR&quot;     &quot;sparkr.zip&quot; To add the SparkR library to your library paths, use the .libPaths function to include the directory in the search path for R’s library tree. The library paths could also be changed from in the Rprofile, either for the user or system wide. See the help on ?StartUp for more details on R’s startup mechanism. .libPaths(c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;), .libPaths()))   7.2 Creating a Spark Context To create a SparkContext, you should use the spark.init function and pass in options for the environment parameters, application properties, any spark packages depended on, and any other additional Spark parameters. In order to create and manipulate DataFrames, we will need a SQLContext, which can be created from the SparkContext using the sparkRSQL.init function. library(SparkR)  sparkEnvir &lt;- list(spark.executor.instance = &#39;10&#39;,                    spark.yarn.executor.memoryOverhead = &#39;8000&#39;)  sc &lt;- sparkR.init(   sparkEnvir = sparkEnvir,   sparkPackages = &quot;com.databricks:spark-csv_2.10:1.3.0&quot; )  sqlContext &lt;- sparkRSQL.init(sc) We added the sparkPackages argument and set it to the value of spark-csv package provided by databricks. This will allow us to read csv objects into Spark DataFrames. Databricks are a prominent vendor for Spark, and are developers of many Spark packages (think of them as the RStudio of the Spark ecosystem). After you are done using your Spark session, you can terminate your backend to Spark by running sparkR.stop().   7.3 Creating DataFrames Using our sqlContext variable, we can create DataFrames from local R data.frames, from Hive tables, or from other data sources. You could create a Spark DataFrame from a local R data.frame using the createDataFrame function,  7.3.1 From Local R data.frames Creating Spark DataFrames from local R data.frames might not seem like a great idea. After all, if it can fit in an R data.frame, that means it can fit in a single node’s memory, so why distribute it? It might be that you are testing your methods out on a sample dataset, and will scale out your analysis to larger datasets when your tests have passed. For expository purposes, this is a useful exercise, as it’ll expose you to the fundamentals of Spark DataFrames, and the SparkR syntax. We’ll import data from the nycflight13 package into a Spark DataFrame, and use it’s data aggregation functions to tabulate the number of flights to each destination of flights originating from JFK library(nycflights13) df &lt;- createDataFrame(sqlContext, nycflights13::flights) jfk_flights &lt;- filter(flights, flights$origin == &quot;JFK&quot;) # Group the flights by destination and aggregate by the number of flights dest_flights &lt;- agg(group_by(jfk_flights, jfk_flights$dest), count = n(jfk_flights$dest)) # Now sort by the `count` column and print the first few rows head(arrange(dest_flights, desc(dest_flights$count))) This same analysis could be streamlined using the %&gt;% operator exposed by the magrittr package: library(magrittr) dest_flights &lt;- filter(flights, flights$origin == &quot;JFK&quot;) %&gt;%    group_by(flights$dest) %&gt;%    summarize(count = n(flights$dest)) dest_flights %&gt;% arrange(desc(dest_flights$count)) %&gt;% head   7.3.2 Creating DataFrames from CSV Files Since we imported the spark-csv package, we can import CSV files into DataFrames. We will be using the full airlines dataset, which I assume you have already ingested into Blob storage by following the steps in Section 6. We have saved in our data_dir a couple of data directories (virtually, everything in Azure Storage is a simple blob!). Let’s see what we have in our directory using the rxHadoopListFiles command, which is simply a wrapper to hadoop shell command hadoop fs -ls data_dir &lt;- &quot;/user/RevoShare/alizaidi&quot; rxHadoopListFiles(data_dir) Taking a look at the individual directories, we can see how many files there are for the Airlines directory and Weather directory rxHadoopCommand(&quot;fs -ls /user/RevoShare/alizaidi/AirOnTimeCSV | head&quot;) rxHadoopCommand(&quot;fs -ls /user/RevoShare/alizaidi/delayDataLarge/WeatherRaw | head&quot;)    "],
["references.html", "Chapter 8 References", " Chapter 8 References     "]
]
