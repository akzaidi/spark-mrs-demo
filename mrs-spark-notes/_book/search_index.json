[
["index.html", "Scalable Machine Learning and Data Science with Microsoft R Server and Spark Chapter 1 Abstract 1.1 Scalable Machine Learning with Microsoft R Server and Spark 1.2 Useful Resources", " Scalable Machine Learning and Data Science with Microsoft R Server and Spark Ali Zaidi, Machine Learning and Data Science, Microsoft 2016-04-21   Chapter 1 Abstract  1.1 Scalable Machine Learning with Microsoft R Server and Spark This collection of notes in progress will help you get started on using Microsoft R Server and Spark with the Azure HDInsight platform.   1.2 Useful Resources There are a number of useful resources on Spark, R, and Azure HDInsight. I have listed a few that I found particularly useful.  1.2.1 Spark  Spark Documentation Home Page The Founding Paper Origination of RDDs MLlib – Machine Learning in Spark Spark Programming Guide Spark Packages Spark Summit Jacek Laskowski - Mastering Apache Spark     1.2.2 Microsoft R Server  Landing Page for R Server Documentation     1.2.3 Azure HDInsight  HDInsight Documentation Home Page Machine Learning with HDInsight Spark R Server on HDInsight     "],
["introduction.html", "Chapter 2 Introduction 2.1 Why R? 2.2 Microsoft R Server FTW 2.3 Apache Spark 2.4 SparkR 2.5 Azure HDInsight 2.6 Prerequisites - What You’ll Need", " Chapter 2 Introduction This book is organized into modules, each of which provide a motivated example of doing data science with R and Spark. The modules are based on notes I created while learning how to make machine learning models scalable, focusing on the tools provided by Microsoft R Server, Azure HDInsight, and Spark.  2.1 Why R? R is a tool of choice for many data scientists. The abundance of available packages for statistical modeling, visualization, and machine learning, coupled with the deep interactivity baked into it’s very foundation, push it to the top of the stack for off-the-shelf languages for data science. Unfortunately, in order to maintain the level of interactivity R provides, it must sacrifice on performance relative to low-level, statically typed languages, which makes it inherantly difficult for R to scale, and inhibits it’s adoption in enterprise.   2.2 Microsoft R Server FTW Microsoft R Server (formerly known as Revolution R Enterprise) was developed to tackle R’s scalability challenges and increase the adoption of the R project in industry. The MRS distribution includes R packages designed for specifically for scalability, exposing new parallel external memory algorithms that interact with data residing in disk or distributed data stores, and a new highly optimized columnar data object, called xdf (short for eXternal Data Frame), that is chunked and especially amenable for parallelization. A data scientist’s coding and debugging time is the most important resource in data science applications, and MRS makes it possible for the data scientist to execute highly performant distributed algorithms on huge amounts of data without ever having to leave their favorite programming environment!   2.3 Apache Spark Developed at the AMPLab at Berkeley, Spark was designed to tackle scalability. Data is growing much faster than Moore’s law for CPUs, so creating commodity computers were not feasible or scalable for the type of data we face today. However, the cost of memory is dropping at a rate that is comparable to the growth of data. While Hadoop revolutionized computing by reintroducing distributed computing through the MapReduce framework, and distributed storage through HDFS, Spark spurred the revolution further by utilizing memory for in-data sharing during interactive Map Reduce jobs. Spark can be 10 - 100 orders of magnitude faster than traditional Map Reduce.   2.4 SparkR Spark has a number of APIs, allowing you to write code in your favorite language to be executed in Spark. The most popular APIs for Spark are Scala and Python. The SparkR API is less mature than the Python and Scala APIs, but provides R with an abstraction to interact with data residing in Spark DataFrames in a manner that looks a lot like manipulating R data.frames, and has a syntax that will be familiar to many R users of the dplyr package.   2.5 Azure HDInsight [todo] add overview of azure HDInsight…   2.6 Prerequisites - What You’ll Need While much of the material in these notes will generalize to other implementations of Spark and R, in order to take complete advantage of everything here you’ll need an Azure subscription, and enough credit in your subscription to provision a Premium Spark HDInsight Cluster. More details on provisioning are provided in the HDInsight chapter. The complete prerequisites (in order of importance):  An Azure subscription A terminal emulator with openSSH or bash, e.g., bash on Linux, Mac Terminal, iTerm2, Putty, or Cygwin/MobaXterm PowerBI Desktop Azure Storage explorer Visual Studio 2015    "],
["apache-spark-todo.html", "Chapter 3 Apache Spark - todo 3.1 Distributed Programming Abstractions 3.2 RDDs 3.3 DataFrames 3.4 MLlib 3.5 Spark APIs", " Chapter 3 Apache Spark - todo The core of Apache Spark consists of four components:  Spark SQL Spark Streaming MLlib - Machine Learning Library GraphX - Graphical Computing Library  The Spark project is constantly under development, and the AMPLab is frequently adding new packages and implementing new ideas. An overview of the current state of the ecosystem can be found here: Berkeley Data Analytics Stack  3.1 Distributed Programming Abstractions A Spark program consists of two programs, a driver program and a worker program. The driver programs run on the driver machine, whereas the worker program runs on cluster nodes (or in local threads when running on a single machine).   3.2 RDDs RDDs, short for Resilient Distributed Datasets   3.3 DataFrames   3.4 MLlib   3.5 Spark APIs  3.5.1 Scala   3.5.2 PySpark The PySpark API might be the most commonly used API, due to Python’s ubiquity in data science. It is perhaps second in maturity to the Scala API, and when working with DataFrames, provides nearly the same performance as what you would get when writing Scala.   3.5.3 SparkR The SparkR API provides two things: a package called SparkR for creating the necessary abstractions in your R session to access Spark’s functionality, and a sparkR shell which loads the library on top of the interactive R shell and creates the necessary SparkContexts automatically. To use the shell, see this quickstart. The examples in these notes use the SparkR library from RStudio, but you could use the SparkR library from an environment of your choice.    "],
["microsoft-r-server-todo.html", "Chapter 4 Microsoft R Server - todo 4.1 PEMA Algorithms and the RevoScaleR Package 4.2 eXternal Data Frames (XDFs) 4.3 Compute Contexts", " Chapter 4 Microsoft R Server - todo  4.1 PEMA Algorithms and the RevoScaleR Package   4.2 eXternal Data Frames (XDFs)   4.3 Compute Contexts   "],
["azure-hdinsight-managed-hadoop-in-the-cloud-todo.html", "Chapter 5 Azure HDInsight – Managed Hadoop in the Cloud - todo 5.1 HDInsight Premium Spark Clusters with R Server 5.2 Dashboards for Management 5.3 Jupyter and RStudio Server", " Chapter 5 Azure HDInsight – Managed Hadoop in the Cloud - todo  5.1 HDInsight Premium Spark Clusters with R Server   5.2 Dashboards for Management   5.3 Jupyter and RStudio Server   "],
["provisioning-instructions.html", "Chapter 6 Provisioning Instructions 6.1 Provision Cluster from Azure Portal 6.2 Installing Packages", " Chapter 6 Provisioning Instructions This module provides a walkthrough of how to provision a Spark cluster on Azure HDInsight Premium with Microsoft R Server, and how to add an edge node with RStudio Server.  6.1 Provision Cluster from Azure Portal The Azure documentation page provides details on how to provision a Spark cluster with Microsoft R Server. The first steps are outlined here: Get started using R Server on HDInsight (preview) I have summarized the steps here to help you get started quickly:  Login to portal.azure.com with your Azure subscription New -&gt; Data + Analytics -&gt; HDInsight Choose Premium cluster: R Server on Spark Create an sshkey, using putty or openSSH, and include the public key in the credentials tab Install RStudio Server on the Edge Node Tunnel into your RStudio Server instance, and start your ML pipeline!    6.2 Installing Packages For packages you only need to run on the edge node, you can continue using install.packages. For packages you need installed on the edge node as well as all the worker nodes, you’ll need to use a script action  6.2.1 todo - install packages demo    "],
["starting-your-machine-learning-pipeline.html", "Chapter 7 Starting Your Machine Learning Pipeline 7.1 Finding SparkR Library in Microsoft R Server 7.2 Creating a Spark Context", " Chapter 7 Starting Your Machine Learning Pipeline The first steps to start your machine learning and data science pipeline is to set your compute environment, and point to your data. In Spark, you’ll need to create a SparkContext. This constructor provides Spark with the details of the cluster: how and where to access it, additional package directories, etc. You’ll use this constructor to create new RDDs or DataFrames.  7.1 Finding SparkR Library in Microsoft R Server In order to create a Spark Context from your RStudio Server environment, you’ll need to access the SparkR library. Since Spark 1.4, SparkR has shipped the R API directly with its core implementation. Therefore, since 1.4 you do not need to install Spark from CRAN or a development version from github, but you need to add the SparkR library to your library paths in order to access it. A system variable called “SPARK_HOME” has been set that points to the Spark installation directory, and in it you’ll find subdirectories, “R/lib”, containing the SparkR library. list.files(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;)) .libPaths(c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;), .libPaths()))   7.2 Creating a Spark Context To create a SparkContext, you should use the spark.init function and pass in options for the environment parameters, any spark packages depended on, and any other additional Spark parameters. In order to create and manipulate DataFrames, we will need a SQLContext, which can be created from the SparkContext. # specify memory environment variables sparkEnvir &lt;- list(spark.executor.instance = &#39;10&#39;,                    spark.yarn.executor.memoryOverhead = &#39;8000&#39;)  sc &lt;- sparkR.init(   sparkEnvir = sparkEnvir,   sparkPackages = &quot;com.databricks:spark-csv_2.10:1.3.0&quot; )  sqlContext &lt;- sparkRSQL.init(sc)   "],
["references.html", "Chapter 8 References", " Chapter 8 References     "]
]
