[
["index.html", "Scalable Machine Learning and Data Science with Microsoft R Server and Spark Abstract Scalable Machine Learning with Microsoft R Server and Spark Useful Resources", " Scalable Machine Learning and Data Science with Microsoft R Server and Spark Ali Zaidi, Machine Learning and Data Science, Microsoft 2016-05-05 Abstract Scalable Machine Learning with Microsoft R Server and Spark This collection of notes in progress will help you get started with using Microsoft R Server and Spark with the Azure HDInsight platform. Useful Resources There are a number of useful resources for Spark, R, and Azure HDInsight. I have listed a few that I found particularly useful. todo: this should probably got into the .bib file instead, and use citations. Spark Spark Documentation Home Page The Founding Paper Origination of RDDs MLlib – Machine Learning in Spark Spark Programming Guide Spark Packages Spark Summit Jacek Laskowski - Mastering Apache Spark SparkR: Scaling R Programs with Spark Microsoft R Server Landing Page for R Server Documentation Azure HDInsight HDInsight Documentation Home Page Machine Learning with HDInsight Spark R Server on HDInsight "],
["introduction.html", "Chapter 1 Introduction 1.1 Why R? 1.2 Microsoft R Server FTW 1.3 Apache Spark 1.4 SparkR 1.5 Azure HDInsight 1.6 Prerequisites - What You’ll Need 1.7 Versioning", " Chapter 1 Introduction This book is organized into modules, each of which provide a motivated example of doing data science with R and Spark. The modules are based on notes I created while learning how to make scalable machine learning pipelines, focusing on the tools provided by Microsoft R Server, Azure HDInsight, and Spark. 1.1 Why R? R is a tool of choice for many data scientists. The abundance of available packages for statistical modeling, visualization, and machine learning, coupled with the deep interactivity baked into it’s very foundation, push it to the top of the stack for off-the-shelf languages for data science. Unfortunately, in order to maintain the level of interactivity R provides, it must sacrifice on performance relative to low-level, statically typed languages, which makes it inherantly difficult for R to scale, and inhibits it’s adoption in enterprise. 1.2 Microsoft R Server FTW Microsoft R Server (formerly known as Revolution R Enterprise) was developed to tackle R’s scalability challenges and increase the adoption of the R project in industry. The MRS distribution includes R packages designed specifically for scalability, exposing new parallel external memory algorithms that interact with data residing in disk or distributed data stores, and a new highly optimized columnar data object, called xdf (short for eXternal Data Frame), that is chunked and especially amenable for parallelization. A data scientist’s coding and debugging time is the most important resource in data science applications, and MRS makes it possible for the data scientist to execute highly performant distributed algorithms on huge amounts of data without ever having to leave their favorite programming environment! 1.3 Apache Spark Developed at the AMPLab at Berkeley, Spark was designed to tackle scalability. Data is growing much faster than Moore’s law for CPUs, so creating commodity computers was not feasible or scalable for the type of data we face today. However, the cost of memory is dropping at a rate that is comparable to the growth of data. While Hadoop revolutionized computing by reintroducing distributed computing through the MapReduce framework, and distributed storage through HDFS, Spark spurred the revolution further by utilizing memory for in-data sharing during interactive Map Reduce jobs. Spark can be 10 - 100 orders of magnitude faster than traditional Map Reduce. 1.4 SparkR Spark has a number of APIs, allowing you to write code in your favorite language to be executed in Spark. The most popular APIs for Spark are Scala and Python. The SparkR API is less mature than the Python and Scala APIs, but provides R with an abstraction to interact with data residing in Spark DataFrames in a manner that looks a lot like manipulating R data.frames, and has a syntax that will be familiar to many R users of the dplyr package (in fact, the Spark DataFrames API was directly influenced by R data.frames and the Python pandas package). While the API shines in data manipulation and aggregation, it is wholly insufficient for modeling and predictive analysis. However, when used in conjuction with the algorithms provided by MRS, developers can build full data science pipelines utilizing the scalability of Spark. 1.5 Azure HDInsight todo add overview of azure HDInsight… 1.6 Prerequisites - What You’ll Need While much of the material in these notes will generalize to other implementations of Spark and R, in order to take complete advantage of everything here you’ll need an Azure subscription, and enough credit in your subscription to provision a Premium Spark HDInsight Cluster. More details on provisioning are provided in the HDInsight chapter. The complete prerequisites (in order of importance): An Azure subscription A terminal emulator with openSSH or bash, e.g., bash on Linux, Mac Terminal, iTerm2, Putty, or Cygwin/MobaXterm PowerBI Desktop Azure Storage explorer Visual Studio 2015 These notes will be most useful to those that have been programming with R, have a solid knowledge of statistics and machine learning, but have limited exposure to Spark. I don’t assume any Spark background for these notes, and try to explain the Spark concepts from the ground up. I also do not presume that you have used the Microsoft R Server implementation of R, or have used the RevoScaleR package that MRS ships with. 1.7 Versioning We will be using HDInsight 3.4, which is running HDP 2.4 and Spark 1.6. The MRS version is Microsoft R Server 8.0.3. The HDInsight cluster consists of 4 worker nodes, two head nodes, and an edge node. Each node conists of 8 cores, and 28 GB of RAM. "],
["apache-spark-1.html", "Chapter 2 Apache Spark 2.1 Functional Programming and Lazy Evaluation 2.2 Distributed Programming Abstractions 2.3 RDDs 2.4 DataFrames 2.5 Datasets 2.6 MLlib 2.7 Spark APIs", " Chapter 2 Apache Spark The core of Apache Spark consists of four components: Spark SQL Spark Streaming MLlib - Machine Learning Library GraphX - Graphical Computing Library The Spark project is constantly under development, and the AMPLab is frequently adding new packages and implementing new ideas. An overview of the current state of the ecosystem can be found here: Berkeley Data Analytics Stack Even though Spark was envisioned as a better way of running iterative jobs on distributed datasets, it is actually complementary to Hadoop and can be placed over the Hadoop file system. This behavior is supported through a third-party clustering framework called Mesos. 2.1 Functional Programming and Lazy Evaluation In functional programming, all constructs are functions, and all computations are evaluated as function calls (higher-order functions being functions that call upon other functions, think of it as function composition). Pure functional languages (such as Haskell), aim to achieve purity in their evaluation, and therefore rarely change-state or mutate objects, and don’t cause any side-effects. Scala, which composes ~80% of the code in Spark, has full support for functional programming. Although not a pure functional progrmaming langauge like Haskell, it is actually a hybrid of object oriented languages like Java and functional languages like Haskell. Lazy evaluation defers computation of a function until it is necessary. This makes it easy to create higher order functions, and then optimize their computation by only evaluation what is necessary, and optimizing the order of computationss. This is particularly useful for data science, where the programmer is often manipulating large amounts of data and creating complex functional pipelines. 2.2 Distributed Programming Abstractions A Spark program consists of two programs, a driver program and a worker program. The driver programs run on the driver machine, whereas the worker program runs on cluster nodes (or in local threads when running on a single machine). The first thing a Spark program is required to do is create a SparkContext object, which tells Spark the details of the cluster. These SparkContexts can be created using constructors through R’s or Python’s API. The most important parameter for a SparkContext is the master parameter, which determines the type and size of the cluster to use: Master Parameter Description local run Spark locally with one worker thread (no parallelism) local[K] run Spark locally with K worker threads (most commonly set to number of available cores) spark://HOST:PORT connect to a Spark standalone cluster: PORT depends on configuration (by default is 7077) mesos://HOST:PORT connect to a Mesos cluster: PORT depends on configuration (by default is 5050) 2.3 RDDs RDDs, short for Resilient Distributed Datasets, are the primary abstraction for data in Spark. They are immutable, so once they are created they cannot be modified. An RDD is a read-only collection of objects distributed across the nodes of the cluster. These collections are called resilienet due to their fault-tolerant nature, as they can be rebuilt if any portion is lost. Spark tracks lineage information of RDDs to efficeintly recompute any lost data due to machine failuers. An RDD is represented as a Scala object and can be created from a file in HDFS or any other storage system; as a parallelized slice (spread across nodes), perhaps by parallelizing Python or R collections (such as lists); as a transformation of another RDD; and finally through changing the persistence of an existing RDD, such as requesting that it be cached in memory. The programmer can specify the number of partitions for an RDD, which when unspecified will utilize the default value. The more partitions used in an RDD, the higher the amount of parallelism the program will achieve (if there are more partitions than worker machines, then some worker machines will have multiple partitions). Spark applications are called drivers, and a driver can perform one of two operations on a dataset: an action and a transformation. An action performs a computation on a dataset and returns a value to the driver; a transformation creates a new dataset from an existing dataset. Transformations are lazily evaluated, and are therefore not computed immediately, but only when an action calls it. Examples of actions include performing a Reduce operation (using a function) and iterating a dataset (running a function on each element, similar to the Map operation). Examples of transformations include Map and Filter operations, and the Cache operation (which saves an RDD to memory or disk). 2.3.1 Common Transformations and Actions Here are some of the most common transformations when working with RDDs: Master Parameter Description map(func) return a new distributed dataset by passing each element of the source through a function func filter(func) return a new dataset by selecting those elementsof the source where func returns TRUE distinct([numTasks]) return a new dataset that contains the distinct elements of the source dataset flatMap(func) similar to map, but instead of mapping to a single value, func returns a sequence These functions will not be evaluated when called, but only when they are passed onto an action. Here are some common actions: Master Parameter Description reduce(func) aggregate elements of the dataset by func. func must take two arguments and returns a singleton take(n) returns an array with the first n elements of the dataset collect() returns all the elements as an array (make sure it can fit in the memory of the driver) takeOrdered(n, key = func) return n elements ordered in ascending order or as specified by the key function The reduce function must be associative and commutative so that it can correctly compute in parallel. Since the introduction of the DataFrames API in Spark 1.3, most data science applications will not need to create and compute with RDDs. In fact, the R API, SparkR does not even export many of the transformations below in the package’s namespace. However, they can still be called by utilizing the triple colon (i.e., SparkR:::filterMap. Here’s a very simple example of creating a filterMap using the SparkR API: library(SparkR) sparkEnvir &lt;- list(spark.executor.instance = &#39;10&#39;, spark.yarn.executor.memoryOverhead = &#39;8000&#39;) sc &lt;- sparkR.init( sparkEnvir = sparkEnvir, sparkPackages = &quot;com.databricks:spark-csv_2.10:1.3.0&quot; ) ## Re-using existing Spark Context. Please stop SparkR with sparkR.stop() or restart R to create a new Spark Context sqlContext &lt;- sparkRSQL.init(sc) rdd &lt;- SparkR:::parallelize(sc, 1:10) multiplyByTwo &lt;- SparkR:::flatMap(rdd, function(x) { list(x*2, x*10) }) results &lt;- collect(multiplyByTwo) unlist(results) ## [1] 2 10 4 20 6 30 8 40 10 50 12 60 14 70 16 80 18 ## [18] 90 20 100 sparkR.stop() 2.4 DataFrames When working with relational data for structured data processing, most data scientists will think of using SQL, due to it’s highly efficient relational algebra. Spark provides a SQL interface with Spark SQL and a SQL context, SQLContext. Spark SQL is a Spark module for structured data processing. While the RDD API is great for generic data storage, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL, the DataFrames API and the Datasets API. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between the various APIs based on which provides the most natural way to express a given transformation. DataFrames use the catalyst query optimizer to make querying more efficient. The catalyst query optimizer leverages advanced Scala features, such as pattern matching and quasiquotes to build an extenisble query optimizer. 2.5 Datasets A Dataset is a new experimental interface added in Spark 1.6 that tries to bring the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine to DataFrames. A Dataset can be constructed from JVM objects and then manipulated using functional transformations. Furthermore, Datasets are an abstraction above DataFrames, a DataFrame is simply a type alias of a Dataset. The unified Dataset API can be used both in Scala and Java. Python does not yet have support for the Dataset API, but due to its dynamic nature many of the benefits are already available (i.e. you can access the field of a row by name naturally row.columnName). Full python support is expected to come in Spark 2.0. 2.6 MLlib MLlib is a scalable machine learning library for Spark. todo: ML Pipelines 2.7 Spark APIs There are numerous high-levels APIs for Spark: Java, Scala, Python and R, and an optimized engine that supports general execution graphs. The Scala, Python and R APIs all ship with HDInsight Spark Premium clusters, and the former two can be accessed via the jupyter kernel, which can be accessed via the jupyter dashboard: https://&lt;&gt;.azurehdinsight.net/jupyter/tree/. 2.7.1 Scala The main Spark shell is a Scala API. It can be accessed directly from the bin directory of your Spark installation, as well as from the Jupyter notebooks running on your HDInsight cluster, where you can also find some demo notebooks illustrating how to use the API. While it is the most advanced and mature of the APIs we discussed in this section, it won’t be as familiar to data scientists who are more likely to know Python and R than Scala. 2.7.2 PySpark The PySpark API might be the most commonly used API, due to Python’s ubiquity in data science. It is perhaps second in maturity to the Scala API, and when working with DataFrames, provides nearly the same performance as what you would get when writing native Spark applications with Scala. The PySpark kernel exposes all the transformation and action functions we described in the section on RDDs above. 2.7.3 SparkR The SparkR API provides two ways of interacting with Spark through R: a package called SparkR for creating the necessary abstractions in your R session to access Spark’s functionality, and a sparkR shell which loads the library on top of the interactive R shell and creates the necessary SparkContexts automatically. To use the shell, see this quickstart. The examples in these notes use the SparkR library from RStudio, but you could use the SparkR library from any R environment of your choice. We describe how to to load the SparkR library in the next section, and how to configure your __.Rprofile_ file to automatically load the package at startup of your R session. As of Spark 1.4, the SparkR API is automatically bundled with Spark. Previous versions of Spark will need to install the library directly from the AMPLab github repository. This requires the devtools package: devtools::install_github('amplab-extras/SparkR-pkg'). "],
["rserver.html", "Chapter 3 R &amp; Microsoft R Server - todo 3.1 Functional Programming and Lazy Evaluation in R 3.2 PEMA Algorithms and the RevoScaleR Package 3.3 eXternal Data Frames (XDFs) 3.4 Compute Contexts", " Chapter 3 R &amp; Microsoft R Server - todo 3.1 Functional Programming and Lazy Evaluation in R As we mentioned in Section 2.1, Spark takes advantage of the functional programming paradigm and lazy evaluation to optimize it’s operations and improve upon algorithmic complexity. R is also at it’s heart a functional programming langauge. Moreover, the arguments in a function are evaluated lazily by R: only evaluated if they’re actually used, and only when they’re needed. This allows R to be highly expressive, capable of doing many intricate things with few lines of code, but also causes R to have a rather heavy memory footprint. Many packages for R have been written to take advantage of it’s lazy, and non-standard evaluation procedures. Most famously, the dplyr package utlizes R’s NSE mechanism to have it’s functions connect to backends in different databases, translating R into syntax that can be understood and evaluated by those backends. Thee RevoScaleR similarly reimagines R’s algorithms as distributable C++ code, taht can be optimized and compiled in various compute contexts. 3.2 PEMA Algorithms and the RevoScaleR Package 3.3 eXternal Data Frames (XDFs) 3.4 Compute Contexts "],
["azure-hdinsight-managed-hadoop-in-the-cloud-todo.html", "Chapter 4 Azure HDInsight – Managed Hadoop in the Cloud - todo 4.1 HDInsight Premium Spark Clusters with R Server 4.2 Dashboards for Management 4.3 Jupyter and RStudio Server", " Chapter 4 Azure HDInsight – Managed Hadoop in the Cloud - todo 4.1 HDInsight Premium Spark Clusters with R Server 4.2 Dashboards for Management 4.3 Jupyter and RStudio Server "],
["provisioning-instructions.html", "Chapter 5 Provisioning Instructions 5.1 Provision Cluster from Azure Portal 5.2 Installing Packages", " Chapter 5 Provisioning Instructions This module provides a walkthrough of how to provision a Spark cluster on Azure HDInsight Premium with Microsoft R Server, and how to add an edge node with RStudio Server. 5.1 Provision Cluster from Azure Portal The Azure documentation page provides details on how to provision a Spark cluster with Microsoft R Server. The first steps are outlined here: Get started using R Server on HDInsight (preview) I have summarized the steps here to help you get started quickly: Login to portal.azure.com with your Azure subscription New -&gt; Data + Analytics -&gt; HDInsight Choose Premium cluster: R Server on Spark Create an sshkey, using putty or openSSH, and include the public key in the credentials tab Install RStudio Server on the Edge Node Tunnel into your RStudio Server instance, and start your ML pipeline! 5.2 Installing Packages For packages you only need to run on the edge node, you can continue using install.packages. For packages you need installed on the edge node as well as all the worker nodes, you’ll need to use a script action 5.2.1 todo - install packages demo "],
["ingestion.html", "Chapter 6 Ingesting Data into Azure Blob Storage - todo 6.1 AzCopy 6.2 Azure Storage Explorer", " Chapter 6 Ingesting Data into Azure Blob Storage - todo Azure HDInsight utilizes low-cost Blob storage as it’s data store. Through a Hadoop distributed file system (HDFS) interface, the full set of components in HDInsight can operate directly on structured or unstructured data in Blob storage. The storage accounts containing your blob storage is separated from your compute environment, allowing you to delete your HDInsight cluster for computation without losing your data, or pointing multiple compute systems to the same data store. 6.1 AzCopy 6.2 Azure Storage Explorer "],
["setting-your-r-profile.html", "Chapter 7 Setting Your R Profile", " Chapter 7 Setting Your R Profile When developing on the RStudio server instance on your Spark HDInsight Cluster, it might be useful to configure your profile so that your R environment can find the SparkR library. This can save some tedious operations that can easily be missed, or mistyped. Here is an example .Rprofile, specifying the location of the SparkR library. If you prefer not to load the SparkR library by default, or change your user Rprofile, you can load the package directly from it’s directory before conducting your analysis. Details of doing this are provided in the following chapter. .First &lt;- function() { .libPaths(c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;), .libPaths())) pkgs_to_load &lt;- c(getOption(&quot;defaultPackages&quot;), &quot;SparkR&quot;) options(defaultPackages = pkgs_to_load) } .Last &lt;- function() { if (interactive()) { hist_file &lt;- Sys.getenv(&quot;R_HISTFILE&quot;) if (hist_file == &quot;&quot;) hist_file &lt;- &quot;~/.RHistory&quot; savehistory(hist_file) } } For more information about your user Rprofile, see the R documentation on startup configurations: help(Startup). "],
["starting-your-machine-learning-pipeline.html", "Chapter 8 Starting Your Machine Learning Pipeline 8.1 todo 8.2 Finding the SparkR Library 8.3 Creating a Spark Context 8.4 Creating DataFrames", " Chapter 8 Starting Your Machine Learning Pipeline 8.1 todo since rmarkdown/knitr start a new session when building docs, can’t access current spark context either build from scratch, or persist rdds/dfs alternatively, make ipynb in jupyter and save as md and render in book as md no console output from rxHadoop shell wrappers, need to sink and show if want to see output to cache results, might make sense to persist DFs and then reuse when needed take a look at sparkr-ext and SKKU-SKT/ggplot2.SparkR The first steps to start your machine learning and data science pipeline is to set your compute environment, and point to your data. In Spark, you’ll need to create a SparkContext. This constructor provides Spark with the details of the cluster: how and where to access it, additional package directories, etc. You’ll use this constructor to create new RDDs or DataFrames. 8.2 Finding the SparkR Library In order to create a Spark Context from your RStudio Server environment, you’ll need to access the SparkR library. Since Spark 1.4, SparkR has shipped the R API directly with its core implementation. Therefore, since 1.4 you do not need to install Spark from CRAN or a development version from github, but you need to add the SparkR library to your library paths in order to access it. A system variable called “SPARK_HOME” has been set that points to the Spark installation directory, and in it you’ll find subdirectories, “R/lib”, containing the SparkR library. list.files(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;)) ## [1] &quot;SparkR&quot; &quot;sparkr.zip&quot; To add the SparkR library to your library paths, use the .libPaths function to include the directory in the search path for R’s library tree. The library paths could also be changed from in the Rprofile, either for the user or system wide. See the help on ?StartUp for more details on R’s startup mechanism. .libPaths(c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;), .libPaths())) 8.3 Creating a Spark Context To create a SparkContext, you should use the spark.init function and pass in options for the environment parameters, application properties, any spark packages depended on, and any other additional Spark parameters. In order to create and manipulate DataFrames, we will need a SQLContext, which can be created from the SparkContext using the sparkRSQL.init function. library(SparkR) sparkEnvir &lt;- list(spark.executor.instance = &#39;10&#39;, spark.yarn.executor.memoryOverhead = &#39;8000&#39;) sc &lt;- sparkR.init( sparkEnvir = sparkEnvir, sparkPackages = &quot;com.databricks:spark-csv_2.10:1.3.0&quot; ) sqlContext &lt;- sparkRSQL.init(sc) We added the sparkPackages argument and set it to the value of spark-csv package provided by databricks. This will allow us to read csv objects into Spark DataFrames. Databricks are a prominent vendor for Spark, and are developers of many Spark packages (think of them as the RStudio of the Spark ecosystem). After you are done using your Spark session, you can terminate your backend to Spark by running sparkR.stop(). 8.4 Creating DataFrames Using our sqlContext variable, we can create DataFrames from local R data.frames, from Hive tables, or from other data sources. You could create a Spark DataFrame from a local R data.frame using the createDataFrame function, 8.4.1 From Local R data.frames Creating Spark DataFrames from local R data.frames might not seem like a great idea. After all, if it can fit in an R data.frame, that means it can fit in a single node’s memory, so why distribute it? It might be that you are testing your methods out on a sample dataset, and will scale out your analysis to larger datasets when your tests have passed. For expository purposes, this is a useful exercise, as it’ll expose you to the fundamentals of Spark DataFrames, and the SparkR syntax. We’ll import data from the nycflight13 package into a Spark DataFrame, and use it’s data aggregation functions to tabulate the number of flights to each destination of flights originating from JFK library(nycflights13) flights &lt;- createDataFrame(sqlContext, nycflights13::flights) jfk_flights &lt;- filter(flights, flights$origin == &quot;JFK&quot;) # Group the flights by destination and aggregate by the number of flights dest_flights &lt;- agg(group_by(jfk_flights, jfk_flights$dest), count = n(jfk_flights$dest)) # Now sort by the `count` column and print the first few rows head(arrange(dest_flights, desc(dest_flights$count))) This same analysis could be streamlined using the %&gt;% operator exposed by the magrittr package: library(magrittr) dest_flights &lt;- filter(flights, flights$origin == &quot;JFK&quot;) %&gt;% group_by(flights$dest) %&gt;% summarize(count = n(flights$dest)) dest_flights %&gt;% arrange(desc(dest_flights$count)) %&gt;% head 8.4.2 Creating DataFrames from CSV Files Since we imported the spark-csv package, we can import CSV files into DataFrames. We will be using the full airlines dataset, which I assume you have already ingested into Blob storage by following the steps in Section 6. We have saved in our data directory a couple of data directories (virtually, everything in Azure Storage is a simple blob!). Let’s see what we have in our directory using the rxHadoopListFiles command, which is simply a wrapper to hadoop shell command hadoop fs -ls data_dir &lt;- &quot;/user/RevoShare/alizaidi&quot; rxHadoopListFiles(data_dir) Taking a look at the individual directories, we can see how many files there are for the Airlines directory and Weather directory rxHadoopCommand(&quot;fs -ls /user/RevoShare/alizaidi/AirOnTimeCSV | head&quot;) rxHadoopCommand(&quot;fs -ls /user/RevoShare/alizaidi/delayDataLarge/WeatherRaw | head&quot;) Let’s read the airlines directory and the weather directory to Spark DataFrames. We will use the read.df function from the spark.csv package. Note that it took more than 6 minutes to load our airlines data into Spark DataFrames. However, subsequent operations on the airDF object will occur in-memory, and should be very fast. Let’s count the number of rows in each of our DataFrames and print the first few rows: library(SparkR) lapply(list(airDF, weatherDF), count) # [[1]] # [1] 148619655 # # [[2]] # [1] 14829028 lapply(list(airDF, weatherDF), head) # [[1]] # YEAR MONTH DAY_OF_MONTH DAY_OF_WEEK FL_DATE UNIQUE_CARRIER TAIL_NUM FL_NUM # 1 1987 10 1 4 1987-10-01 AA 1 # 2 1987 10 2 5 1987-10-02 AA 1 # 3 1987 10 3 6 1987-10-03 AA 1 # 4 1987 10 4 7 1987-10-04 AA 1 # 5 1987 10 5 1 1987-10-05 AA 1 # 6 1987 10 6 2 1987-10-06 AA 1 # ORIGIN_AIRPORT_ID ORIGIN ORIGIN_STATE_ABR DEST_AIRPORT_ID DEST DEST_STATE_ABR # 1 12478 JFK NY 12892 LAX CA # 2 12478 JFK NY 12892 LAX CA # 3 12478 JFK NY 12892 LAX CA # 4 12478 JFK NY 12892 LAX CA # 5 12478 JFK NY 12892 LAX CA # 6 12478 JFK NY 12892 LAX CA # CRS_DEP_TIME DEP_TIME DEP_DELAY DEP_DELAY_NEW DEP_DEL15 DEP_DELAY_GROUP TAXI_OUT # 1 900 901 1 1 0 0 # 2 900 901 1 1 0 0 # 3 900 859 -1 0 0 -1 # 4 900 900 0 0 0 0 # 5 900 902 2 2 0 0 # 6 900 900 0 0 0 0 # WHEELS_OFF WHEELS_ON TAXI_IN CRS_ARR_TIME ARR_TIME ARR_DELAY ARR_DELAY_NEW # 1 1152 1117 -35 0 # 2 1152 1137 -15 0 # 3 1152 1111 -41 0 # 4 1152 1116 -36 0 # 5 1152 1119 -33 0 # 6 1152 NA NA NA # ARR_DEL15 ARR_DELAY_GROUP CANCELLED CANCELLATION_CODE DIVERTED CRS_ELAPSED_TIME # 1 0 -2 0 0 352 # 2 0 -1 0 0 352 # 3 0 -2 0 0 352 # 4 0 -2 0 0 352 # 5 0 -2 0 0 352 # 6 NA NA 0 1 352 # ACTUAL_ELAPSED_TIME AIR_TIME FLIGHTS DISTANCE DISTANCE_GROUP CARRIER_DELAY # 1 316 1 2475 10 # 2 336 1 2475 10 # 3 312 1 2475 10 # 4 316 1 2475 10 # 5 317 1 2475 10 # 6 NA 1 2475 10 # WEATHER_DELAY NAS_DELAY SECURITY_DELAY LATE_AIRCRAFT_DELAY # 1 # 2 # 3 # 4 # 5 # 6 # # [[2]] # Visibility DryBulbCelsius DewPointCelsius RelativeHumidity WindSpeed Altimeter # 1 4.000000 0.000000 -1.000000 92.000000 0.000000 29.690000 # 2 10.000000 7.000000 -3.000000 49.000000 11.000000 29.790000 # 3 3.000000 1.000000 0.000000 92.000000 3.000000 29.710000 # 4 10.000000 7.000000 -5.000000 42.000000 18.000000 29.710000 # 5 1.250000 3.000000 0.000000 82.000000 6.000000 29.720000 # 6 10.000000 8.000000 -4.000000 44.000000 14.000000 29.770000 # AdjustedYear AdjustedMonth AdjustedDay AdjustedHour AirportID # 1 2007 5 4 18 15177 # 2 2007 5 4 6 15177 # 3 2007 5 4 17 15177 # 4 2007 5 4 9 15177 # 5 2007 5 4 10 15177 # 6 2007 5 4 7 15177 "],
["exploratory-data-analysis-with-sparkr.html", "Chapter 9 Exploratory Data Analysis with SparkR 9.1 SparkR the Explorer 9.2 Doing Data Aggregations with SparkR Efficiently 9.3 From Spark DataFrames to Local Dataframes 9.4 Plotting Results", " Chapter 9 Exploratory Data Analysis with SparkR 9.1 SparkR the Explorer SparkR has a limited API for modeling. As of 1.6.1, the only supported modeling function in SparkR is a glm. There are many more available modeling functions in the R Server RevoScaleR library that can be computed using a Spark compute context. We will discuss this in more detail in the subsequent chapters. While limited in modeling, SparkR shows its versatility for data exploration. We show how easy it is to create data exploration pipelines with SparkR and open source R packages. 9.2 Doing Data Aggregations with SparkR Efficiently Since Spark evaluates objects lazily, tremendous speedups can be achieved by putting a little thought into our data analysis pipelines. In ths section, we will analyze the average arrival delay for flights and group them by carrier, origin and destination. These few feature variables are the only ones we need, so we can remove the redundant columns. select_cols &lt;- function(sparkDF = airDF) { library(magrittr) sparkDF %&gt;% SparkR::select(airDF$FL_DATE, airDF$DAY_OF_WEEK, airDF$UNIQUE_CARRIER, airDF$ORIGIN, airDF$DEST, airDF$ARR_DELAY) -&gt; skinny_df skinny_df %&lt;&gt;% SparkR::rename( flight_date = skinny_df$FL_DATE, day_of_week = skinny_df$DAY_OF_WEEK, carrier = skinny_df$UNIQUE_CARRIER, origin = skinny_df$ORIGIN, destination = skinny_df$DEST, arrival_delay = skinny_df$ARR_DELAY ) return(skinny_df) } air_df &lt;- select_cols() The function above creates a transformation on our airlines dataset and renames the columns we need for our analysis. At this point, we don’t have any actions yet, so this is simply a promise the Spark interpreter has given us for later evaluation. 9.3 From Spark DataFrames to Local Dataframes For our next step, we will group by carrier, origin and destination, and calculate the average arrival delay. Our resulting dataframe should be rather condensed, so we will collect our results and save them to a local R data.frame. Observe how simple it is to create a pipeline that starts with a Spark DataFrame and outputs a local data.frame you can interact with locally. agg_delay &lt;- function(airdf = air_df) { library(magrittr) airdf %&gt;% SparkR::group_by(airdf$carrier, airdf$origin, airdf$destination) %&gt;% SparkR::summarize(counts = n(airdf$arrival_delay), ave_delay = mean(airdf$arrival_delay)) -&gt; summary_df return(summary_df) } agg_df &lt;- agg_delay() agg_df_local &lt;- agg_df %&gt;% collect() %&gt;% dplyr::tbl_df save(agg_df_local, file = &quot;aggflightslocal.RData&quot;) If you are familiar with the dplyr grammar of data manipulation, you should be ecstatic by how your knowledge transfers over directly to manipulating Spark dataframes (and you’ll probably just wonder why Spark DataFrames don’t just have a supported backend by dplyr yet). We saved our local data.frame to disk so we don’t have to re-run the Spark datasteps again. 9.4 Plotting Results We deliberately kept the carrier column in our analysis, in case we wanted to visualize or analyze delays by that feature. For now, let us narrow our focus to simply the routes of our flights. # load the agg_df_local calculated above load(&quot;aggflightslocal.RData&quot;) delays_routes &lt;- function(delay_df = agg_df_local) { library(dplyr) delay_df %&gt;% group_by(origin, destination) %&gt;% summarise(total = sum(counts), arrival_delay = weighted.mean(ave_delay, counts)) -&gt; route_delays return(route_delays) } delays_routes() ## Source: local data frame [9,171 x 4] ## Groups: origin [?] ## ## origin destination total arrival_delay ## (chr) (chr) (dbl) (dbl) ## 1 ABE ALB 2 23.0000000 ## 2 ABE ATL 18482 7.4414566 ## 3 ABE AVP 1587 2.3238815 ## 4 ABE AZO 0 NaN ## 5 ABE BDL 1 1.0000000 ## 6 ABE BHM 1 -3.0000000 ## 7 ABE BWI 2502 4.3033573 ## 8 ABE CLE 6331 -2.4669089 ## 9 ABE CLT 8531 0.1807525 ## 10 ABE CVG 6654 0.4804629 ## .. ... ... ... ... Let us make a heatmap of the delays, by picking just a few routes. We will use the ggplot library to make our base plot, and then use the plotly package to make our plot more interactive! library(ggplot2) plot_route_delays &lt;- function(min_routes = 10) { library(dplyr) library(ggplot2) gplot &lt;- delays_routes() %&gt;% filter(total &gt; min_routes) %&gt;% arrange(desc(total)) %&gt;% filter(origin %in% c(&quot;JFK&quot;, &quot;LGA&quot;, &quot;IAD&quot;, &quot;DCA&quot;, &quot;ATL&quot;, &quot;DFW&quot;, &quot;ORD&quot;, &quot;IAH&quot;, &quot;DEN&quot;, &quot;CLT&quot;), destination %in% c(&quot;ATL&quot;, &quot;BDL&quot;, &quot;BOS&quot;, &quot;JFK&quot;, &quot;IAD&quot;, &quot;LGA&quot;, &quot;DCA&quot;, &quot;IAD&quot;, &quot;DFW&quot;, &quot;ORD&quot;, &quot;IAH&quot;, &quot;DEN&quot;, &quot;CLT&quot;)) %&gt;% ggplot(aes(x = origin, y = destination, fill = arrival_delay)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;steelblue&quot;) return(gplot) } gplot &lt;- plot_route_delays(100) + theme_bw() library(plotly) ggplotly(gplot, width = 650) "],
["data-manipulation-with-sparkr.html", "Chapter 10 Data Manipulation with SparkR 10.1 Data Aggregations", " Chapter 10 Data Manipulation with SparkR Now that we have our two datasets saved as Spark DataFrames, we can conduct standard data manipulation techniques to visualize and explore our data. First, we’ll use the rename function to rename our columns, and the select function to select the columns we need. We’ll also transform the These SparkR functions look just like the verbs from the dplyr package for data manipulation, but are designed to work with Spark DataFrames. system.time(airDF &lt;- rename(airDF, ArrDel15 = airDF$ARR_DEL15, Year = airDF$YEAR, Month = airDF$MONTH, DayofMonth = airDF$DAY_OF_MONTH, DayOfWeek = airDF$DAY_OF_WEEK, Carrier = airDF$UNIQUE_CARRIER, OriginAirportID = airDF$ORIGIN_AIRPORT_ID, DestAirportID = airDF$DEST_AIRPORT_ID, CRSDepTime = airDF$CRS_DEP_TIME, CRSArrTime = airDF$CRS_ARR_TIME, Distance = airDF$DISTANCE, DepDelay = airDF$DEP_DELAY, ArrDelay = airDF$ARR_DELAY ) ) # user system elapsed # 0.136 0.000 0.242 # Select desired columns from the flight data. varsToKeep &lt;- c(&quot;ArrDel15&quot;, &quot;Year&quot;, &quot;Month&quot;, &quot;DayofMonth&quot;, &quot;DayOfWeek&quot;, &quot;Carrier&quot;, &quot;OriginAirportID&quot;, &quot;DestAirportID&quot;, &quot;CRSDepTime&quot;, &quot;CRSArrTime&quot;, &quot;Distance&quot;, &quot;DepDelay&quot;, &quot;ArrDelay&quot;) system.time(airDF &lt;- select(airDF, varsToKeep)) # user system elapsed # 0.064 0.000 0.112 # Round down scheduled departure time to full hour. system.time(airDF$CRSDepTime &lt;- floor(airDF$CRSDepTime / 100)) # user system elapsed # 0.00 0.00 0.06 10.1 Data Aggregations SparkR is great at merges, and data aggregation. For instance, suppose we want to see the average departure delay for each carrier and arrange it in descending order. The following example shows just how easy the syntax for SparkR is. sum_df &lt;- airDF %&gt;% select(&quot;Carrier&quot;, &quot;DepDelay&quot;) %&gt;% groupBy(airDF$Carrier) %&gt;% summarize(count = n(airDF$Carrier), ave_delay = mean(airDF$DepDelay)) # user system elapsed # 0.024 0.000 0.055 The syntax is almost exactly like the syntax from the dplyr package, and the %&gt;% operator makes chaining the additive methods exceptionally simple. Note that the above operation will not be run until we call an action upon the sum_df. As of right now it is a series of transformations, so it is a recipe for doing some computations, but the actual evaluation has been deferred until we call an action. In order to evaluate and bring the summarized data into an R data.frame, we can use the collect action. sum_local &lt;- sum_df %&gt;% collect() # user system elapsed # 0.616 0.536 337.758 library(dplyr) sum_local %&gt;% arrange(desc(ave_delay)) Now that our data resides as a local data.frame, we can plot it using any R plotting library. load(&quot;local_df.RData&quot;) library(rcdimple) sum_local %&gt;% dimple(x =&quot;Carrier&quot;, y = &quot;ave_delay&quot;, z = &quot;count&quot;, type = &quot;bar&quot;) %&gt;% add_title(html = &quot;&lt;h4&gt;Average Delay in Minutes by Carrier&lt;/h4&gt;&quot; ) %&gt;% zAxis(outputFormat = &quot;#,### &quot;) In order to make the weather data correspond to the airline data, let us aggregate it by date and airport, and obtain it’s average value. If you are familiar with the dplyr package, you should be very familiar with the syntax provided by SparkR. weatherAgg &lt;- weatherDF %&gt;% SparkR::group_by(&quot;AdjustedYear&quot;, &quot;AdjustedMonth&quot;, &quot;AdjustedDay&quot;, &quot;AdjustedHour&quot;, &quot;AirportID&quot;) %&gt;% SparkR::summarize(Visibility = mean(weatherDF$Visibility), DryBulbCelsius = mean(weatherDF$DryBulbCelsius), DewPointCelsius = mean(weatherDF$DewPointCelsius), RelativeHumidity = mean(weatherDF$RelativeHumidity), WindSpeed = mean(weatherDF$RelativeHumidity), Altimeter = mean(weatherDF$Altimeter)) 10.1.1 Merging Data We can use SparkR for merging data sets as well. Let’s merge the airlines dataset with the weather dataset. We’ll first add weather data to the origination airport, and then add it to the destination airport. To keep our data in manageable size, we will remove the redundant columns. Finally, we save the DataFrame to a CSV file, stored in HDFS so that we may access it later. joinedDF &lt;- SparkR::join( airDF, weatherAgg, airDF$OriginAirportID == weatherAgg$AirportID &amp; airDF$Year == weatherAgg$AdjustedYear &amp; airDF$Month == weatherAgg$AdjustedMonth &amp; airDF$DayofMonth == weatherAgg$AdjustedDay &amp; airDF$CRSDepTime == weatherAgg$AdjustedHour, joinType = &quot;left_outer&quot; ) vars &lt;- names(joinedDF) varsToDrop &lt;- c(&#39;AdjustedYear&#39;, &#39;AdjustedMonth&#39;, &#39;AdjustedDay&#39;, &#39;AdjustedHour&#39;, &#39;AirportID&#39;) varsToKeep &lt;- vars[!(vars %in% varsToDrop)] joinedDF1 &lt;- select(joinedDF, varsToKeep) joinedDF2 &lt;- rename(joinedDF1, VisibilityOrigin = joinedDF1$Visibility, DryBulbCelsiusOrigin = joinedDF1$DryBulbCelsius, DewPointCelsiusOrigin = joinedDF1$DewPointCelsius, RelativeHumidityOrigin = joinedDF1$RelativeHumidity, WindSpeedOrigin = joinedDF1$WindSpeed, AltimeterOrigin = joinedDF1$Altimeter ) joinedDF3 &lt;- join( joinedDF2, weatherAgg, airDF$DestAirportID == weatherAgg$AirportID &amp; airDF$Year == weatherAgg$AdjustedYear &amp; airDF$Month == weatherAgg$AdjustedMonth &amp; airDF$DayofMonth == weatherAgg$AdjustedDay &amp; airDF$CRSDepTime == weatherAgg$AdjustedHour, joinType = &quot;left_outer&quot; ) # Remove redundant columns vars &lt;- names(joinedDF3) varsToDrop &lt;- c(&#39;AdjustedYear&#39;, &#39;AdjustedMonth&#39;, &#39;AdjustedDay&#39;, &#39;AdjustedHour&#39;, &#39;AirportID&#39;) varsToKeep &lt;- vars[!(vars %in% varsToDrop)] joinedDF4 &lt;- select(joinedDF3, varsToKeep) joinedDF5 &lt;- rename(joinedDF4, VisibilityDest = joinedDF4$Visibility, DryBulbCelsiusDest = joinedDF4$DryBulbCelsius, DewPointCelsiusDest = joinedDF4$DewPointCelsius, RelativeHumidityDest = joinedDF4$RelativeHumidity, WindSpeedDest = joinedDF4$WindSpeed, AltimeterDest = joinedDF4$Altimeter ) joinedDF5 &lt;- repartition(joinedDF5, 80) # write result to directory of CSVs write.df(joinedDF5, file.path(&quot;/user/RevoShare/alizaidi/delayDataLarge&quot;, &quot;JoinAirWeatherDelay&quot;), &quot;com.databricks.spark.csv&quot;, &quot;overwrite&quot;, header = &quot;true&quot;) # We can shut down the SparkR Spark context now sparkR.stop() "],
["modeling-with-microsoft-r-server.html", "Chapter 11 Modeling with Microsoft R Server 11.1 Import CSV to XDF 11.2 Splitting XDF into Train and Test Tests 11.3 Training Binary Classification Models 11.4 Testing Models", " Chapter 11 Modeling with Microsoft R Server 11.1 Import CSV to XDF To take full advantage of the PEMA algorithms provided by MRS, we will import the merged data, currently saved as csv in blob storage, into an xdf. We first have some housekeeping items to take care. We need to specify the spark compute context for the RevoScaleR package to properly utlize the Spark cluster. Saving a text file to HDFS creates blocks of the data and saves them in separate directories, and also saves an additional directory entitled “_SUCCESS&quot; to indicate the import operation was successful. We need to remove this file before importing to xdf, as it has no value for the final data. Further, in order to make sure the MRS modeling functions respect the data types of the columns in our merged dataset, we need to provide it with some column metadata. This can be provided with the colInfo argument inside of rxImport. Lastly, we need to provide MRS with pointers to the HDFS store we will be saving our XDF to. rxOptions(fileSystem = RxHdfsFileSystem(), reportProgress = 0) dataDir &lt;- &quot;/user/RevoShare/alizaidi/delayDataLarge&quot; if(rxOptions()$hdfsHost == &quot;default&quot;) { fullDataDir &lt;- dataDir } else { fullDataDir &lt;- paste0(rxOptions()$hdfsHost, dataDir) } computeContext &lt;- RxSpark(consoleOutput = TRUE) # there&#39;s a folder called SUCCESS_ that we need to delete manually file_to_delete &lt;- file.path(data_dir, &quot;delayDataLarge&quot;, &quot;JoinAirWeatherDelay&quot;, &quot;_SUCCESS&quot;) delete_command &lt;- paste(&quot;fs -rm&quot;, file_to_delete) rxHadoopCommand(delete_command) colInfo &lt;- list( ArrDel15 = list(type=&quot;numeric&quot;), Year = list(type=&quot;factor&quot;), Month = list(type=&quot;factor&quot;), DayofMonth = list(type=&quot;factor&quot;), DayOfWeek = list(type=&quot;factor&quot;), Carrier = list(type=&quot;factor&quot;), OriginAirportID = list(type=&quot;factor&quot;), DestAirportID = list(type=&quot;factor&quot;), RelativeHumidityOrigin = list(type=&quot;numeric&quot;), AltimeterOrigin = list(type=&quot;numeric&quot;), DryBulbCelsiusOrigin = list(type=&quot;numeric&quot;), WindSpeedOrigin = list(type=&quot;numeric&quot;), VisibilityOrigin = list(type=&quot;numeric&quot;), DewPointCelsiusOrigin = list(type=&quot;numeric&quot;), RelativeHumidityDest = list(type=&quot;numeric&quot;), AltimeterDest = list(type=&quot;numeric&quot;), DryBulbCelsiusDest = list(type=&quot;numeric&quot;), WindSpeedDest = list(type=&quot;numeric&quot;), VisibilityDest = list(type=&quot;numeric&quot;), DewPointCelsiusDest = list(type=&quot;numeric&quot;), CRSDepTime = list(type = &quot;numeric&quot;), CRSArrTime = list(type = &quot;numeric&quot;), DepDelay = list(type = &quot;numeric&quot;), ArrDelay = list(type = &quot;numeric&quot;) ) myNameNode &lt;- &quot;default&quot; myPort &lt;- 0 hdfsFS &lt;- RxHdfsFileSystem(hostName = myNameNode, port = myPort) joined_txt &lt;- RxTextData(file.path(data_dir, &quot;delayDataLarge&quot;, &quot;JoinAirWeatherDelay&quot;), colInfo = colInfo, fileSystem = hdfsFS) dest_xdf &lt;- RxXdfData(file.path(data_dir, &quot;delayDataLarge&quot;, &quot;joinedAirWeatherXdf&quot;), fileSystem = hdfsFS) rxImport(inData = joined_txt, dest_xdf, overwrite = TRUE) Now that we have imported our data to an XDF, we can get some information about the variables: rxGetInfo(RxXdfData(file.path(data_dir, &quot;delayDataLarge&quot;, &quot;joinedAirWeatherXdf&quot;), fileSystem = hdfsFS), getVarInfo = T, numRows = 2) ## File name: /user/RevoShare/alizaidi/delayDataLarge/joinedAirWeatherXdf ## Number of composite data files: 80 ## Number of observations: 148619655 ## Number of variables: 22 ## Number of blocks: 320 ## Compression type: zlib ## Variable information: ## Var 1: ArrDel15, Type: numeric, Low/High: (0.0000, 1.0000) ## Var 2: Year ## 26 factor levels: 1990 1992 1994 1997 1999 ... 2005 2008 2010 2011 2012 ## Var 3: Month ## 12 factor levels: 3 8 9 2 5 ... 11 6 12 7 10 ## Var 4: DayofMonth ## 31 factor levels: 11 23 9 26 4 ... 12 8 27 25 22 ## Var 5: DayOfWeek ## 7 factor levels: 7 5 3 2 4 1 6 ## Var 6: Carrier ## 30 factor levels: US UA DL NW DH ... PA (1) KH HA PS VX ## Var 7: OriginAirportID ## 374 factor levels: 10821 13930 11057 13230 11433 ... 10559 13341 14314 11931 10558 ## Var 8: DestAirportID ## 378 factor levels: 10135 10136 10140 10146 10155 ... 10559 11931 10894 14475 12899 ## Var 9: CRSDepTime, Type: numeric, Low/High: (0.0000, 24.0000) ## Var 10: CRSArrTime, Type: numeric, Low/High: (0.0000, 2400.0000) ## Var 11: RelativeHumidityOrigin, Type: numeric, Low/High: (0.0000, 100.0000) ## Var 12: AltimeterOrigin, Type: numeric, Low/High: (28.1700, 31.1600) ## Var 13: DryBulbCelsiusOrigin, Type: numeric, Low/High: (-46.1000, 47.2000) ## Var 14: WindSpeedOrigin, Type: numeric, Low/High: (0.0000, 81.0000) ## Var 15: VisibilityOrigin, Type: numeric, Low/High: (0.0000, 88.0000) ## Var 16: DewPointCelsiusOrigin, Type: numeric, Low/High: (-41.7000, 29.0000) ## Var 17: RelativeHumidityDest, Type: numeric, Low/High: (0.0000, 100.0000) ## Var 18: AltimeterDest, Type: numeric, Low/High: (28.1700, 31.1600) ## Var 19: DryBulbCelsiusDest, Type: numeric, Low/High: (-46.1000, 53.9000) ## Var 20: WindSpeedDest, Type: numeric, Low/High: (0.0000, 63.0000) ## Var 21: VisibilityDest, Type: numeric, Low/High: (0.0000, 88.0000) ## Var 22: DewPointCelsiusDest, Type: numeric, Low/High: (-43.0000, 29.0000) ## Data (2 rows starting with row 1): ## ArrDel15 Year Month DayofMonth DayOfWeek Carrier OriginAirportID ## 1 0 1990 3 11 7 US 10821 ## 2 1 1992 8 23 7 UA 13930 ## DestAirportID CRSDepTime CRSArrTime RelativeHumidityOrigin ## 1 10135 10 1056 NA ## 2 10135 6 928 NA ## AltimeterOrigin DryBulbCelsiusOrigin WindSpeedOrigin VisibilityOrigin ## 1 NA NA NA NA ## 2 NA NA NA NA ## DewPointCelsiusOrigin RelativeHumidityDest AltimeterDest ## 1 NA NA NA ## 2 NA NA NA ## DryBulbCelsiusDest WindSpeedDest VisibilityDest DewPointCelsiusDest ## 1 NA NA NA NA ## 2 NA NA NA NA 11.2 Splitting XDF into Train and Test Tests Prior to estimating our predictive models, we need to split our dataset into a training set, which we’ll use for estimation, and a test set that we’ll use for validating our results. Since we have time series data (data ordered by time), we will split our data by time. We’ll use the data prior to 2012 for training, and the data in 2012 for testing. trainDS &lt;- RxXdfData( file.path(dataDir, &quot;finalDataTrain&quot; ), fileSystem = hdfsFS) rxDataStep( inData = dest_xdf, outFile = trainDS, rowSelection = ( Year != 2012 ), overwrite = T ) testDS &lt;- RxXdfData( file.path(dataDir, &quot;finalDataTest&quot; ), fileSystem = hdfsFS) rxDataStep( inData = dest_xdf, outFile = testDS, rowSelection = ( Year == 2012 ), overwrite = T ) 11.3 Training Binary Classification Models Now that we have our train and test sets, we can estimate our predictive model. Let’s try to predict the probability that a flight will be delayed as a function of other variables. 11.3.1 Logistic Regression Models RevoScaleR provides a highly optimized logistic regression model based on the Iteratively Reweighted Least Squares (IRLS) algorithm, which can be called using the rxLogit function. The rxLogit function looks nearly identical to the standard logistic regression function provided by the glm function in the base stats package, taking a formula as it’s first argument, and the data as it’s second argument. We create a handy function make_formula for creating formula objects based on the variables in the all_vars argument of the function. make_formula &lt;- function(resp_var, vars_exclude, all_vars) { features &lt;- all_vars[!(all_vars %in% c(resp_var, vars_exclude))] form &lt;- as.formula(paste(resp_var, paste0(features, collapse = &quot; + &quot;), sep = &quot; ~ &quot;)) return(form) } data_names &lt;- rxGetVarNames(trainDS) form &lt;- make_formula(&quot;ArrDel15&quot;, c(&quot;DepDelay&quot;, &quot;ArrDelay&quot;), data_names) system.time(logitModel &lt;- rxLogit(form, data = trainDS)) # user system elapsed # 15.916 17.068 302.806 base::summary(logitModel) 11.3.2 Tree and Ensemble Classifiers Training the logistic regression model on the full training set took about five minutes. Logistic regression models are frequently used for classification problems due to their interpability and extensibility. However, without adequeate feature engineering, logistic regression models tend to lack the expressiveness and predictive power of ensemble methods, such as boosted trees, or random forests. Using the same methodology as above, we could estimate decision trees and decision forests (random forests) just as easily with the same formula: system.time(dTreeModel &lt;- rxDTree(form, data = trainDS, maxDepth = 6, pruneCp = &quot;auto&quot;)) # user system elapsed # 29.088 67.940 1265.633 save(dTreeModel, file = &quot;dTreeModel.RData&quot;) 11.4 Testing Models Now that we have estimated our models, we can test them on the unseen dataset, which is the dataset from the year 2012. The RevoScaleR package is equipped with a predict function that allows you to score a rx model on a new dataset. load(&quot;testModels.RData&quot;) treePredict &lt;- RxXdfData(file.path(dataDir, &quot;treePredict&quot;), fileSystem = hdfsFS) system.time(rxPredict(dTreeModel, data = testDS, outData = treePredict, extraVarsToWrite = c(&quot;ArrDel15&quot;), overwrite = TRUE)) # user system elapsed # 13.436 3.616 142.326 logitPredict &lt;- RxXdfData(file.path(dataDir, &quot;logitPredict&quot;), fileSystem = hdfsFS) rxPredict(logitModel, data = testDS, outData = logitPredict, extraVarsToWrite = c(&quot;ArrDel15&quot;), type = &#39;response&#39;, overwrite = TRUE) Using our predicted results, we can calculate the accuracy and recall of our model. A very simple way of viewing model’s accuracy is by way of a ROC curve, which plots the recall and accuracy. The metric from this chart, AUC, is a useful validation metric for viewing your moels’ accuracy. # Calculate ROC and Area Under the Curve (AUC). load(&quot;testModels.RData&quot;) logitRoc &lt;- rxRoc(&quot;ArrDel15&quot;, &quot;ArrDel15_Pred&quot;, logitPredict) logitAuc &lt;- rxAuc(logitRoc) plot(logitRoc) # Calculate ROC and Area Under the Curve (AUC) treeRoc &lt;- rxRoc(&quot;ArrDel15&quot;, &quot;ArrDel15_Pred&quot;, treePredict) treeAuc &lt;- rxAuc(treeRoc) plot(treeRoc) "],
["references.html", "Chapter 12 References", " Chapter 12 References "]
]
