[
["index.html", "Scalable Machine Learning and Data Science with Microsoft R Server and Spark Abstract Scalable Machine Learning with Microsoft R Server and Spark Useful Resources", " Scalable Machine Learning and Data Science with Microsoft R Server and Spark Ali Zaidi, Machine Learning and Data Science, Microsoft 2016-04-22   Abstract  Scalable Machine Learning with Microsoft R Server and Spark This collection of notes in progress will help you get started with using Microsoft R Server and Spark with the Azure HDInsight platform.   Useful Resources There are a number of useful resources for Spark, R, and Azure HDInsight. I have listed a few that I found particularly useful. todo: this should probably got into the .bib file instead, and use citations.  Spark  Spark Documentation Home Page The Founding Paper Origination of RDDs MLlib – Machine Learning in Spark Spark Programming Guide Spark Packages Spark Summit Jacek Laskowski - Mastering Apache Spark SparkR: Scaling R Programs with Spark    Microsoft R Server  Landing Page for R Server Documentation     Azure HDInsight  HDInsight Documentation Home Page Machine Learning with HDInsight Spark R Server on HDInsight        "],
["introduction.html", "Chapter 1 Introduction 1.1 Why R? 1.2 Microsoft R Server FTW 1.3 Apache Spark 1.4 SparkR 1.5 Azure HDInsight 1.6 Prerequisites - What You’ll Need 1.7 Versioning", " Chapter 1 Introduction This book is organized into modules, each of which provide a motivated example of doing data science with R and Spark. The modules are based on notes I created while learning how to make scalable machine learning pipelines, focusing on the tools provided by Microsoft R Server, Azure HDInsight, and Spark.  1.1 Why R? R is a tool of choice for many data scientists. The abundance of available packages for statistical modeling, visualization, and machine learning, coupled with the deep interactivity baked into it’s very foundation, push it to the top of the stack for off-the-shelf languages for data science. Unfortunately, in order to maintain the level of interactivity R provides, it must sacrifice on performance relative to low-level, statically typed languages, which makes it inherantly difficult for R to scale, and inhibits it’s adoption in enterprise.   1.2 Microsoft R Server FTW Microsoft R Server (formerly known as Revolution R Enterprise) was developed to tackle R’s scalability challenges and increase the adoption of the R project in industry. The MRS distribution includes R packages designed specifically for scalability, exposing new parallel external memory algorithms that interact with data residing in disk or distributed data stores, and a new highly optimized columnar data object, called xdf (short for eXternal Data Frame), that is chunked and especially amenable for parallelization. A data scientist’s coding and debugging time is the most important resource in data science applications, and MRS makes it possible for the data scientist to execute highly performant distributed algorithms on huge amounts of data without ever having to leave their favorite programming environment!   1.3 Apache Spark Developed at the AMPLab at Berkeley, Spark was designed to tackle scalability. Data is growing much faster than Moore’s law for CPUs, so creating commodity computers were not feasible or scalable for the type of data we face today. However, the cost of memory is dropping at a rate that is comparable to the growth of data. While Hadoop revolutionized computing by reintroducing distributed computing through the MapReduce framework, and distributed storage through HDFS, Spark spurred the revolution further by utilizing memory for in-data sharing during interactive Map Reduce jobs. Spark can be 10 - 100 orders of magnitude faster than traditional Map Reduce.   1.4 SparkR Spark has a number of APIs, allowing you to write code in your favorite language to be executed in Spark. The most popular APIs for Spark are Scala and Python. The SparkR API is less mature than the Python and Scala APIs, but provides R with an abstraction to interact with data residing in Spark DataFrames in a manner that looks a lot like manipulating R data.frames, and has a syntax that will be familiar to many R users of the dplyr package.   1.5 Azure HDInsight todo add overview of azure HDInsight…   1.6 Prerequisites - What You’ll Need While much of the material in these notes will generalize to other implementations of Spark and R, in order to take complete advantage of everything here you’ll need an Azure subscription, and enough credit in your subscription to provision a Premium Spark HDInsight Cluster. More details on provisioning are provided in the HDInsight chapter. The complete prerequisites (in order of importance):  An Azure subscription A terminal emulator with openSSH or bash, e.g., bash on Linux, Mac Terminal, iTerm2, Putty, or Cygwin/MobaXterm PowerBI Desktop Azure Storage explorer Visual Studio 2015  These notes will be most useful to those that have been programming with R, have a solid knowledge of statistics and machine learning, but have limited exposure to Spark. I don’t assume any Spark background for these notes, and try to explain the Spark concepts from the ground up. I also do not presume that you have used the Microsoft R Server implementation of R, or have used the RevoScaleR package that MRS ships with.   1.7 Versioning We will be using HDInsight 3.4, which is running HDP 2.4 and Spark 1.6. The MRS version is Microsoft R Server 8.0.3. The HDInsight cluster consists of 4 worker nodes, two head nodes, and an edge node. Each node conists of 8 cores, and 28 GB of RAM.   "],
["apache-spark-todo.html", "Chapter 2 Apache Spark - todo 2.1 Functional Programming and Lazy Evaluation 2.2 Distributed Programming Abstractions 2.3 RDDs 2.4 DataFrames 2.5 MLlib 2.6 Spark APIs", " Chapter 2 Apache Spark - todo The core of Apache Spark consists of four components:  Spark SQL Spark Streaming MLlib - Machine Learning Library GraphX - Graphical Computing Library  The Spark project is constantly under development, and the AMPLab is frequently adding new packages and implementing new ideas. An overview of the current state of the ecosystem can be found here: Berkeley Data Analytics Stack  2.1 Functional Programming and Lazy Evaluation In functional programming, all constructs are functions, and all computations are evaluated as function calls (higher-order functions being functions that call upon other functions, think of it as function composition). Pure functional languages (such as Haskell), aim to achieve purity in their evaluation, and therefore rarely change-state or mutate objects, and don’t cause any side-effects. Scala, which composes ~80% of the code in Spark, has full support for functional programming. Although not a pure functional progrmaming langauge like Haskell, it is actually a hybrid of object oriented languages like Java and functional languages like Haskell. Lazy evaluation allows you defer evaluation of a function until it is necessary. This makes it easy to create higher order functions, and then optimize their computation by only evaluation what is necessary. This is particularly useful for data science, where one is often manipulating large amounts of data, filtering through the noise to find the signal, which is where you would want most of your computation to focus on.   2.2 Distributed Programming Abstractions A Spark program consists of two programs, a driver program and a worker program. The driver programs run on the driver machine, whereas the worker program runs on cluster nodes (or in local threads when running on a single machine).   2.3 RDDs RDDs, short for Resilient Distributed Datasets   2.4 DataFrames When working with relational data for structured data processing, most data scientists will think of using SQL, due to it’s highly efficient relational algebra. Spark provides a SQL interface with Spark SQL and a SQL context, SQLContext. Spark SQL is a Spark module for structured data processing. While the RDD API is great for generic data storage, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL, the DataFrames API and the Datasets API. When computing a result the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between the various APIs based on which provides the most natural way to express a given transformation. DataFrames use the catalyst query optimizer to make querying more efficient. The catalyst query optimizer leverages advanced Scala features, such as pattern matching and quasiquotes to build an extenisble query optimizer.   2.5 MLlib   2.6 Spark APIs  2.6.1 Scala   2.6.2 PySpark The PySpark API might be the most commonly used API, due to Python’s ubiquity in data science. It is perhaps second in maturity to the Scala API, and when working with DataFrames, provides nearly the same performance as what you would get when writing Scala.   2.6.3 SparkR The SparkR API provides two ways of interacting with Spark through R: a package called SparkR for creating the necessary abstractions in your R session to access Spark’s functionality, and a sparkR shell which loads the library on top of the interactive R shell and creates the necessary SparkContexts automatically. To use the shell, see this quickstart. The examples in these notes use the SparkR library from RStudio, but you could use the SparkR library from any R environment of your choice.    "],
["rserver.html", "Chapter 3 R &amp; Microsoft R Server - todo 3.1 Functional Programming and Lazy Evaluation in R 3.2 PEMA Algorithms and the RevoScaleR Package 3.3 eXternal Data Frames (XDFs) 3.4 Compute Contexts", " Chapter 3 R &amp; Microsoft R Server - todo  3.1 Functional Programming and Lazy Evaluation in R As we mentioned in Section 2.1, Spark takes advantage of the functional programming paradigm and lazy evaluation to optimize it’s operations and improve upon algorithmic complexity. R is also at it’s heart a functional programming langauge. Moreover, the arguments in a function are evaluated lazily by R: only evaluated if they’re actually used, and only when they’re needed. This allows R to be highly expressive, capable of doing many intricate things with few lines of code, but also causes R to have a rather heavy memory footprint. Many packages for R have been written to take advantage of it’s lazy, and non-standard evaluation procedures. Most famously, the dplyr package utlizes R’s NSE mechanism to have it’s functions connect to backends in different databases, translating R into syntax that can be understood and evaluated by those backends. Thee RevoScaleR similarly reimagines R’s algorithms as distributable C++ code, taht can be optimized and compiled in various compute contexts.   3.2 PEMA Algorithms and the RevoScaleR Package   3.3 eXternal Data Frames (XDFs)   3.4 Compute Contexts   "],
["azure-hdinsight-managed-hadoop-in-the-cloud-todo.html", "Chapter 4 Azure HDInsight – Managed Hadoop in the Cloud - todo 4.1 HDInsight Premium Spark Clusters with R Server 4.2 Dashboards for Management 4.3 Jupyter and RStudio Server", " Chapter 4 Azure HDInsight – Managed Hadoop in the Cloud - todo  4.1 HDInsight Premium Spark Clusters with R Server   4.2 Dashboards for Management   4.3 Jupyter and RStudio Server      "],
["provisioning-instructions.html", "Chapter 5 Provisioning Instructions 5.1 Provision Cluster from Azure Portal 5.2 Installing Packages", " Chapter 5 Provisioning Instructions This module provides a walkthrough of how to provision a Spark cluster on Azure HDInsight Premium with Microsoft R Server, and how to add an edge node with RStudio Server.  5.1 Provision Cluster from Azure Portal The Azure documentation page provides details on how to provision a Spark cluster with Microsoft R Server. The first steps are outlined here: Get started using R Server on HDInsight (preview) I have summarized the steps here to help you get started quickly:  Login to portal.azure.com with your Azure subscription New -&gt; Data + Analytics -&gt; HDInsight Choose Premium cluster: R Server on Spark Create an sshkey, using putty or openSSH, and include the public key in the credentials tab Install RStudio Server on the Edge Node Tunnel into your RStudio Server instance, and start your ML pipeline!    5.2 Installing Packages For packages you only need to run on the edge node, you can continue using install.packages. For packages you need installed on the edge node as well as all the worker nodes, you’ll need to use a script action  5.2.1 todo - install packages demo   "],
["ingestion.html", "Chapter 6 Ingesting Data into Azure Blob Storage - todo 6.1 AzCopy 6.2 Azure Storage Explorer", " Chapter 6 Ingesting Data into Azure Blob Storage - todo Azure HDInsight utilizes low-cost Blob storage as it’s data store. Through a Hadoop distributed file system (HDFS) interface, the full set of components in HDInsight can operate directly on structured or unstructured data in Blob storage. The storage accounts containing your blob storage is separated from your compute environment, allowing you to delete your HDInsight cluster for computation without losing your data, or pointing multiple compute systems to the same data store.  6.1 AzCopy   6.2 Azure Storage Explorer      "],
["starting-your-machine-learning-pipeline.html", "Chapter 7 Starting Your Machine Learning Pipeline 7.1 todo 7.2 Finding the SparkR Library 7.3 Creating a Spark Context 7.4 Creating DataFrames", " Chapter 7 Starting Your Machine Learning Pipeline  7.1 todo  since rmarkdown/knitr start a new session when building docs, can’t access current spark context either build from scratch, or persist rdds/dfs alternatively, make ipynb in jupyter and save as md and render in book as md no console output from rxHadoop shell wrappers, need to sink and show if want to see output to cache results, might make sense to persist DFs and then reuse when needed take a look at sparkr-ext and SKKU-SKT/ggplot2.SparkR  The first steps to start your machine learning and data science pipeline is to set your compute environment, and point to your data. In Spark, you’ll need to create a SparkContext. This constructor provides Spark with the details of the cluster: how and where to access it, additional package directories, etc. You’ll use this constructor to create new RDDs or DataFrames.   7.2 Finding the SparkR Library In order to create a Spark Context from your RStudio Server environment, you’ll need to access the SparkR library. Since Spark 1.4, SparkR has shipped the R API directly with its core implementation. Therefore, since 1.4 you do not need to install Spark from CRAN or a development version from github, but you need to add the SparkR library to your library paths in order to access it. A system variable called “SPARK_HOME” has been set that points to the Spark installation directory, and in it you’ll find subdirectories, “R/lib”, containing the SparkR library. list.files(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;)) ## [1] &quot;SparkR&quot;     &quot;sparkr.zip&quot; To add the SparkR library to your library paths, use the .libPaths function to include the directory in the search path for R’s library tree. The library paths could also be changed from in the Rprofile, either for the user or system wide. See the help on ?StartUp for more details on R’s startup mechanism. .libPaths(c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;), .libPaths()))   7.3 Creating a Spark Context To create a SparkContext, you should use the spark.init function and pass in options for the environment parameters, application properties, any spark packages depended on, and any other additional Spark parameters. In order to create and manipulate DataFrames, we will need a SQLContext, which can be created from the SparkContext using the sparkRSQL.init function. library(SparkR)  sparkEnvir &lt;- list(spark.executor.instance = &#39;10&#39;,                    spark.yarn.executor.memoryOverhead = &#39;8000&#39;)  sc &lt;- sparkR.init(   sparkEnvir = sparkEnvir,   sparkPackages = &quot;com.databricks:spark-csv_2.10:1.3.0&quot; )  sqlContext &lt;- sparkRSQL.init(sc) We added the sparkPackages argument and set it to the value of spark-csv package provided by databricks. This will allow us to read csv objects into Spark DataFrames. Databricks are a prominent vendor for Spark, and are developers of many Spark packages (think of them as the RStudio of the Spark ecosystem). After you are done using your Spark session, you can terminate your backend to Spark by running sparkR.stop().   7.4 Creating DataFrames Using our sqlContext variable, we can create DataFrames from local R data.frames, from Hive tables, or from other data sources. You could create a Spark DataFrame from a local R data.frame using the createDataFrame function,  7.4.1 From Local R data.frames Creating Spark DataFrames from local R data.frames might not seem like a great idea. After all, if it can fit in an R data.frame, that means it can fit in a single node’s memory, so why distribute it? It might be that you are testing your methods out on a sample dataset, and will scale out your analysis to larger datasets when your tests have passed. For expository purposes, this is a useful exercise, as it’ll expose you to the fundamentals of Spark DataFrames, and the SparkR syntax. We’ll import data from the nycflight13 package into a Spark DataFrame, and use it’s data aggregation functions to tabulate the number of flights to each destination of flights originating from JFK library(nycflights13) flights &lt;- createDataFrame(sqlContext, nycflights13::flights) jfk_flights &lt;- filter(flights, flights$origin == &quot;JFK&quot;) # Group the flights by destination and aggregate by the number of flights dest_flights &lt;- agg(group_by(jfk_flights, jfk_flights$dest), count = n(jfk_flights$dest)) # Now sort by the `count` column and print the first few rows head(arrange(dest_flights, desc(dest_flights$count))) This same analysis could be streamlined using the %&gt;% operator exposed by the magrittr package: library(magrittr) dest_flights &lt;- filter(flights, flights$origin == &quot;JFK&quot;) %&gt;%    group_by(flights$dest) %&gt;%    summarize(count = n(flights$dest)) dest_flights %&gt;% arrange(desc(dest_flights$count)) %&gt;% head   7.4.2 Creating DataFrames from CSV Files Since we imported the spark-csv package, we can import CSV files into DataFrames. We will be using the full airlines dataset, which I assume you have already ingested into Blob storage by following the steps in Section 6. We have saved in our data directory a couple of data directories (virtually, everything in Azure Storage is a simple blob!). Let’s see what we have in our directory using the rxHadoopListFiles command, which is simply a wrapper to hadoop shell command hadoop fs -ls data_dir &lt;- &quot;/user/RevoShare/alizaidi&quot; rxHadoopListFiles(data_dir) Taking a look at the individual directories, we can see how many files there are for the Airlines directory and Weather directory rxHadoopCommand(&quot;fs -ls /user/RevoShare/alizaidi/AirOnTimeCSV | head&quot;) rxHadoopCommand(&quot;fs -ls /user/RevoShare/alizaidi/delayDataLarge/WeatherRaw | head&quot;) Let’s read the airlines directory and the weather directory to Spark DataFrames. We will use the read.df function from the spark.csv package. airPath &lt;- file.path(data_dir, &quot;AirOnTimeCSV&quot;) weatherPath &lt;- file.path(data_dir, &quot;delayDataLarge&quot;, &quot;Weather&quot;) # pre-processed weather data  airDF &lt;- read.df(sqlContext, airPath, source = &quot;com.databricks.spark.csv&quot;,                   header = &quot;true&quot;, inferSchema = &quot;true&quot;) # user  system elapsed  # 0.724   0.680 378.561   weatherDF &lt;- read.df(sqlContext, weatherPath, source = &quot;com.databricks.spark.csv&quot;,                       header = &quot;true&quot;, inferSchema = &quot;true&quot;) Note that it took more than 6 minutes to load our airlines data into Spark DataFrames. However, subsequent operations on the airDF object will occur in-memory, and should be very fast. Let’s count the number of rows in each of our DataFrames and print the first few rows: library(SparkR) lapply(list(airDF, weatherDF), count) # [[1]] # [1] 148619655 #  # [[2]] # [1] 14829028  lapply(list(airDF, weatherDF), head) # [[1]] #   YEAR MONTH DAY_OF_MONTH DAY_OF_WEEK    FL_DATE UNIQUE_CARRIER TAIL_NUM FL_NUM # 1 1987    10            1           4 1987-10-01             AA               1 # 2 1987    10            2           5 1987-10-02             AA               1 # 3 1987    10            3           6 1987-10-03             AA               1 # 4 1987    10            4           7 1987-10-04             AA               1 # 5 1987    10            5           1 1987-10-05             AA               1 # 6 1987    10            6           2 1987-10-06             AA               1 #   ORIGIN_AIRPORT_ID ORIGIN ORIGIN_STATE_ABR DEST_AIRPORT_ID DEST DEST_STATE_ABR # 1             12478    JFK               NY           12892  LAX             CA # 2             12478    JFK               NY           12892  LAX             CA # 3             12478    JFK               NY           12892  LAX             CA # 4             12478    JFK               NY           12892  LAX             CA # 5             12478    JFK               NY           12892  LAX             CA # 6             12478    JFK               NY           12892  LAX             CA #   CRS_DEP_TIME DEP_TIME DEP_DELAY DEP_DELAY_NEW DEP_DEL15 DEP_DELAY_GROUP TAXI_OUT # 1          900      901         1             1         0               0          # 2          900      901         1             1         0               0          # 3          900      859        -1             0         0              -1          # 4          900      900         0             0         0               0          # 5          900      902         2             2         0               0          # 6          900      900         0             0         0               0          #   WHEELS_OFF WHEELS_ON TAXI_IN CRS_ARR_TIME ARR_TIME ARR_DELAY ARR_DELAY_NEW # 1                                      1152     1117       -35             0 # 2                                      1152     1137       -15             0 # 3                                      1152     1111       -41             0 # 4                                      1152     1116       -36             0 # 5                                      1152     1119       -33             0 # 6                                      1152       NA        NA            NA #   ARR_DEL15 ARR_DELAY_GROUP CANCELLED CANCELLATION_CODE DIVERTED CRS_ELAPSED_TIME # 1         0              -2         0                          0              352 # 2         0              -1         0                          0              352 # 3         0              -2         0                          0              352 # 4         0              -2         0                          0              352 # 5         0              -2         0                          0              352 # 6        NA              NA         0                          1              352 #   ACTUAL_ELAPSED_TIME AIR_TIME FLIGHTS DISTANCE DISTANCE_GROUP CARRIER_DELAY # 1                 316                1     2475             10               # 2                 336                1     2475             10               # 3                 312                1     2475             10               # 4                 316                1     2475             10               # 5                 317                1     2475             10               # 6                  NA                1     2475             10               #   WEATHER_DELAY NAS_DELAY SECURITY_DELAY LATE_AIRCRAFT_DELAY  # 1                                                             # 2                                                             # 3                                                             # 4                                                             # 5                                                             # 6                                                             #  # [[2]] #   Visibility DryBulbCelsius DewPointCelsius RelativeHumidity WindSpeed Altimeter # 1   4.000000       0.000000       -1.000000        92.000000  0.000000 29.690000 # 2  10.000000       7.000000       -3.000000        49.000000 11.000000 29.790000 # 3   3.000000       1.000000        0.000000        92.000000  3.000000 29.710000 # 4  10.000000       7.000000       -5.000000        42.000000 18.000000 29.710000 # 5   1.250000       3.000000        0.000000        82.000000  6.000000 29.720000 # 6  10.000000       8.000000       -4.000000        44.000000 14.000000 29.770000 #   AdjustedYear AdjustedMonth AdjustedDay AdjustedHour AirportID # 1         2007             5           4           18     15177 # 2         2007             5           4            6     15177 # 3         2007             5           4           17     15177 # 4         2007             5           4            9     15177 # 5         2007             5           4           10     15177 # 6         2007             5           4            7     15177   "],
["data-manipulation-with-sparkr.html", "Chapter 8 Data Manipulation with SparkR 8.1 Data Aggregations", " Chapter 8 Data Manipulation with SparkR Now that we have our two datasets saved as Spark DataFrames, we can conduct standard data manipulation techniques to visualize and explore our data. First, we’ll use the rename function to rename our columns, and the select function to select the columns we need. We’ll also transform the These SparkR functions look just like the verbs from teh dplyr package for data manipulation, but are designed to work with Spark DataFrames. system.time(airDF &lt;- rename(airDF,                 ArrDel15 = airDF$ARR_DEL15,                 Year = airDF$YEAR,                 Month = airDF$MONTH,                 DayofMonth = airDF$DAY_OF_MONTH,                 DayOfWeek = airDF$DAY_OF_WEEK,                 Carrier = airDF$UNIQUE_CARRIER,                 OriginAirportID = airDF$ORIGIN_AIRPORT_ID,                 DestAirportID = airDF$DEST_AIRPORT_ID,                 CRSDepTime = airDF$CRS_DEP_TIME,                 CRSArrTime =  airDF$CRS_ARR_TIME,                 Distance = airDF$DISTANCE,                 DepDelay = airDF$DEP_DELAY,                 ArrDelay = airDF$ARR_DELAY                 )             )   #  user  system elapsed    # 0.136   0.000   0.242   # Select desired columns from the flight data.  varsToKeep &lt;- c(&quot;ArrDel15&quot;, &quot;Year&quot;, &quot;Month&quot;, &quot;DayofMonth&quot;, &quot;DayOfWeek&quot;, &quot;Carrier&quot;,                 &quot;OriginAirportID&quot;, &quot;DestAirportID&quot;, &quot;CRSDepTime&quot;, &quot;CRSArrTime&quot;,                 &quot;Distance&quot;, &quot;DepDelay&quot;, &quot;ArrDelay&quot;) system.time(airDF &lt;- select(airDF, varsToKeep))   #  user  system elapsed    # 0.064   0.000   0.112   # Round down scheduled departure time to full hour. system.time(airDF$CRSDepTime &lt;- floor(airDF$CRSDepTime / 100))   # user  system elapsed    #  0.00    0.00    0.06   8.1 Data Aggregations SparkR is great at merges, and data aggregation. For instance, suppose we want to see the average departure delay for each carrier and arrange it in descending order. The following syntax makes that very easy. sum_df &lt;- airDF %&gt;% select(&quot;Carrier&quot;, &quot;DepDelay&quot;) %&gt;%    groupBy(airDF$Carrier) %&gt;%    summarize(count = n(airDF$Carrier),              ave_delay = mean(airDF$DepDelay))    # user  system elapsed    # 0.024   0.000   0.055  The syntax is almost exactly like the syntax from the dplyr package, and the %&gt;% operator makes chaining the additive methods exceptionally simple. Note that the above operation will not be run until we call upon the sum_df. It is for now, just a promise for deferred evaluation. In order to evaluate and bring the summarized data into an R data.frame, we can use the collect statement. sum_local &lt;- sum_df %&gt;% collect() library(dplyr) sum_local %&gt;% arrange(desc(ave_delay))   #  user  system elapsed    # 0.616   0.536 337.758  Now that our data resides as a local data.frame, we can plot it using any R plotting library. load(&quot;local_df.RData&quot;) library(rcdimple) ## Loading required package: htmlwidgets ## Loading required package: htmltools sum_local %&gt;%    dimple(x =&quot;Carrier&quot;, y = &quot;ave_delay&quot;, z =  &quot;count&quot;, type = &quot;bar&quot;) %&gt;%   add_title(html = &quot;&lt;h4&gt;Average Delay in Minutes by Carrier&lt;/h4&gt;&quot; ) %&gt;%    zAxis(outputFormat = &quot;#,### &quot;)  {\"x\":{\"options\":{\"chart\":[],\"xAxis\":{\"type\":\"addCategoryAxis\"},\"yAxis\":{\"type\":\"addMeasureAxis\"},\"zAxis\":{\"type\":\"addMeasureAxis\",\"outputFormat\":\"#,### \"},\"colorAxis\":[],\"defaultColors\":[],\"layers\":[],\"legend\":[],\"x\":\"Carrier\",\"y\":\"ave_delay\",\"type\":\"bar\",\"z\":\"count\",\"title\":{\"text\":null,\"html\":\"Average Delay in Minutes by Carrier\\u003c/h4>\"}},\"data\":{\"Carrier\":[\"AA\",\"PA (1)\",\"TW\",\"TZ\",\"HA\",\"AS\",\"UA\",\"B6\",\"NW\",\"HP\",\"US\",\"OH\",\"OO\",\"PI\",\"VX\",\"CO\",\"ML (1)\",\"PS\",\"WN\",\"DH\",\"DL\",\"KH\",\"XE\",\"EA\",\"EV\",\"F9\",\"9E\",\"YV\",\"FL\",\"MQ\"],\"count\":[17140606,316167,3757747,208420,555683,3443588,14862404,1652137,10585760,3636682,15709733,1765828,5443169,873957,54742,8888536,70622,83617,20529039,693047,19168060,154381,3459389,919785,3384793,670653,1045396,1563254,2232262,5750198],\"ave_delay\":[8.09216330413803,5.53244244289068,7.65825114221727,5.55423481294241,-0.470471755160245,7.24013290246557,9.6410715967209,10.9892686446028,6.02139368193511,8.10779026658562,6.99896030027832,9.52695744411818,6.95087777743635,9.56033602798461,10.0396614090817,7.81891288846895,6.2296766743649,8.92810370334441,9.32814933847665,9.61263938968893,7.55365844885708,1.59931768991184,8.55590331293135,8.67405056543554,12.7600643883231,6.82652221551061,6.8708289697235,9.63959007239046,8.28258416166276,8.69911067862048]}},\"evals\":[],\"jsHooks\":[]} In order to make the weather data correspond to the airline data, let us aggregate it by weatherAgg &lt;- weatherDF %&gt;%    groupBy(&quot;AdjustedYear&quot;, &quot;AdjustedMonth&quot;, &quot;AdjustedDay&quot;, &quot;AdjustedHour&quot;, &quot;AirportID&quot;) %&gt;%    agg(Visibility = avg(weatherDF$Visibility),       DryBulbCelsius = avg(weatherDF$DryBulbCelsius),       DewPointCelsius = avg(weatherDF$DewPointCelsius),       RelativeHumidity = avg(weatherDF$RelativeHumidity),       WindSpeed = avg(weatherDF$RelativeHumidity),       Altimeter = avg(weatherDF$Altimeter))  8.1.1 Merging Data We can use SparkR for merging data sets as well. Let’s merge the airlines dataset with the weather dataset. We’ll first add weather data to the origination airport, and then add it to the destination airport. To keep our data in manageable size, we will remove the redundant columns. Finally, we save the DataFrame to a CSV file, stored in HDFS for easier access at a later date. joinedDF &lt;- SparkR::join(   airDF,   weatherAgg,   airDF$OriginAirportID == weatherAgg$AirportID &amp;     airDF$Year == weatherAgg$AdjustedYear &amp;     airDF$Month == weatherAgg$AdjustedMonth &amp;     airDF$DayofMonth == weatherAgg$AdjustedDay &amp;     airDF$CRSDepTime == weatherAgg$AdjustedHour,   joinType = &quot;left_outer&quot; )  vars &lt;- names(joinedDF) varsToDrop &lt;- c(&#39;AdjustedYear&#39;, &#39;AdjustedMonth&#39;, &#39;AdjustedDay&#39;, &#39;AdjustedHour&#39;, &#39;AirportID&#39;) varsToKeep &lt;- vars[!(vars %in% varsToDrop)] joinedDF1 &lt;- select(joinedDF, varsToKeep)  joinedDF2 &lt;- rename(joinedDF1,                     VisibilityOrigin = joinedDF1$Visibility,                     DryBulbCelsiusOrigin = joinedDF1$DryBulbCelsius,                     DewPointCelsiusOrigin = joinedDF1$DewPointCelsius,                     RelativeHumidityOrigin = joinedDF1$RelativeHumidity,                     WindSpeedOrigin = joinedDF1$WindSpeed,                     AltimeterOrigin = joinedDF1$Altimeter )   joinedDF3 &lt;- join(   joinedDF2,   weatherAgg,   airDF$DestAirportID == weatherAgg$AirportID &amp;     airDF$Year == weatherAgg$AdjustedYear &amp;     airDF$Month == weatherAgg$AdjustedMonth &amp;     airDF$DayofMonth == weatherAgg$AdjustedDay &amp;     airDF$CRSDepTime == weatherAgg$AdjustedHour,   joinType = &quot;left_outer&quot; )  # Remove redundant columns vars &lt;- names(joinedDF3) varsToDrop &lt;- c(&#39;AdjustedYear&#39;, &#39;AdjustedMonth&#39;, &#39;AdjustedDay&#39;, &#39;AdjustedHour&#39;, &#39;AirportID&#39;) varsToKeep &lt;- vars[!(vars %in% varsToDrop)] joinedDF4 &lt;- select(joinedDF3, varsToKeep)  joinedDF5 &lt;- rename(joinedDF4,                     VisibilityDest = joinedDF4$Visibility,                     DryBulbCelsiusDest = joinedDF4$DryBulbCelsius,                     DewPointCelsiusDest = joinedDF4$DewPointCelsius,                     RelativeHumidityDest = joinedDF4$RelativeHumidity,                     WindSpeedDest = joinedDF4$WindSpeed,                     AltimeterDest = joinedDF4$Altimeter                     )   joinedDF5 &lt;- repartition(joinedDF5, 80)   # write result to directory of CSVs write.df(joinedDF5, file.path(&quot;/user/RevoShare/alizaidi/delayDataLarge&quot;,                               &quot;JoinAirWeatherDelay&quot;),          &quot;com.databricks.spark.csv&quot;, &quot;overwrite&quot;,           header = &quot;true&quot;)  # We can shut down the SparkR Spark context now sparkR.stop()        "],
["modeling-with-microsoft-r-server.html", "Chapter 9 Modeling with Microsoft R Server 9.1 Import CSV to XDF 9.2 Splitting XDF into Train and Test Tests 9.3 Training Binary Classification Models 9.4 Testing Models", " Chapter 9 Modeling with Microsoft R Server  9.1 Import CSV to XDF To take full advantage of the PEMA algorithms provided by MRS, we will import the merged data, currently saved as csv in blob storage, into an xdf. We first have some housekeeping items to take care. We need to specify the spark compute context for the RevoScaleR package to properly utlize the Spark cluster. Saving a text file to HDFS creates blocks of the data and saves them in separate directories, and also saves an additional directory entitled “_SUCCESS&quot; to indicate the import operation was successful. We need to remove this file before importing to xdf, as it has no value for the final data. Further, in order to make sure the MRS modeling functions respect the data types of the columns in our merged dataset, we need to provide it with some column metadata. This can be provided with the colInfo argument inside of rxImport. Lastly, we need to provide MRS with pointers to the HDFS store we will be saving our XDF to. rxOptions(fileSystem = RxHdfsFileSystem())  dataDir &lt;- &quot;/user/RevoShare/alizaidi/delayDataLarge&quot;  if(rxOptions()$hdfsHost == &quot;default&quot;) {  fullDataDir &lt;- dataDir } else {  fullDataDir &lt;- paste0(rxOptions()$hdfsHost, dataDir) }    computeContext &lt;- RxSpark(consoleOutput = TRUE)  # there&#39;s a folder called SUCCESS_ that we need to delete manually file_to_delete &lt;- file.path(data_dir, &quot;delayDataLarge&quot;, &quot;JoinAirWeatherDelay&quot;, &quot;_SUCCESS&quot;) delete_command &lt;- paste(&quot;fs -rm&quot;, file_to_delete) rxHadoopCommand(delete_command)   colInfo &lt;- list(   ArrDel15 = list(type=&quot;numeric&quot;),   Year = list(type=&quot;factor&quot;),   Month = list(type=&quot;factor&quot;),   DayofMonth = list(type=&quot;factor&quot;),   DayOfWeek = list(type=&quot;factor&quot;),   Carrier = list(type=&quot;factor&quot;),   OriginAirportID = list(type=&quot;factor&quot;),   DestAirportID = list(type=&quot;factor&quot;),   RelativeHumidityOrigin = list(type=&quot;numeric&quot;),   AltimeterOrigin = list(type=&quot;numeric&quot;),   DryBulbCelsiusOrigin = list(type=&quot;numeric&quot;),   WindSpeedOrigin = list(type=&quot;numeric&quot;),   VisibilityOrigin = list(type=&quot;numeric&quot;),   DewPointCelsiusOrigin = list(type=&quot;numeric&quot;),   RelativeHumidityDest = list(type=&quot;numeric&quot;),   AltimeterDest = list(type=&quot;numeric&quot;),   DryBulbCelsiusDest = list(type=&quot;numeric&quot;),   WindSpeedDest = list(type=&quot;numeric&quot;),   VisibilityDest = list(type=&quot;numeric&quot;),   DewPointCelsiusDest = list(type=&quot;numeric&quot;),   CRSDepTime = list(type = &quot;numeric&quot;),   CRSArrTime = list(type = &quot;numeric&quot;),   DepDelay = list(type = &quot;numeric&quot;),   ArrDelay = list(type = &quot;numeric&quot;) )  myNameNode &lt;- &quot;default&quot; myPort &lt;- 0 hdfsFS &lt;- RxHdfsFileSystem(hostName = myNameNode,                             port = myPort)  joined_txt &lt;- RxTextData(file.path(data_dir, &quot;delayDataLarge&quot;, &quot;JoinAirWeatherDelay&quot;),                            colInfo = colInfo,                            fileSystem = hdfsFS)  dest_xdf &lt;- RxXdfData(file.path(data_dir, &quot;delayDataLarge&quot;, &quot;joinedAirWeatherXdf&quot;),                       fileSystem = hdfsFS)    rxImport(inData = joined_txt, dest_xdf, overwrite = TRUE) Now that we have imported our data to an XDF, we can get some information about the variables: rxGetInfo(RxXdfData(file.path(data_dir, &quot;delayDataLarge&quot;, &quot;joinedAirWeatherXdf&quot;),                       fileSystem = hdfsFS), getVarInfo = T, numRows = 2) ## ======  ed00-mrs-sp (Master HPA Process) has started run at Fri Apr 22 04:44:16 2016  ======  ## ======  ed00-mrs-sp (Master HPA Process) has completed run at Fri Apr 22 04:44:44 2016  ====== ## File name: /user/RevoShare/alizaidi/delayDataLarge/joinedAirWeatherXdf  ## Number of composite data files: 80  ## Number of observations: 148619655  ## Number of variables: 22  ## Number of blocks: 320  ## Compression type: zlib  ## Variable information:  ## Var 1: ArrDel15, Type: numeric, Low/High: (0.0000, 1.0000) ## Var 2: Year ##        26 factor levels: 1990 1992 1994 1997 1999 ... 2005 2008 2010 2011 2012 ## Var 3: Month ##        12 factor levels: 3 8 9 2 5 ... 11 6 12 7 10 ## Var 4: DayofMonth ##        31 factor levels: 11 23 9 26 4 ... 12 8 27 25 22 ## Var 5: DayOfWeek ##        7 factor levels: 7 5 3 2 4 1 6 ## Var 6: Carrier ##        30 factor levels: US UA DL NW DH ... PA (1) KH HA PS VX ## Var 7: OriginAirportID ##        374 factor levels: 10821 13930 11057 13230 11433 ... 10559 13341 14314 11931 10558 ## Var 8: DestAirportID ##        378 factor levels: 10135 10136 10140 10146 10155 ... 10559 11931 10894 14475 12899 ## Var 9: CRSDepTime, Type: numeric, Low/High: (0.0000, 24.0000) ## Var 10: CRSArrTime, Type: numeric, Low/High: (0.0000, 2400.0000) ## Var 11: RelativeHumidityOrigin, Type: numeric, Low/High: (0.0000, 100.0000) ## Var 12: AltimeterOrigin, Type: numeric, Low/High: (28.1700, 31.1600) ## Var 13: DryBulbCelsiusOrigin, Type: numeric, Low/High: (-46.1000, 47.2000) ## Var 14: WindSpeedOrigin, Type: numeric, Low/High: (0.0000, 81.0000) ## Var 15: VisibilityOrigin, Type: numeric, Low/High: (0.0000, 88.0000) ## Var 16: DewPointCelsiusOrigin, Type: numeric, Low/High: (-41.7000, 29.0000) ## Var 17: RelativeHumidityDest, Type: numeric, Low/High: (0.0000, 100.0000) ## Var 18: AltimeterDest, Type: numeric, Low/High: (28.1700, 31.1600) ## Var 19: DryBulbCelsiusDest, Type: numeric, Low/High: (-46.1000, 53.9000) ## Var 20: WindSpeedDest, Type: numeric, Low/High: (0.0000, 63.0000) ## Var 21: VisibilityDest, Type: numeric, Low/High: (0.0000, 88.0000) ## Var 22: DewPointCelsiusDest, Type: numeric, Low/High: (-43.0000, 29.0000) ## Data (2 rows starting with row 1): ##   ArrDel15 Year Month DayofMonth DayOfWeek Carrier OriginAirportID ## 1        0 1990     3         11         7      US           10821 ## 2        1 1992     8         23         7      UA           13930 ##   DestAirportID CRSDepTime CRSArrTime RelativeHumidityOrigin ## 1         10135         10       1056                     NA ## 2         10135          6        928                     NA ##   AltimeterOrigin DryBulbCelsiusOrigin WindSpeedOrigin VisibilityOrigin ## 1              NA                   NA              NA               NA ## 2              NA                   NA              NA               NA ##   DewPointCelsiusOrigin RelativeHumidityDest AltimeterDest ## 1                    NA                   NA            NA ## 2                    NA                   NA            NA ##   DryBulbCelsiusDest WindSpeedDest VisibilityDest DewPointCelsiusDest ## 1                 NA            NA             NA                  NA ## 2                 NA            NA             NA                  NA   9.2 Splitting XDF into Train and Test Tests Prior to estimating our predictive models, we need to split our dataset into a training set, which we’ll use for estimation, and a test set that we’ll use for validating our results. Since we have time series data (data ordered by time), we will split our data by time. We’ll use the data prior to 2012 for training, and the data in 2012 for testing. trainDS &lt;- RxXdfData( file.path(dataDir, &quot;finalDataTrain&quot; ),                       fileSystem = hdfsFS)  rxDataStep( inData = dest_xdf, outFile = trainDS,             rowSelection = ( Year != 2012 ), overwrite = T )  testDS &lt;- RxXdfData( file.path(dataDir, &quot;finalDataTest&quot; ),                      fileSystem = hdfsFS)  rxDataStep( inData = dest_xdf, outFile = testDS,             rowSelection = ( Year == 2012 ), overwrite = T )   9.3 Training Binary Classification Models Now that we have our train and test sets, we can estimate our predictive model. Let’s try to predict the probability that a flight will be delayed as a function of other variables.  9.3.1 Logistic Regression Models RevoScaleR provides a highly optimized logistic regression model based on the Iteratively Reweighted Least Squares (IRLS) algorithm, which can be called using the rxLogit function. The rxLogit function looks nearly identical to the standard logistic regression function provided by the glm function in the base stats package, taking a formula as it’s first argument, and the data as it’s second argument. We create a handy function make_formula for creating formula objects based on the variables in the all_vars argument of the function. make_formula &lt;- function(resp_var,                          vars_exclude,                          all_vars) {      features &lt;- all_vars[!(all_vars %in% c(resp_var, vars_exclude))]   form &lt;- as.formula(paste(resp_var, paste0(features, collapse = &quot; + &quot;),                            sep  = &quot; ~ &quot;))      return(form) }  data_names &lt;- rxGetVarNames(trainDS)  form &lt;- make_formula(&quot;ArrDel15&quot;, c(&quot;DepDelay&quot;, &quot;ArrDelay&quot;), data_names)  system.time(logitModel &lt;- rxLogit(form, data = trainDS))  #   user  system elapsed   # 15.916  17.068 302.806   base::summary(logitModel)   9.3.2 Tree and Ensemble Classifiers Training the logistic regression model on the full training set took about five minutes. Logistic regression models are frequently used for classification problems due to their interpability and extensibility. However, without adequeate feature engineering, logistic regression models tend to lack the expressiveness and predictive power of ensemble methods, such as boosted trees, or random forests. Using the same methodology as above, we could estimate decision trees and decision forests (random forests) just as easily with the same formula: system.time(dTreeModel &lt;- rxDTree(form, data = trainDS,                                   maxDepth = 6, pruneCp = &quot;auto&quot;))   #   user   system  elapsed    # 29.088   67.940 1265.633    9.4 Testing Models Now that we have estimated our models, we can calculate valuation metrics for them by scoring on the test/held-out set. load(&quot;testModels.RData&quot;)  treePredict &lt;- RxXdfData(file.path(dataDir, &quot;treePredict&quot;),                          fileSystem = hdfsFS)  system.time(rxPredict(dTreeModel, data = testDS, outData = treePredict,                        extraVarsToWrite = c(&quot;ArrDel15&quot;), overwrite = TRUE)) ## Rows Read: 20577, Total Rows Processed: 20577, Total Chunk Time: 2.625 seconds ## Rows Read: 20524, Total Rows Processed: 41101, Total Chunk Time: 4.197 seconds ## Rows Read: 20487, Total Rows Processed: 61588, Total Chunk Time: 0.048 seconds ## Rows Read: 14729, Total Rows Processed: 76317, Total Chunk Time: 0.047 seconds ## Rows Read: 20509, Total Rows Processed: 96826, Total Chunk Time: 0.447 seconds ## Rows Read: 20455, Total Rows Processed: 117281, Total Chunk Time: 0.041 seconds ## Rows Read: 20623, Total Rows Processed: 137904, Total Chunk Time: 0.042 seconds ## Rows Read: 14643, Total Rows Processed: 152547, Total Chunk Time: 0.048 seconds ## Rows Read: 20513, Total Rows Processed: 173060, Total Chunk Time: 0.514 seconds ## Rows Read: 20587, Total Rows Processed: 193647, Total Chunk Time: 0.042 seconds ## Rows Read: 20583, Total Rows Processed: 214230, Total Chunk Time: 0.042 seconds ## Rows Read: 14622, Total Rows Processed: 228852, Total Chunk Time: 0.041 seconds ## Rows Read: 20532, Total Rows Processed: 249384, Total Chunk Time: 0.377 seconds ## Rows Read: 20500, Total Rows Processed: 269884, Total Chunk Time: 0.040 seconds ## Rows Read: 20609, Total Rows Processed: 290493, Total Chunk Time: 0.039 seconds ## Rows Read: 14703, Total Rows Processed: 305196, Total Chunk Time: 0.039 seconds ## Rows Read: 20573, Total Rows Processed: 325769, Total Chunk Time: 0.355 seconds ## Rows Read: 20516, Total Rows Processed: 346285, Total Chunk Time: 0.043 seconds ## Rows Read: 20584, Total Rows Processed: 366869, Total Chunk Time: 0.042 seconds ## Rows Read: 14682, Total Rows Processed: 381551, Total Chunk Time: 0.046 seconds ## Rows Read: 20569, Total Rows Processed: 402120, Total Chunk Time: 0.369 seconds ## Rows Read: 20514, Total Rows Processed: 422634, Total Chunk Time: 0.044 seconds ## Rows Read: 20588, Total Rows Processed: 443222, Total Chunk Time: 0.042 seconds ## Rows Read: 14649, Total Rows Processed: 457871, Total Chunk Time: 0.041 seconds ## Rows Read: 20520, Total Rows Processed: 478391, Total Chunk Time: 0.377 seconds ## Rows Read: 20495, Total Rows Processed: 498886, Total Chunk Time: 0.045 seconds ## Rows Read: 20651, Total Rows Processed: 519537, Total Chunk Time: 0.044 seconds ## Rows Read: 14678, Total Rows Processed: 534215, Total Chunk Time: 0.045 seconds ## Rows Read: 20525, Total Rows Processed: 554740, Total Chunk Time: 0.370 seconds ## Rows Read: 20535, Total Rows Processed: 575275, Total Chunk Time: 0.040 seconds ## Rows Read: 20575, Total Rows Processed: 595850, Total Chunk Time: 0.041 seconds ## Rows Read: 14686, Total Rows Processed: 610536, Total Chunk Time: 0.039 seconds ## Rows Read: 20585, Total Rows Processed: 631121, Total Chunk Time: 0.324 seconds ## Rows Read: 20456, Total Rows Processed: 651577, Total Chunk Time: 0.043 seconds ## Rows Read: 20564, Total Rows Processed: 672141, Total Chunk Time: 0.042 seconds ## Rows Read: 14710, Total Rows Processed: 686851, Total Chunk Time: 0.042 seconds ## Rows Read: 20561, Total Rows Processed: 707412, Total Chunk Time: 0.404 seconds ## Rows Read: 20506, Total Rows Processed: 727918, Total Chunk Time: 0.042 seconds ## Rows Read: 20560, Total Rows Processed: 748478, Total Chunk Time: 0.041 seconds ## Rows Read: 14680, Total Rows Processed: 763158, Total Chunk Time: 0.049 seconds ## Rows Read: 20451, Total Rows Processed: 783609, Total Chunk Time: 0.411 seconds ## Rows Read: 20605, Total Rows Processed: 804214, Total Chunk Time: 0.041 seconds ## Rows Read: 20533, Total Rows Processed: 824747, Total Chunk Time: 0.038 seconds ## Rows Read: 14678, Total Rows Processed: 839425, Total Chunk Time: 0.042 seconds ## Rows Read: 20543, Total Rows Processed: 859968, Total Chunk Time: 0.373 seconds ## Rows Read: 20446, Total Rows Processed: 880414, Total Chunk Time: 0.041 seconds ## Rows Read: 20573, Total Rows Processed: 900987, Total Chunk Time: 0.040 seconds ## Rows Read: 14665, Total Rows Processed: 915652, Total Chunk Time: 0.040 seconds ## Rows Read: 20523, Total Rows Processed: 936175, Total Chunk Time: 0.409 seconds ## Rows Read: 20523, Total Rows Processed: 956698, Total Chunk Time: 0.041 seconds ## Rows Read: 20543, Total Rows Processed: 977241, Total Chunk Time: 0.040 seconds ## Rows Read: 14628, Total Rows Processed: 991869, Total Chunk Time: 0.054 seconds ## Rows Read: 20489, Total Rows Processed: 1012358, Total Chunk Time: 0.407 seconds ## Rows Read: 20521, Total Rows Processed: 1032879, Total Chunk Time: 0.044 seconds ## Rows Read: 20542, Total Rows Processed: 1053421, Total Chunk Time: 0.043 seconds ## Rows Read: 14690, Total Rows Processed: 1068111, Total Chunk Time: 0.042 seconds ## Rows Read: 20514, Total Rows Processed: 1088625, Total Chunk Time: 0.330 seconds ## Rows Read: 20501, Total Rows Processed: 1109126, Total Chunk Time: 0.043 seconds ## Rows Read: 20565, Total Rows Processed: 1129691, Total Chunk Time: 0.041 seconds ## Rows Read: 14654, Total Rows Processed: 1144345, Total Chunk Time: 0.041 seconds ## Rows Read: 20549, Total Rows Processed: 1164894, Total Chunk Time: 0.349 seconds ## Rows Read: 20465, Total Rows Processed: 1185359, Total Chunk Time: 0.040 seconds ## Rows Read: 20527, Total Rows Processed: 1205886, Total Chunk Time: 0.037 seconds ## Rows Read: 14646, Total Rows Processed: 1220532, Total Chunk Time: 0.038 seconds ## Rows Read: 20510, Total Rows Processed: 1241042, Total Chunk Time: 0.422 seconds ## Rows Read: 20576, Total Rows Processed: 1261618, Total Chunk Time: 0.042 seconds ## Rows Read: 20392, Total Rows Processed: 1282010, Total Chunk Time: 0.042 seconds ## Rows Read: 14688, Total Rows Processed: 1296698, Total Chunk Time: 0.040 seconds ## Rows Read: 20511, Total Rows Processed: 1317209, Total Chunk Time: 0.418 seconds ## Rows Read: 20520, Total Rows Processed: 1337729, Total Chunk Time: 0.042 seconds ## Rows Read: 20428, Total Rows Processed: 1358157, Total Chunk Time: 0.041 seconds ## Rows Read: 14674, Total Rows Processed: 1372831, Total Chunk Time: 0.041 seconds ## Rows Read: 20410, Total Rows Processed: 1393241, Total Chunk Time: 0.985 seconds ## Rows Read: 20448, Total Rows Processed: 1413689, Total Chunk Time: 0.042 seconds ## Rows Read: 20614, Total Rows Processed: 1434303, Total Chunk Time: 0.041 seconds ## Rows Read: 14668, Total Rows Processed: 1448971, Total Chunk Time: 0.039 seconds ## Rows Read: 20440, Total Rows Processed: 1469411, Total Chunk Time: 0.396 seconds ## Rows Read: 20558, Total Rows Processed: 1489969, Total Chunk Time: 0.037 seconds ## Rows Read: 20481, Total Rows Processed: 1510450, Total Chunk Time: 0.038 seconds ## Rows Read: 14652, Total Rows Processed: 1525102, Total Chunk Time: 0.038 seconds ## Rows Read: 20498, Total Rows Processed: 1545600, Total Chunk Time: 0.670 seconds ## Rows Read: 20597, Total Rows Processed: 1566197, Total Chunk Time: 0.038 seconds ## Rows Read: 20500, Total Rows Processed: 1586697, Total Chunk Time: 0.039 seconds ## Rows Read: 14590, Total Rows Processed: 1601287, Total Chunk Time: 0.038 seconds ## Rows Read: 20563, Total Rows Processed: 1621850, Total Chunk Time: 0.564 seconds ## Rows Read: 20468, Total Rows Processed: 1642318, Total Chunk Time: 0.042 seconds ## Rows Read: 20504, Total Rows Processed: 1662822, Total Chunk Time: 0.042 seconds ## Rows Read: 14647, Total Rows Processed: 1677469, Total Chunk Time: 0.042 seconds ## Rows Read: 20545, Total Rows Processed: 1698014, Total Chunk Time: 0.350 seconds ## Rows Read: 20521, Total Rows Processed: 1718535, Total Chunk Time: 0.041 seconds ## Rows Read: 20413, Total Rows Processed: 1738948, Total Chunk Time: 0.044 seconds ## Rows Read: 14748, Total Rows Processed: 1753696, Total Chunk Time: 0.040 seconds ## Rows Read: 20503, Total Rows Processed: 1774199, Total Chunk Time: 0.442 seconds ## Rows Read: 20683, Total Rows Processed: 1794882, Total Chunk Time: 0.038 seconds ## Rows Read: 20372, Total Rows Processed: 1815254, Total Chunk Time: 0.038 seconds ## Rows Read: 14649, Total Rows Processed: 1829903, Total Chunk Time: 0.038 seconds ## Rows Read: 20478, Total Rows Processed: 1850381, Total Chunk Time: 0.440 seconds ## Rows Read: 20533, Total Rows Processed: 1870914, Total Chunk Time: 0.039 seconds ## Rows Read: 20581, Total Rows Processed: 1891495, Total Chunk Time: 0.037 seconds ## Rows Read: 14662, Total Rows Processed: 1906157, Total Chunk Time: 0.037 seconds ## Rows Read: 20475, Total Rows Processed: 1926632, Total Chunk Time: 0.502 seconds ## Rows Read: 20514, Total Rows Processed: 1947146, Total Chunk Time: 0.037 seconds ## Rows Read: 20556, Total Rows Processed: 1967702, Total Chunk Time: 0.038 seconds ## Rows Read: 14698, Total Rows Processed: 1982400, Total Chunk Time: 0.037 seconds ## Rows Read: 20581, Total Rows Processed: 2002981, Total Chunk Time: 0.387 seconds ## Rows Read: 20490, Total Rows Processed: 2023471, Total Chunk Time: 0.043 seconds ## Rows Read: 20546, Total Rows Processed: 2044017, Total Chunk Time: 0.040 seconds ## Rows Read: 14635, Total Rows Processed: 2058652, Total Chunk Time: 0.040 seconds ## Rows Read: 20462, Total Rows Processed: 2079114, Total Chunk Time: 0.329 seconds ## Rows Read: 20599, Total Rows Processed: 2099713, Total Chunk Time: 0.041 seconds ## Rows Read: 20516, Total Rows Processed: 2120229, Total Chunk Time: 0.041 seconds ## Rows Read: 14618, Total Rows Processed: 2134847, Total Chunk Time: 0.040 seconds ## Rows Read: 20602, Total Rows Processed: 2155449, Total Chunk Time: 0.339 seconds ## Rows Read: 20582, Total Rows Processed: 2176031, Total Chunk Time: 0.040 seconds ## Rows Read: 20383, Total Rows Processed: 2196414, Total Chunk Time: 0.039 seconds ## Rows Read: 14681, Total Rows Processed: 2211095, Total Chunk Time: 0.039 seconds ## Rows Read: 20561, Total Rows Processed: 2231656, Total Chunk Time: 0.426 seconds ## Rows Read: 20513, Total Rows Processed: 2252169, Total Chunk Time: 0.041 seconds ## Rows Read: 20515, Total Rows Processed: 2272684, Total Chunk Time: 0.041 seconds ## Rows Read: 14641, Total Rows Processed: 2287325, Total Chunk Time: 0.041 seconds ## Rows Read: 20501, Total Rows Processed: 2307826, Total Chunk Time: 0.413 seconds ## Rows Read: 20510, Total Rows Processed: 2328336, Total Chunk Time: 0.041 seconds ## Rows Read: 20608, Total Rows Processed: 2348944, Total Chunk Time: 0.037 seconds ## Rows Read: 14594, Total Rows Processed: 2363538, Total Chunk Time: 0.040 seconds ## Rows Read: 20485, Total Rows Processed: 2384023, Total Chunk Time: 0.415 seconds ## Rows Read: 20523, Total Rows Processed: 2404546, Total Chunk Time: 0.039 seconds ## Rows Read: 20526, Total Rows Processed: 2425072, Total Chunk Time: 0.053 seconds ## Rows Read: 14626, Total Rows Processed: 2439698, Total Chunk Time: 0.045 seconds ## Rows Read: 20549, Total Rows Processed: 2460247, Total Chunk Time: 0.410 seconds ## Rows Read: 20427, Total Rows Processed: 2480674, Total Chunk Time: 0.042 seconds ## Rows Read: 20594, Total Rows Processed: 2501268, Total Chunk Time: 0.037 seconds ## Rows Read: 14637, Total Rows Processed: 2515905, Total Chunk Time: 0.038 seconds ## Rows Read: 20498, Total Rows Processed: 2536403, Total Chunk Time: 0.338 seconds ## Rows Read: 20505, Total Rows Processed: 2556908, Total Chunk Time: 0.042 seconds ## Rows Read: 20444, Total Rows Processed: 2577352, Total Chunk Time: 0.042 seconds ## Rows Read: 14686, Total Rows Processed: 2592038, Total Chunk Time: 0.041 seconds ## Rows Read: 20534, Total Rows Processed: 2612572, Total Chunk Time: 0.409 seconds ## Rows Read: 20404, Total Rows Processed: 2632976, Total Chunk Time: 0.040 seconds ## Rows Read: 20496, Total Rows Processed: 2653472, Total Chunk Time: 0.039 seconds ## Rows Read: 14685, Total Rows Processed: 2668157, Total Chunk Time: 0.037 seconds ## Rows Read: 20582, Total Rows Processed: 2688739, Total Chunk Time: 0.323 seconds ## Rows Read: 20414, Total Rows Processed: 2709153, Total Chunk Time: 0.044 seconds ## Rows Read: 20562, Total Rows Processed: 2729715, Total Chunk Time: 0.045 seconds ## Rows Read: 14598, Total Rows Processed: 2744313, Total Chunk Time: 0.046 seconds ## Rows Read: 20432, Total Rows Processed: 2764745, Total Chunk Time: 0.387 seconds ## Rows Read: 20467, Total Rows Processed: 2785212, Total Chunk Time: 0.040 seconds ## Rows Read: 20623, Total Rows Processed: 2805835, Total Chunk Time: 0.040 seconds ## Rows Read: 14636, Total Rows Processed: 2820471, Total Chunk Time: 0.040 seconds ## Rows Read: 20462, Total Rows Processed: 2840933, Total Chunk Time: 1.003 seconds ## Rows Read: 20517, Total Rows Processed: 2861450, Total Chunk Time: 0.053 seconds ## Rows Read: 20419, Total Rows Processed: 2881869, Total Chunk Time: 0.058 seconds ## Rows Read: 14688, Total Rows Processed: 2896557, Total Chunk Time: 0.039 seconds ## Rows Read: 20451, Total Rows Processed: 2917008, Total Chunk Time: 0.388 seconds ## Rows Read: 20546, Total Rows Processed: 2937554, Total Chunk Time: 0.040 seconds ## Rows Read: 20457, Total Rows Processed: 2958011, Total Chunk Time: 0.040 seconds ## Rows Read: 14651, Total Rows Processed: 2972662, Total Chunk Time: 0.041 seconds ## Rows Read: 20502, Total Rows Processed: 2993164, Total Chunk Time: 0.338 seconds ## Rows Read: 20460, Total Rows Processed: 3013624, Total Chunk Time: 0.047 seconds ## Rows Read: 20477, Total Rows Processed: 3034101, Total Chunk Time: 0.045 seconds ## Rows Read: 14654, Total Rows Processed: 3048755, Total Chunk Time: 0.041 seconds ## Rows Read: 20441, Total Rows Processed: 3069196, Total Chunk Time: 0.664 seconds ## Rows Read: 20479, Total Rows Processed: 3089675, Total Chunk Time: 0.041 seconds ## Rows Read: 20585, Total Rows Processed: 3110260, Total Chunk Time: 0.040 seconds ## Rows Read: 14603, Total Rows Processed: 3124863, Total Chunk Time: 0.040 seconds ## Rows Read: 20452, Total Rows Processed: 3145315, Total Chunk Time: 0.357 seconds ## Rows Read: 20463, Total Rows Processed: 3165778, Total Chunk Time: 0.039 seconds ## Rows Read: 20570, Total Rows Processed: 3186348, Total Chunk Time: 0.039 seconds ## Rows Read: 14622, Total Rows Processed: 3200970, Total Chunk Time: 0.039 seconds ## Rows Read: 20543, Total Rows Processed: 3221513, Total Chunk Time: 0.376 seconds ## Rows Read: 20478, Total Rows Processed: 3241991, Total Chunk Time: 0.038 seconds ## Rows Read: 20462, Total Rows Processed: 3262453, Total Chunk Time: 0.038 seconds ## Rows Read: 14622, Total Rows Processed: 3277075, Total Chunk Time: 0.037 seconds ## Rows Read: 20470, Total Rows Processed: 3297545, Total Chunk Time: 0.345 seconds ## Rows Read: 20517, Total Rows Processed: 3318062, Total Chunk Time: 0.041 seconds ## Rows Read: 20497, Total Rows Processed: 3338559, Total Chunk Time: 0.043 seconds ## Rows Read: 14656, Total Rows Processed: 3353215, Total Chunk Time: 0.040 seconds ## Rows Read: 20446, Total Rows Processed: 3373661, Total Chunk Time: 0.359 seconds ## Rows Read: 20508, Total Rows Processed: 3394169, Total Chunk Time: 0.042 seconds ## Rows Read: 20485, Total Rows Processed: 3414654, Total Chunk Time: 0.042 seconds ## Rows Read: 14667, Total Rows Processed: 3429321, Total Chunk Time: 0.041 seconds ## Rows Read: 20461, Total Rows Processed: 3449782, Total Chunk Time: 0.351 seconds ## Rows Read: 20573, Total Rows Processed: 3470355, Total Chunk Time: 0.043 seconds ## Rows Read: 20482, Total Rows Processed: 3490837, Total Chunk Time: 0.041 seconds ## Rows Read: 14610, Total Rows Processed: 3505447, Total Chunk Time: 0.040 seconds ## Rows Read: 20546, Total Rows Processed: 3525993, Total Chunk Time: 0.309 seconds ## Rows Read: 20562, Total Rows Processed: 3546555, Total Chunk Time: 0.040 seconds ## Rows Read: 20464, Total Rows Processed: 3567019, Total Chunk Time: 0.038 seconds ## Rows Read: 14558, Total Rows Processed: 3581577, Total Chunk Time: 0.038 seconds ## Rows Read: 20512, Total Rows Processed: 3602089, Total Chunk Time: 0.330 seconds ## Rows Read: 20444, Total Rows Processed: 3622533, Total Chunk Time: 0.041 seconds ## Rows Read: 20452, Total Rows Processed: 3642985, Total Chunk Time: 0.044 seconds ## Rows Read: 14697, Total Rows Processed: 3657682, Total Chunk Time: 0.042 seconds ## Rows Read: 20501, Total Rows Processed: 3678183, Total Chunk Time: 0.408 seconds ## Rows Read: 20526, Total Rows Processed: 3698709, Total Chunk Time: 0.041 seconds ## Rows Read: 20543, Total Rows Processed: 3719252, Total Chunk Time: 0.040 seconds ## Rows Read: 14620, Total Rows Processed: 3733872, Total Chunk Time: 0.039 seconds ## Rows Read: 20489, Total Rows Processed: 3754361, Total Chunk Time: 0.425 seconds ## Rows Read: 20596, Total Rows Processed: 3774957, Total Chunk Time: 0.043 seconds ## Rows Read: 20469, Total Rows Processed: 3795426, Total Chunk Time: 0.041 seconds ## Rows Read: 14657, Total Rows Processed: 3810083, Total Chunk Time: 0.041 seconds ## Rows Read: 20469, Total Rows Processed: 3830552, Total Chunk Time: 0.366 seconds ## Rows Read: 20502, Total Rows Processed: 3851054, Total Chunk Time: 0.040 seconds ## Rows Read: 20524, Total Rows Processed: 3871578, Total Chunk Time: 0.041 seconds ## Rows Read: 14694, Total Rows Processed: 3886272, Total Chunk Time: 0.039 seconds ## Rows Read: 20503, Total Rows Processed: 3906775, Total Chunk Time: 0.375 seconds ## Rows Read: 20474, Total Rows Processed: 3927249, Total Chunk Time: 0.045 seconds ## Rows Read: 20573, Total Rows Processed: 3947822, Total Chunk Time: 0.043 seconds ## Rows Read: 14637, Total Rows Processed: 3962459, Total Chunk Time: 0.039 seconds ## Rows Read: 20566, Total Rows Processed: 3983025, Total Chunk Time: 0.380 seconds ## Rows Read: 20441, Total Rows Processed: 4003466, Total Chunk Time: 0.046 seconds ## Rows Read: 20457, Total Rows Processed: 4023923, Total Chunk Time: 0.040 seconds ## Rows Read: 14706, Total Rows Processed: 4038629, Total Chunk Time: 0.040 seconds ## Rows Read: 20459, Total Rows Processed: 4059088, Total Chunk Time: 0.353 seconds ## Rows Read: 20431, Total Rows Processed: 4079519, Total Chunk Time: 0.047 seconds ## Rows Read: 20576, Total Rows Processed: 4100095, Total Chunk Time: 0.046 seconds ## Rows Read: 14680, Total Rows Processed: 4114775, Total Chunk Time: 0.039 seconds ## Rows Read: 20398, Total Rows Processed: 4135173, Total Chunk Time: 0.470 seconds ## Rows Read: 20496, Total Rows Processed: 4155669, Total Chunk Time: 0.040 seconds ## Rows Read: 20517, Total Rows Processed: 4176186, Total Chunk Time: 0.045 seconds ## Rows Read: 14696, Total Rows Processed: 4190882, Total Chunk Time: 0.042 seconds ## Rows Read: 20512, Total Rows Processed: 4211394, Total Chunk Time: 0.452 seconds ## Rows Read: 20466, Total Rows Processed: 4231860, Total Chunk Time: 0.041 seconds ## Rows Read: 20475, Total Rows Processed: 4252335, Total Chunk Time: 0.039 seconds ## Rows Read: 14678, Total Rows Processed: 4267013, Total Chunk Time: 0.039 seconds ## Rows Read: 20550, Total Rows Processed: 4287563, Total Chunk Time: 0.850 seconds ## Rows Read: 20479, Total Rows Processed: 4308042, Total Chunk Time: 0.044 seconds ## Rows Read: 20505, Total Rows Processed: 4328547, Total Chunk Time: 0.043 seconds ## Rows Read: 14669, Total Rows Processed: 4343216, Total Chunk Time: 0.040 seconds ## Rows Read: 20470, Total Rows Processed: 4363686, Total Chunk Time: 0.406 seconds ## Rows Read: 20486, Total Rows Processed: 4384172, Total Chunk Time: 0.040 seconds ## Rows Read: 20511, Total Rows Processed: 4404683, Total Chunk Time: 0.039 seconds ## Rows Read: 14714, Total Rows Processed: 4419397, Total Chunk Time: 0.039 seconds ## Rows Read: 20599, Total Rows Processed: 4439996, Total Chunk Time: 0.458 seconds ## Rows Read: 20501, Total Rows Processed: 4460497, Total Chunk Time: 0.040 seconds ## Rows Read: 20479, Total Rows Processed: 4480976, Total Chunk Time: 0.040 seconds ## Rows Read: 14666, Total Rows Processed: 4495642, Total Chunk Time: 0.040 seconds ## Rows Read: 20540, Total Rows Processed: 4516182, Total Chunk Time: 0.369 seconds ## Rows Read: 20514, Total Rows Processed: 4536696, Total Chunk Time: 0.043 seconds ## Rows Read: 20555, Total Rows Processed: 4557251, Total Chunk Time: 0.041 seconds ## Rows Read: 14687, Total Rows Processed: 4571938, Total Chunk Time: 0.042 seconds ## Rows Read: 20529, Total Rows Processed: 4592467, Total Chunk Time: 0.332 seconds ## Rows Read: 20502, Total Rows Processed: 4612969, Total Chunk Time: 0.048 seconds ## Rows Read: 20628, Total Rows Processed: 4633597, Total Chunk Time: 0.044 seconds ## Rows Read: 14652, Total Rows Processed: 4648249, Total Chunk Time: 0.042 seconds ## Rows Read: 20561, Total Rows Processed: 4668810, Total Chunk Time: 0.270 seconds ## Rows Read: 20461, Total Rows Processed: 4689271, Total Chunk Time: 0.041 seconds ## Rows Read: 20577, Total Rows Processed: 4709848, Total Chunk Time: 0.041 seconds ## Rows Read: 14677, Total Rows Processed: 4724525, Total Chunk Time: 0.041 seconds ## Rows Read: 20501, Total Rows Processed: 4745026, Total Chunk Time: 0.601 seconds ## Rows Read: 20534, Total Rows Processed: 4765560, Total Chunk Time: 0.043 seconds ## Rows Read: 20551, Total Rows Processed: 4786111, Total Chunk Time: 0.042 seconds ## Rows Read: 14707, Total Rows Processed: 4800818, Total Chunk Time: 0.042 seconds ## Rows Read: 20572, Total Rows Processed: 4821390, Total Chunk Time: 0.315 seconds ## Rows Read: 20409, Total Rows Processed: 4841799, Total Chunk Time: 0.046 seconds ## Rows Read: 20578, Total Rows Processed: 4862377, Total Chunk Time: 0.044 seconds ## Rows Read: 14695, Total Rows Processed: 4877072, Total Chunk Time: 0.043 seconds ## Rows Read: 20500, Total Rows Processed: 4897572, Total Chunk Time: 0.621 seconds ## Rows Read: 20577, Total Rows Processed: 4918149, Total Chunk Time: 0.041 seconds ## Rows Read: 20487, Total Rows Processed: 4938636, Total Chunk Time: 0.040 seconds ## Rows Read: 14719, Total Rows Processed: 4953355, Total Chunk Time: 0.039 seconds ## Rows Read: 20523, Total Rows Processed: 4973878, Total Chunk Time: 0.320 seconds ## Rows Read: 20520, Total Rows Processed: 4994398, Total Chunk Time: 0.042 seconds ## Rows Read: 20442, Total Rows Processed: 5014840, Total Chunk Time: 0.042 seconds ## Rows Read: 14749, Total Rows Processed: 5029589, Total Chunk Time: 0.042 seconds ## Rows Read: 20483, Total Rows Processed: 5050072, Total Chunk Time: 0.550 seconds ## Rows Read: 20539, Total Rows Processed: 5070611, Total Chunk Time: 0.044 seconds ## Rows Read: 20484, Total Rows Processed: 5091095, Total Chunk Time: 0.041 seconds ## Rows Read: 14709, Total Rows Processed: 5105804, Total Chunk Time: 0.041 seconds ## Rows Read: 20568, Total Rows Processed: 5126372, Total Chunk Time: 0.576 seconds ## Rows Read: 20510, Total Rows Processed: 5146882, Total Chunk Time: 0.044 seconds ## Rows Read: 20522, Total Rows Processed: 5167404, Total Chunk Time: 0.044 seconds ## Rows Read: 14676, Total Rows Processed: 5182080, Total Chunk Time: 0.044 seconds ## Rows Read: 20456, Total Rows Processed: 5202536, Total Chunk Time: 0.344 seconds ## Rows Read: 20565, Total Rows Processed: 5223101, Total Chunk Time: 0.045 seconds ## Rows Read: 20536, Total Rows Processed: 5243637, Total Chunk Time: 0.045 seconds ## Rows Read: 14675, Total Rows Processed: 5258312, Total Chunk Time: 0.045 seconds ## Rows Read: 20454, Total Rows Processed: 5278766, Total Chunk Time: 0.370 seconds ## Rows Read: 20536, Total Rows Processed: 5299302, Total Chunk Time: 0.043 seconds ## Rows Read: 20536, Total Rows Processed: 5319838, Total Chunk Time: 0.043 seconds ## Rows Read: 14690, Total Rows Processed: 5334528, Total Chunk Time: 0.042 seconds ## Rows Read: 20538, Total Rows Processed: 5355066, Total Chunk Time: 0.351 seconds ## Rows Read: 20455, Total Rows Processed: 5375521, Total Chunk Time: 0.042 seconds ## Rows Read: 20553, Total Rows Processed: 5396074, Total Chunk Time: 0.041 seconds ## Rows Read: 14626, Total Rows Processed: 5410700, Total Chunk Time: 0.041 seconds ## Rows Read: 20513, Total Rows Processed: 5431213, Total Chunk Time: 0.423 seconds ## Rows Read: 20447, Total Rows Processed: 5451660, Total Chunk Time: 0.047 seconds ## Rows Read: 20480, Total Rows Processed: 5472140, Total Chunk Time: 0.045 seconds ## Rows Read: 14663, Total Rows Processed: 5486803, Total Chunk Time: 0.045 seconds ## Rows Read: 20481, Total Rows Processed: 5507284, Total Chunk Time: 0.386 seconds ## Rows Read: 20471, Total Rows Processed: 5527755, Total Chunk Time: 0.041 seconds ## Rows Read: 20462, Total Rows Processed: 5548217, Total Chunk Time: 0.041 seconds ## Rows Read: 14735, Total Rows Processed: 5562952, Total Chunk Time: 0.043 seconds ## Rows Read: 20398, Total Rows Processed: 5583350, Total Chunk Time: 0.718 seconds ## Rows Read: 20458, Total Rows Processed: 5603808, Total Chunk Time: 0.038 seconds ## Rows Read: 20644, Total Rows Processed: 5624452, Total Chunk Time: 0.038 seconds ## Rows Read: 14686, Total Rows Processed: 5639138, Total Chunk Time: 0.038 seconds ## Rows Read: 20541, Total Rows Processed: 5659679, Total Chunk Time: 0.459 seconds ## Rows Read: 20462, Total Rows Processed: 5680141, Total Chunk Time: 0.038 seconds ## Rows Read: 20508, Total Rows Processed: 5700649, Total Chunk Time: 0.037 seconds ## Rows Read: 14695, Total Rows Processed: 5715344, Total Chunk Time: 0.037 seconds ## Rows Read: 20432, Total Rows Processed: 5735776, Total Chunk Time: 1.305 seconds ## Rows Read: 20512, Total Rows Processed: 5756288, Total Chunk Time: 0.041 seconds ## Rows Read: 20556, Total Rows Processed: 5776844, Total Chunk Time: 0.042 seconds ## Rows Read: 14700, Total Rows Processed: 5791544, Total Chunk Time: 0.048 seconds ## Rows Read: 20536, Total Rows Processed: 5812080, Total Chunk Time: 0.600 seconds ## Rows Read: 20490, Total Rows Processed: 5832570, Total Chunk Time: 0.047 seconds ## Rows Read: 20605, Total Rows Processed: 5853175, Total Chunk Time: 0.040 seconds ## Rows Read: 14617, Total Rows Processed: 5867792, Total Chunk Time: 0.040 seconds ## Rows Read: 20494, Total Rows Processed: 5888286, Total Chunk Time: 0.318 seconds ## Rows Read: 20573, Total Rows Processed: 5908859, Total Chunk Time: 0.041 seconds ## Rows Read: 20502, Total Rows Processed: 5929361, Total Chunk Time: 0.041 seconds ## Rows Read: 14718, Total Rows Processed: 5944079, Total Chunk Time: 0.041 seconds ## Rows Read: 20501, Total Rows Processed: 5964580, Total Chunk Time: 0.351 seconds ## Rows Read: 20500, Total Rows Processed: 5985080, Total Chunk Time: 0.041 seconds ## Rows Read: 20610, Total Rows Processed: 6005690, Total Chunk Time: 0.041 seconds ## Rows Read: 14721, Total Rows Processed: 6020411, Total Chunk Time: 0.041 seconds ## Rows Read: 20527, Total Rows Processed: 6040938, Total Chunk Time: 0.345 seconds ## Rows Read: 20594, Total Rows Processed: 6061532, Total Chunk Time: 0.043 seconds ## Rows Read: 20555, Total Rows Processed: 6082087, Total Chunk Time: 0.041 seconds ## Rows Read: 14675, Total Rows Processed: 6096762, Total Chunk Time: 0.041 seconds ##    user  system elapsed  ##   0.128   0.020  59.788 # user  system elapsed  # 13.436   3.616 142.326   logitPredict &lt;- RxXdfData(file.path(dataDir, &quot;logitPredict&quot;),                           fileSystem = hdfsFS)  rxPredict(logitModel, data = testDS, outData = logitPredict,           extraVarsToWrite = c(&quot;ArrDel15&quot;),           type = &#39;response&#39;, overwrite = TRUE) ## Rows Read: 20577, Total Rows Processed: 20577, Total Chunk Time: 2.461 seconds ## Rows Read: 20524, Total Rows Processed: 41101, Total Chunk Time: 2.789 seconds ## Rows Read: 20487, Total Rows Processed: 61588, Total Chunk Time: 0.027 seconds ## Rows Read: 14729, Total Rows Processed: 76317, Total Chunk Time: 0.022 seconds ## Rows Read: 20509, Total Rows Processed: 96826, Total Chunk Time: 0.586 seconds ## Rows Read: 20455, Total Rows Processed: 117281, Total Chunk Time: 0.028 seconds ## Rows Read: 20623, Total Rows Processed: 137904, Total Chunk Time: 0.028 seconds ## Rows Read: 14643, Total Rows Processed: 152547, Total Chunk Time: 0.024 seconds ## Rows Read: 20513, Total Rows Processed: 173060, Total Chunk Time: 0.309 seconds ## Rows Read: 20587, Total Rows Processed: 193647, Total Chunk Time: 0.031 seconds ## Rows Read: 20583, Total Rows Processed: 214230, Total Chunk Time: 0.027 seconds ## Rows Read: 14622, Total Rows Processed: 228852, Total Chunk Time: 0.024 seconds ## Rows Read: 20532, Total Rows Processed: 249384, Total Chunk Time: 0.340 seconds ## Rows Read: 20500, Total Rows Processed: 269884, Total Chunk Time: 0.027 seconds ## Rows Read: 20609, Total Rows Processed: 290493, Total Chunk Time: 0.029 seconds ## Rows Read: 14703, Total Rows Processed: 305196, Total Chunk Time: 0.027 seconds ## Rows Read: 20573, Total Rows Processed: 325769, Total Chunk Time: 0.428 seconds ## Rows Read: 20516, Total Rows Processed: 346285, Total Chunk Time: 0.028 seconds ## Rows Read: 20584, Total Rows Processed: 366869, Total Chunk Time: 0.026 seconds ## Rows Read: 14682, Total Rows Processed: 381551, Total Chunk Time: 0.023 seconds ## Rows Read: 20569, Total Rows Processed: 402120, Total Chunk Time: 0.343 seconds ## Rows Read: 20514, Total Rows Processed: 422634, Total Chunk Time: 0.028 seconds ## Rows Read: 20588, Total Rows Processed: 443222, Total Chunk Time: 0.027 seconds ## Rows Read: 14649, Total Rows Processed: 457871, Total Chunk Time: 0.023 seconds ## Rows Read: 20520, Total Rows Processed: 478391, Total Chunk Time: 0.341 seconds ## Rows Read: 20495, Total Rows Processed: 498886, Total Chunk Time: 0.028 seconds ## Rows Read: 20651, Total Rows Processed: 519537, Total Chunk Time: 0.026 seconds ## Rows Read: 14678, Total Rows Processed: 534215, Total Chunk Time: 0.022 seconds ## Rows Read: 20525, Total Rows Processed: 554740, Total Chunk Time: 0.291 seconds ## Rows Read: 20535, Total Rows Processed: 575275, Total Chunk Time: 0.028 seconds ## Rows Read: 20575, Total Rows Processed: 595850, Total Chunk Time: 0.029 seconds ## Rows Read: 14686, Total Rows Processed: 610536, Total Chunk Time: 0.024 seconds ## Rows Read: 20585, Total Rows Processed: 631121, Total Chunk Time: 0.384 seconds ## Rows Read: 20456, Total Rows Processed: 651577, Total Chunk Time: 0.028 seconds ## Rows Read: 20564, Total Rows Processed: 672141, Total Chunk Time: 0.029 seconds ## Rows Read: 14710, Total Rows Processed: 686851, Total Chunk Time: 0.025 seconds ## Rows Read: 20561, Total Rows Processed: 707412, Total Chunk Time: 0.310 seconds ## Rows Read: 20506, Total Rows Processed: 727918, Total Chunk Time: 0.031 seconds ## Rows Read: 20560, Total Rows Processed: 748478, Total Chunk Time: 0.026 seconds ## Rows Read: 14680, Total Rows Processed: 763158, Total Chunk Time: 0.024 seconds ## Rows Read: 20451, Total Rows Processed: 783609, Total Chunk Time: 0.323 seconds ## Rows Read: 20605, Total Rows Processed: 804214, Total Chunk Time: 0.033 seconds ## Rows Read: 20533, Total Rows Processed: 824747, Total Chunk Time: 0.028 seconds ## Rows Read: 14678, Total Rows Processed: 839425, Total Chunk Time: 0.025 seconds ## Rows Read: 20543, Total Rows Processed: 859968, Total Chunk Time: 0.344 seconds ## Rows Read: 20446, Total Rows Processed: 880414, Total Chunk Time: 0.028 seconds ## Rows Read: 20573, Total Rows Processed: 900987, Total Chunk Time: 0.027 seconds ## Rows Read: 14665, Total Rows Processed: 915652, Total Chunk Time: 0.025 seconds ## Rows Read: 20523, Total Rows Processed: 936175, Total Chunk Time: 0.380 seconds ## Rows Read: 20523, Total Rows Processed: 956698, Total Chunk Time: 0.032 seconds ## Rows Read: 20543, Total Rows Processed: 977241, Total Chunk Time: 0.030 seconds ## Rows Read: 14628, Total Rows Processed: 991869, Total Chunk Time: 0.026 seconds ## Rows Read: 20489, Total Rows Processed: 1012358, Total Chunk Time: 0.358 seconds ## Rows Read: 20521, Total Rows Processed: 1032879, Total Chunk Time: 0.033 seconds ## Rows Read: 20542, Total Rows Processed: 1053421, Total Chunk Time: 0.029 seconds ## Rows Read: 14690, Total Rows Processed: 1068111, Total Chunk Time: 0.024 seconds ## Rows Read: 20514, Total Rows Processed: 1088625, Total Chunk Time: 0.379 seconds ## Rows Read: 20501, Total Rows Processed: 1109126, Total Chunk Time: 0.029 seconds ## Rows Read: 20565, Total Rows Processed: 1129691, Total Chunk Time: 0.027 seconds ## Rows Read: 14654, Total Rows Processed: 1144345, Total Chunk Time: 0.023 seconds ## Rows Read: 20549, Total Rows Processed: 1164894, Total Chunk Time: 0.482 seconds ## Rows Read: 20465, Total Rows Processed: 1185359, Total Chunk Time: 0.034 seconds ## Rows Read: 20527, Total Rows Processed: 1205886, Total Chunk Time: 0.030 seconds ## Rows Read: 14646, Total Rows Processed: 1220532, Total Chunk Time: 0.029 seconds ## Rows Read: 20510, Total Rows Processed: 1241042, Total Chunk Time: 0.479 seconds ## Rows Read: 20576, Total Rows Processed: 1261618, Total Chunk Time: 0.027 seconds ## Rows Read: 20392, Total Rows Processed: 1282010, Total Chunk Time: 0.026 seconds ## Rows Read: 14688, Total Rows Processed: 1296698, Total Chunk Time: 0.023 seconds ## Rows Read: 20511, Total Rows Processed: 1317209, Total Chunk Time: 0.591 seconds ## Rows Read: 20520, Total Rows Processed: 1337729, Total Chunk Time: 0.028 seconds ## Rows Read: 20428, Total Rows Processed: 1358157, Total Chunk Time: 0.028 seconds ## Rows Read: 14674, Total Rows Processed: 1372831, Total Chunk Time: 0.024 seconds ## Rows Read: 20410, Total Rows Processed: 1393241, Total Chunk Time: 0.549 seconds ## Rows Read: 20448, Total Rows Processed: 1413689, Total Chunk Time: 0.027 seconds ## Rows Read: 20614, Total Rows Processed: 1434303, Total Chunk Time: 0.027 seconds ## Rows Read: 14668, Total Rows Processed: 1448971, Total Chunk Time: 0.024 seconds ## Rows Read: 20440, Total Rows Processed: 1469411, Total Chunk Time: 0.708 seconds ## Rows Read: 20558, Total Rows Processed: 1489969, Total Chunk Time: 0.027 seconds ## Rows Read: 20481, Total Rows Processed: 1510450, Total Chunk Time: 0.027 seconds ## Rows Read: 14652, Total Rows Processed: 1525102, Total Chunk Time: 0.024 seconds ## Rows Read: 20498, Total Rows Processed: 1545600, Total Chunk Time: 0.492 seconds ## Rows Read: 20597, Total Rows Processed: 1566197, Total Chunk Time: 0.027 seconds ## Rows Read: 20500, Total Rows Processed: 1586697, Total Chunk Time: 0.027 seconds ## Rows Read: 14590, Total Rows Processed: 1601287, Total Chunk Time: 0.024 seconds ## Rows Read: 20563, Total Rows Processed: 1621850, Total Chunk Time: 1.001 seconds ## Rows Read: 20468, Total Rows Processed: 1642318, Total Chunk Time: 0.031 seconds ## Rows Read: 20504, Total Rows Processed: 1662822, Total Chunk Time: 0.028 seconds ## Rows Read: 14647, Total Rows Processed: 1677469, Total Chunk Time: 0.033 seconds ## Rows Read: 20545, Total Rows Processed: 1698014, Total Chunk Time: 0.336 seconds ## Rows Read: 20521, Total Rows Processed: 1718535, Total Chunk Time: 0.031 seconds ## Rows Read: 20413, Total Rows Processed: 1738948, Total Chunk Time: 0.028 seconds ## Rows Read: 14748, Total Rows Processed: 1753696, Total Chunk Time: 0.024 seconds ## Rows Read: 20503, Total Rows Processed: 1774199, Total Chunk Time: 0.400 seconds ## Rows Read: 20683, Total Rows Processed: 1794882, Total Chunk Time: 0.034 seconds ## Rows Read: 20372, Total Rows Processed: 1815254, Total Chunk Time: 0.029 seconds ## Rows Read: 14649, Total Rows Processed: 1829903, Total Chunk Time: 0.025 seconds ## Rows Read: 20478, Total Rows Processed: 1850381, Total Chunk Time: 0.441 seconds ## Rows Read: 20533, Total Rows Processed: 1870914, Total Chunk Time: 0.028 seconds ## Rows Read: 20581, Total Rows Processed: 1891495, Total Chunk Time: 0.027 seconds ## Rows Read: 14662, Total Rows Processed: 1906157, Total Chunk Time: 0.025 seconds ## Rows Read: 20475, Total Rows Processed: 1926632, Total Chunk Time: 0.423 seconds ## Rows Read: 20514, Total Rows Processed: 1947146, Total Chunk Time: 0.031 seconds ## Rows Read: 20556, Total Rows Processed: 1967702, Total Chunk Time: 0.028 seconds ## Rows Read: 14698, Total Rows Processed: 1982400, Total Chunk Time: 0.025 seconds ## Rows Read: 20581, Total Rows Processed: 2002981, Total Chunk Time: 0.345 seconds ## Rows Read: 20490, Total Rows Processed: 2023471, Total Chunk Time: 0.028 seconds ## Rows Read: 20546, Total Rows Processed: 2044017, Total Chunk Time: 0.025 seconds ## Rows Read: 14635, Total Rows Processed: 2058652, Total Chunk Time: 0.022 seconds ## Rows Read: 20462, Total Rows Processed: 2079114, Total Chunk Time: 0.374 seconds ## Rows Read: 20599, Total Rows Processed: 2099713, Total Chunk Time: 0.028 seconds ## Rows Read: 20516, Total Rows Processed: 2120229, Total Chunk Time: 0.026 seconds ## Rows Read: 14618, Total Rows Processed: 2134847, Total Chunk Time: 0.024 seconds ## Rows Read: 20602, Total Rows Processed: 2155449, Total Chunk Time: 0.391 seconds ## Rows Read: 20582, Total Rows Processed: 2176031, Total Chunk Time: 0.028 seconds ## Rows Read: 20383, Total Rows Processed: 2196414, Total Chunk Time: 0.027 seconds ## Rows Read: 14681, Total Rows Processed: 2211095, Total Chunk Time: 0.024 seconds ## Rows Read: 20561, Total Rows Processed: 2231656, Total Chunk Time: 0.684 seconds ## Rows Read: 20513, Total Rows Processed: 2252169, Total Chunk Time: 0.027 seconds ## Rows Read: 20515, Total Rows Processed: 2272684, Total Chunk Time: 0.027 seconds ## Rows Read: 14641, Total Rows Processed: 2287325, Total Chunk Time: 0.024 seconds ## Rows Read: 20501, Total Rows Processed: 2307826, Total Chunk Time: 0.369 seconds ## Rows Read: 20510, Total Rows Processed: 2328336, Total Chunk Time: 0.029 seconds ## Rows Read: 20608, Total Rows Processed: 2348944, Total Chunk Time: 0.030 seconds ## Rows Read: 14594, Total Rows Processed: 2363538, Total Chunk Time: 0.026 seconds ## Rows Read: 20485, Total Rows Processed: 2384023, Total Chunk Time: 0.647 seconds ## Rows Read: 20523, Total Rows Processed: 2404546, Total Chunk Time: 0.030 seconds ## Rows Read: 20526, Total Rows Processed: 2425072, Total Chunk Time: 0.029 seconds ## Rows Read: 14626, Total Rows Processed: 2439698, Total Chunk Time: 0.026 seconds ## Rows Read: 20549, Total Rows Processed: 2460247, Total Chunk Time: 0.472 seconds ## Rows Read: 20427, Total Rows Processed: 2480674, Total Chunk Time: 0.028 seconds ## Rows Read: 20594, Total Rows Processed: 2501268, Total Chunk Time: 0.026 seconds ## Rows Read: 14637, Total Rows Processed: 2515905, Total Chunk Time: 0.024 seconds ## Rows Read: 20498, Total Rows Processed: 2536403, Total Chunk Time: 0.777 seconds ## Rows Read: 20505, Total Rows Processed: 2556908, Total Chunk Time: 0.032 seconds ## Rows Read: 20444, Total Rows Processed: 2577352, Total Chunk Time: 0.029 seconds ## Rows Read: 14686, Total Rows Processed: 2592038, Total Chunk Time: 0.025 seconds ## Rows Read: 20534, Total Rows Processed: 2612572, Total Chunk Time: 1.311 seconds ## Rows Read: 20404, Total Rows Processed: 2632976, Total Chunk Time: 0.029 seconds ## Rows Read: 20496, Total Rows Processed: 2653472, Total Chunk Time: 0.027 seconds ## Rows Read: 14685, Total Rows Processed: 2668157, Total Chunk Time: 0.025 seconds ## Rows Read: 20582, Total Rows Processed: 2688739, Total Chunk Time: 0.760 seconds ## Rows Read: 20414, Total Rows Processed: 2709153, Total Chunk Time: 0.028 seconds ## Rows Read: 20562, Total Rows Processed: 2729715, Total Chunk Time: 0.028 seconds ## Rows Read: 14598, Total Rows Processed: 2744313, Total Chunk Time: 0.025 seconds ## Rows Read: 20432, Total Rows Processed: 2764745, Total Chunk Time: 0.856 seconds ## Rows Read: 20467, Total Rows Processed: 2785212, Total Chunk Time: 0.027 seconds ## Rows Read: 20623, Total Rows Processed: 2805835, Total Chunk Time: 0.027 seconds ## Rows Read: 14636, Total Rows Processed: 2820471, Total Chunk Time: 0.024 seconds ## Rows Read: 20462, Total Rows Processed: 2840933, Total Chunk Time: 0.558 seconds ## Rows Read: 20517, Total Rows Processed: 2861450, Total Chunk Time: 0.028 seconds ## Rows Read: 20419, Total Rows Processed: 2881869, Total Chunk Time: 0.028 seconds ## Rows Read: 14688, Total Rows Processed: 2896557, Total Chunk Time: 0.024 seconds ## Rows Read: 20451, Total Rows Processed: 2917008, Total Chunk Time: 0.722 seconds ## Rows Read: 20546, Total Rows Processed: 2937554, Total Chunk Time: 0.035 seconds ## Rows Read: 20457, Total Rows Processed: 2958011, Total Chunk Time: 0.029 seconds ## Rows Read: 14651, Total Rows Processed: 2972662, Total Chunk Time: 0.030 seconds ## Rows Read: 20502, Total Rows Processed: 2993164, Total Chunk Time: 0.574 seconds ## Rows Read: 20460, Total Rows Processed: 3013624, Total Chunk Time: 0.028 seconds ## Rows Read: 20477, Total Rows Processed: 3034101, Total Chunk Time: 0.028 seconds ## Rows Read: 14654, Total Rows Processed: 3048755, Total Chunk Time: 0.024 seconds ## Rows Read: 20441, Total Rows Processed: 3069196, Total Chunk Time: 0.393 seconds ## Rows Read: 20479, Total Rows Processed: 3089675, Total Chunk Time: 0.027 seconds ## Rows Read: 20585, Total Rows Processed: 3110260, Total Chunk Time: 0.026 seconds ## Rows Read: 14603, Total Rows Processed: 3124863, Total Chunk Time: 0.023 seconds ## Rows Read: 20452, Total Rows Processed: 3145315, Total Chunk Time: 0.621 seconds ## Rows Read: 20463, Total Rows Processed: 3165778, Total Chunk Time: 0.027 seconds ## Rows Read: 20570, Total Rows Processed: 3186348, Total Chunk Time: 0.026 seconds ## Rows Read: 14622, Total Rows Processed: 3200970, Total Chunk Time: 0.023 seconds ## Rows Read: 20543, Total Rows Processed: 3221513, Total Chunk Time: 0.292 seconds ## Rows Read: 20478, Total Rows Processed: 3241991, Total Chunk Time: 0.028 seconds ## Rows Read: 20462, Total Rows Processed: 3262453, Total Chunk Time: 0.026 seconds ## Rows Read: 14622, Total Rows Processed: 3277075, Total Chunk Time: 0.026 seconds ## Rows Read: 20470, Total Rows Processed: 3297545, Total Chunk Time: 0.409 seconds ## Rows Read: 20517, Total Rows Processed: 3318062, Total Chunk Time: 0.026 seconds ## Rows Read: 20497, Total Rows Processed: 3338559, Total Chunk Time: 0.026 seconds ## Rows Read: 14656, Total Rows Processed: 3353215, Total Chunk Time: 0.023 seconds ## Rows Read: 20446, Total Rows Processed: 3373661, Total Chunk Time: 0.354 seconds ## Rows Read: 20508, Total Rows Processed: 3394169, Total Chunk Time: 0.028 seconds ## Rows Read: 20485, Total Rows Processed: 3414654, Total Chunk Time: 0.027 seconds ## Rows Read: 14667, Total Rows Processed: 3429321, Total Chunk Time: 0.026 seconds ## Rows Read: 20461, Total Rows Processed: 3449782, Total Chunk Time: 0.381 seconds ## Rows Read: 20573, Total Rows Processed: 3470355, Total Chunk Time: 0.026 seconds ## Rows Read: 20482, Total Rows Processed: 3490837, Total Chunk Time: 0.025 seconds ## Rows Read: 14610, Total Rows Processed: 3505447, Total Chunk Time: 0.024 seconds ## Rows Read: 20546, Total Rows Processed: 3525993, Total Chunk Time: 0.401 seconds ## Rows Read: 20562, Total Rows Processed: 3546555, Total Chunk Time: 0.029 seconds ## Rows Read: 20464, Total Rows Processed: 3567019, Total Chunk Time: 0.027 seconds ## Rows Read: 14558, Total Rows Processed: 3581577, Total Chunk Time: 0.025 seconds ## Rows Read: 20512, Total Rows Processed: 3602089, Total Chunk Time: 0.309 seconds ## Rows Read: 20444, Total Rows Processed: 3622533, Total Chunk Time: 0.029 seconds ## Rows Read: 20452, Total Rows Processed: 3642985, Total Chunk Time: 0.028 seconds ## Rows Read: 14697, Total Rows Processed: 3657682, Total Chunk Time: 0.025 seconds ## Rows Read: 20501, Total Rows Processed: 3678183, Total Chunk Time: 0.340 seconds ## Rows Read: 20526, Total Rows Processed: 3698709, Total Chunk Time: 0.028 seconds ## Rows Read: 20543, Total Rows Processed: 3719252, Total Chunk Time: 0.027 seconds ## Rows Read: 14620, Total Rows Processed: 3733872, Total Chunk Time: 0.025 seconds ## Rows Read: 20489, Total Rows Processed: 3754361, Total Chunk Time: 0.379 seconds ## Rows Read: 20596, Total Rows Processed: 3774957, Total Chunk Time: 0.037 seconds ## Rows Read: 20469, Total Rows Processed: 3795426, Total Chunk Time: 0.027 seconds ## Rows Read: 14657, Total Rows Processed: 3810083, Total Chunk Time: 0.026 seconds ## Rows Read: 20469, Total Rows Processed: 3830552, Total Chunk Time: 0.337 seconds ## Rows Read: 20502, Total Rows Processed: 3851054, Total Chunk Time: 0.029 seconds ## Rows Read: 20524, Total Rows Processed: 3871578, Total Chunk Time: 0.025 seconds ## Rows Read: 14694, Total Rows Processed: 3886272, Total Chunk Time: 0.027 seconds ## Rows Read: 20503, Total Rows Processed: 3906775, Total Chunk Time: 0.447 seconds ## Rows Read: 20474, Total Rows Processed: 3927249, Total Chunk Time: 0.026 seconds ## Rows Read: 20573, Total Rows Processed: 3947822, Total Chunk Time: 0.025 seconds ## Rows Read: 14637, Total Rows Processed: 3962459, Total Chunk Time: 0.022 seconds ## Rows Read: 20566, Total Rows Processed: 3983025, Total Chunk Time: 0.305 seconds ## Rows Read: 20441, Total Rows Processed: 4003466, Total Chunk Time: 0.027 seconds ## Rows Read: 20457, Total Rows Processed: 4023923, Total Chunk Time: 0.027 seconds ## Rows Read: 14706, Total Rows Processed: 4038629, Total Chunk Time: 0.024 seconds ## Rows Read: 20459, Total Rows Processed: 4059088, Total Chunk Time: 0.479 seconds ## Rows Read: 20431, Total Rows Processed: 4079519, Total Chunk Time: 0.028 seconds ## Rows Read: 20576, Total Rows Processed: 4100095, Total Chunk Time: 0.027 seconds ## Rows Read: 14680, Total Rows Processed: 4114775, Total Chunk Time: 0.025 seconds ## Rows Read: 20398, Total Rows Processed: 4135173, Total Chunk Time: 0.496 seconds ## Rows Read: 20496, Total Rows Processed: 4155669, Total Chunk Time: 0.035 seconds ## Rows Read: 20517, Total Rows Processed: 4176186, Total Chunk Time: 0.028 seconds ## Rows Read: 14696, Total Rows Processed: 4190882, Total Chunk Time: 0.024 seconds ## Rows Read: 20512, Total Rows Processed: 4211394, Total Chunk Time: 0.432 seconds ## Rows Read: 20466, Total Rows Processed: 4231860, Total Chunk Time: 0.027 seconds ## Rows Read: 20475, Total Rows Processed: 4252335, Total Chunk Time: 0.026 seconds ## Rows Read: 14678, Total Rows Processed: 4267013, Total Chunk Time: 0.024 seconds ## Rows Read: 20550, Total Rows Processed: 4287563, Total Chunk Time: 1.001 seconds ## Rows Read: 20479, Total Rows Processed: 4308042, Total Chunk Time: 0.028 seconds ## Rows Read: 20505, Total Rows Processed: 4328547, Total Chunk Time: 0.026 seconds ## Rows Read: 14669, Total Rows Processed: 4343216, Total Chunk Time: 0.023 seconds ## Rows Read: 20470, Total Rows Processed: 4363686, Total Chunk Time: 0.380 seconds ## Rows Read: 20486, Total Rows Processed: 4384172, Total Chunk Time: 0.027 seconds ## Rows Read: 20511, Total Rows Processed: 4404683, Total Chunk Time: 0.026 seconds ## Rows Read: 14714, Total Rows Processed: 4419397, Total Chunk Time: 0.023 seconds ## Rows Read: 20599, Total Rows Processed: 4439996, Total Chunk Time: 0.381 seconds ## Rows Read: 20501, Total Rows Processed: 4460497, Total Chunk Time: 0.027 seconds ## Rows Read: 20479, Total Rows Processed: 4480976, Total Chunk Time: 0.027 seconds ## Rows Read: 14666, Total Rows Processed: 4495642, Total Chunk Time: 0.023 seconds ## Rows Read: 20540, Total Rows Processed: 4516182, Total Chunk Time: 0.379 seconds ## Rows Read: 20514, Total Rows Processed: 4536696, Total Chunk Time: 0.028 seconds ## Rows Read: 20555, Total Rows Processed: 4557251, Total Chunk Time: 0.027 seconds ## Rows Read: 14687, Total Rows Processed: 4571938, Total Chunk Time: 0.024 seconds ## Rows Read: 20529, Total Rows Processed: 4592467, Total Chunk Time: 0.456 seconds ## Rows Read: 20502, Total Rows Processed: 4612969, Total Chunk Time: 0.026 seconds ## Rows Read: 20628, Total Rows Processed: 4633597, Total Chunk Time: 0.026 seconds ## Rows Read: 14652, Total Rows Processed: 4648249, Total Chunk Time: 0.023 seconds ## Rows Read: 20561, Total Rows Processed: 4668810, Total Chunk Time: 1.167 seconds ## Rows Read: 20461, Total Rows Processed: 4689271, Total Chunk Time: 0.032 seconds ## Rows Read: 20577, Total Rows Processed: 4709848, Total Chunk Time: 0.027 seconds ## Rows Read: 14677, Total Rows Processed: 4724525, Total Chunk Time: 0.023 seconds ## Rows Read: 20501, Total Rows Processed: 4745026, Total Chunk Time: 0.441 seconds ## Rows Read: 20534, Total Rows Processed: 4765560, Total Chunk Time: 0.027 seconds ## Rows Read: 20551, Total Rows Processed: 4786111, Total Chunk Time: 0.027 seconds ## Rows Read: 14707, Total Rows Processed: 4800818, Total Chunk Time: 0.026 seconds ## Rows Read: 20572, Total Rows Processed: 4821390, Total Chunk Time: 0.350 seconds ## Rows Read: 20409, Total Rows Processed: 4841799, Total Chunk Time: 0.034 seconds ## Rows Read: 20578, Total Rows Processed: 4862377, Total Chunk Time: 0.030 seconds ## Rows Read: 14695, Total Rows Processed: 4877072, Total Chunk Time: 0.024 seconds ## Rows Read: 20500, Total Rows Processed: 4897572, Total Chunk Time: 0.582 seconds ## Rows Read: 20577, Total Rows Processed: 4918149, Total Chunk Time: 0.031 seconds ## Rows Read: 20487, Total Rows Processed: 4938636, Total Chunk Time: 0.027 seconds ## Rows Read: 14719, Total Rows Processed: 4953355, Total Chunk Time: 0.024 seconds ## Rows Read: 20523, Total Rows Processed: 4973878, Total Chunk Time: 1.401 seconds ## Rows Read: 20520, Total Rows Processed: 4994398, Total Chunk Time: 0.028 seconds ## Rows Read: 20442, Total Rows Processed: 5014840, Total Chunk Time: 0.025 seconds ## Rows Read: 14749, Total Rows Processed: 5029589, Total Chunk Time: 0.023 seconds ## Rows Read: 20483, Total Rows Processed: 5050072, Total Chunk Time: 0.578 seconds ## Rows Read: 20539, Total Rows Processed: 5070611, Total Chunk Time: 0.027 seconds ## Rows Read: 20484, Total Rows Processed: 5091095, Total Chunk Time: 0.025 seconds ## Rows Read: 14709, Total Rows Processed: 5105804, Total Chunk Time: 0.023 seconds ## Rows Read: 20568, Total Rows Processed: 5126372, Total Chunk Time: 0.675 seconds ## Rows Read: 20510, Total Rows Processed: 5146882, Total Chunk Time: 0.030 seconds ## Rows Read: 20522, Total Rows Processed: 5167404, Total Chunk Time: 0.029 seconds ## Rows Read: 14676, Total Rows Processed: 5182080, Total Chunk Time: 0.025 seconds ## Rows Read: 20456, Total Rows Processed: 5202536, Total Chunk Time: 0.793 seconds ## Rows Read: 20565, Total Rows Processed: 5223101, Total Chunk Time: 0.032 seconds ## Rows Read: 20536, Total Rows Processed: 5243637, Total Chunk Time: 0.028 seconds ## Rows Read: 14675, Total Rows Processed: 5258312, Total Chunk Time: 0.025 seconds ## Rows Read: 20454, Total Rows Processed: 5278766, Total Chunk Time: 0.517 seconds ## Rows Read: 20536, Total Rows Processed: 5299302, Total Chunk Time: 0.034 seconds ## Rows Read: 20536, Total Rows Processed: 5319838, Total Chunk Time: 0.031 seconds ## Rows Read: 14690, Total Rows Processed: 5334528, Total Chunk Time: 0.027 seconds ## Rows Read: 20538, Total Rows Processed: 5355066, Total Chunk Time: 0.480 seconds ## Rows Read: 20455, Total Rows Processed: 5375521, Total Chunk Time: 0.028 seconds ## Rows Read: 20553, Total Rows Processed: 5396074, Total Chunk Time: 0.030 seconds ## Rows Read: 14626, Total Rows Processed: 5410700, Total Chunk Time: 0.024 seconds ## Rows Read: 20513, Total Rows Processed: 5431213, Total Chunk Time: 0.325 seconds ## Rows Read: 20447, Total Rows Processed: 5451660, Total Chunk Time: 0.029 seconds ## Rows Read: 20480, Total Rows Processed: 5472140, Total Chunk Time: 0.027 seconds ## Rows Read: 14663, Total Rows Processed: 5486803, Total Chunk Time: 0.024 seconds ## Rows Read: 20481, Total Rows Processed: 5507284, Total Chunk Time: 0.459 seconds ## Rows Read: 20471, Total Rows Processed: 5527755, Total Chunk Time: 0.031 seconds ## Rows Read: 20462, Total Rows Processed: 5548217, Total Chunk Time: 0.027 seconds ## Rows Read: 14735, Total Rows Processed: 5562952, Total Chunk Time: 0.024 seconds ## Rows Read: 20398, Total Rows Processed: 5583350, Total Chunk Time: 0.402 seconds ## Rows Read: 20458, Total Rows Processed: 5603808, Total Chunk Time: 0.030 seconds ## Rows Read: 20644, Total Rows Processed: 5624452, Total Chunk Time: 0.027 seconds ## Rows Read: 14686, Total Rows Processed: 5639138, Total Chunk Time: 0.023 seconds ## Rows Read: 20541, Total Rows Processed: 5659679, Total Chunk Time: 0.835 seconds ## Rows Read: 20462, Total Rows Processed: 5680141, Total Chunk Time: 0.033 seconds ## Rows Read: 20508, Total Rows Processed: 5700649, Total Chunk Time: 0.033 seconds ## Rows Read: 14695, Total Rows Processed: 5715344, Total Chunk Time: 0.027 seconds ## Rows Read: 20432, Total Rows Processed: 5735776, Total Chunk Time: 0.421 seconds ## Rows Read: 20512, Total Rows Processed: 5756288, Total Chunk Time: 0.029 seconds ## Rows Read: 20556, Total Rows Processed: 5776844, Total Chunk Time: 0.028 seconds ## Rows Read: 14700, Total Rows Processed: 5791544, Total Chunk Time: 0.025 seconds ## Rows Read: 20536, Total Rows Processed: 5812080, Total Chunk Time: 0.741 seconds ## Rows Read: 20490, Total Rows Processed: 5832570, Total Chunk Time: 0.028 seconds ## Rows Read: 20605, Total Rows Processed: 5853175, Total Chunk Time: 0.028 seconds ## Rows Read: 14617, Total Rows Processed: 5867792, Total Chunk Time: 0.025 seconds ## Rows Read: 20494, Total Rows Processed: 5888286, Total Chunk Time: 0.478 seconds ## Rows Read: 20573, Total Rows Processed: 5908859, Total Chunk Time: 0.029 seconds ## Rows Read: 20502, Total Rows Processed: 5929361, Total Chunk Time: 0.028 seconds ## Rows Read: 14718, Total Rows Processed: 5944079, Total Chunk Time: 0.025 seconds ## Rows Read: 20501, Total Rows Processed: 5964580, Total Chunk Time: 0.454 seconds ## Rows Read: 20500, Total Rows Processed: 5985080, Total Chunk Time: 0.027 seconds ## Rows Read: 20610, Total Rows Processed: 6005690, Total Chunk Time: 0.027 seconds ## Rows Read: 14721, Total Rows Processed: 6020411, Total Chunk Time: 0.024 seconds ## Rows Read: 20527, Total Rows Processed: 6040938, Total Chunk Time: 0.664 seconds ## Rows Read: 20594, Total Rows Processed: 6061532, Total Chunk Time: 0.028 seconds ## Rows Read: 20555, Total Rows Processed: 6082087, Total Chunk Time: 0.026 seconds ## Rows Read: 14675, Total Rows Processed: 6096762, Total Chunk Time: 0.023 seconds # Calculate ROC and Area Under the Curve (AUC).  logitRoc &lt;- rxRoc(&quot;ArrDel15&quot;, &quot;ArrDel15_Pred&quot;, logitPredict) logitAuc &lt;- rxAuc(logitRoc)  plot(logitRoc)  # Calculate ROC and Area Under the Curve (AUC)  treeRoc &lt;- rxRoc(&quot;ArrDel15&quot;, &quot;ArrDel15_Pred&quot;, treePredict) treeAuc &lt;- rxAuc(treeRoc)  plot(treeRoc)  save(dTreeModel, file = &quot;dTreeModel.RData&quot;)   "],
["references.html", "Chapter 10 References", " Chapter 10 References     "]
]
