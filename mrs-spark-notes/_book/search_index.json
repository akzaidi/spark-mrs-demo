[
["starting-your-machine-learning-pipeline.html", "Chapter 4 Starting Your Machine Learning Pipeline 4.1 todo 4.2 Finding the SparkR Library 4.3 Creating a Spark Context 4.4 Creating DataFrames", " Chapter 4 Starting Your Machine Learning Pipeline  4.1 todo  since rmarkdown/knitr start a new session when building docs, can’t access current spark context either build from scratch, or persist rdds/dfs alternatively, make ipynb in jupyter and save as md and render in book as md no console output from rxHadoop shell wrappers, need to sink and show if want to see output to cache results, might make sense to persist DFs and then reuse when needed  The first steps to start your machine learning and data science pipeline is to set your compute environment, and point to your data. In Spark, you’ll need to create a SparkContext. This constructor provides Spark with the details of the cluster: how and where to access it, additional package directories, etc. You’ll use this constructor to create new RDDs or DataFrames.   4.2 Finding the SparkR Library In order to create a Spark Context from your RStudio Server environment, you’ll need to access the SparkR library. Since Spark 1.4, SparkR has shipped the R API directly with its core implementation. Therefore, since 1.4 you do not need to install Spark from CRAN or a development version from github, but you need to add the SparkR library to your library paths in order to access it. A system variable called “SPARK_HOME” has been set that points to the Spark installation directory, and in it you’ll find subdirectories, “R/lib”, containing the SparkR library. list.files(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;)) ## [1] &quot;SparkR&quot;     &quot;sparkr.zip&quot; To add the SparkR library to your library paths, use the .libPaths function to include the directory in the search path for R’s library tree. The library paths could also be changed from in the Rprofile, either for the user or system wide. See the help on ?StartUp for more details on R’s startup mechanism. .libPaths(c(file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;, &quot;lib&quot;), .libPaths()))   4.3 Creating a Spark Context To create a SparkContext, you should use the spark.init function and pass in options for the environment parameters, application properties, any spark packages depended on, and any other additional Spark parameters. In order to create and manipulate DataFrames, we will need a SQLContext, which can be created from the SparkContext using the sparkRSQL.init function. library(SparkR)  sparkEnvir &lt;- list(spark.executor.instance = &#39;10&#39;,                    spark.yarn.executor.memoryOverhead = &#39;8000&#39;)  sc &lt;- sparkR.init(   sparkEnvir = sparkEnvir,   sparkPackages = &quot;com.databricks:spark-csv_2.10:1.3.0&quot; )  sqlContext &lt;- sparkRSQL.init(sc) We added the sparkPackages argument and set it to the value of spark-csv package provided by databricks. This will allow us to read csv objects into Spark DataFrames. Databricks are a prominent vendor for Spark, and are developers of many Spark packages (think of them as the RStudio of the Spark ecosystem). After you are done using your Spark session, you can terminate your backend to Spark by running sparkR.stop().   4.4 Creating DataFrames Using our sqlContext variable, we can create DataFrames from local R data.frames, from Hive tables, or from other data sources. You could create a Spark DataFrame from a local R data.frame using the createDataFrame function,  4.4.1 From Local R data.frames Creating Spark DataFrames from local R data.frames might not seem like a great idea. After all, if it can fit in an R data.frame, that means it can fit in a single node’s memory, so why distribute it? It might be that you are testing your methods out on a sample dataset, and will scale out your analysis to larger datasets when your tests have passed. For expository purposes, this is a useful exercise, as it’ll expose you to the fundamentals of Spark DataFrames, and the SparkR syntax. We’ll import data from the nycflight13 package into a Spark DataFrame, and use it’s data aggregation functions to tabulate the number of flights to each destination of flights originating from JFK library(nycflights13) flights &lt;- createDataFrame(sqlContext, nycflights13::flights) jfk_flights &lt;- filter(flights, flights$origin == &quot;JFK&quot;) # Group the flights by destination and aggregate by the number of flights dest_flights &lt;- agg(group_by(jfk_flights, jfk_flights$dest), count = n(jfk_flights$dest)) # Now sort by the `count` column and print the first few rows head(arrange(dest_flights, desc(dest_flights$count))) This same analysis could be streamlined using the %&gt;% operator exposed by the magrittr package: library(magrittr) dest_flights &lt;- filter(flights, flights$origin == &quot;JFK&quot;) %&gt;%    group_by(flights$dest) %&gt;%    summarize(count = n(flights$dest)) dest_flights %&gt;% arrange(desc(dest_flights$count)) %&gt;% head   4.4.2 Creating DataFrames from CSV Files Since we imported the spark-csv package, we can import CSV files into DataFrames. We will be using the full airlines dataset, which I assume you have already ingested into Blob storage by following the steps in Section ??. We have saved in our data directory a couple of data directories (virtually, everything in Azure Storage is a simple blob!). Let’s see what we have in our directory using the rxHadoopListFiles command, which is simply a wrapper to hadoop shell command hadoop fs -ls data_dir &lt;- &quot;/user/RevoShare/alizaidi&quot; rxHadoopListFiles(data_dir) Taking a look at the individual directories, we can see how many files there are for the Airlines directory and Weather directory rxHadoopCommand(&quot;fs -ls /user/RevoShare/alizaidi/AirOnTimeCSV | head&quot;) rxHadoopCommand(&quot;fs -ls /user/RevoShare/alizaidi/delayDataLarge/WeatherRaw | head&quot;) Let’s read the airlines directory and the weather directory to Spark DataFrames. We will use the read.df function from the spark.csv package. airPath &lt;- file.path(data_dir, &quot;AirOnTimeCSV&quot;) weatherPath &lt;- file.path(data_dir, &quot;delayDataLarge&quot;, &quot;Weather&quot;) # pre-processed weather data  airDF &lt;- read.df(sqlContext, airPath, source = &quot;com.databricks.spark.csv&quot;,                   header = &quot;true&quot;, inferSchema = &quot;true&quot;) # user  system elapsed  # 0.724   0.680 378.561   weatherDF &lt;- read.df(sqlContext, weatherPath, source = &quot;com.databricks.spark.csv&quot;,                       header = &quot;true&quot;, inferSchema = &quot;true&quot;) Note that it took more than 6 minutes to load our airlines data into Spark DataFrames. However, subsequent operations on the airDF object will occur in-memory, and should be very fast. Let’s count the number of rows in each of our DataFrames and print the first few rows: library(SparkR) lapply(list(airDF, weatherDF), count) # [[1]] # [1] 148619655 #  # [[2]] # [1] 14829028  lapply(list(airDF, weatherDF), head) # [[1]] #   YEAR MONTH DAY_OF_MONTH DAY_OF_WEEK    FL_DATE UNIQUE_CARRIER TAIL_NUM FL_NUM # 1 1987    10            1           4 1987-10-01             AA               1 # 2 1987    10            2           5 1987-10-02             AA               1 # 3 1987    10            3           6 1987-10-03             AA               1 # 4 1987    10            4           7 1987-10-04             AA               1 # 5 1987    10            5           1 1987-10-05             AA               1 # 6 1987    10            6           2 1987-10-06             AA               1 #   ORIGIN_AIRPORT_ID ORIGIN ORIGIN_STATE_ABR DEST_AIRPORT_ID DEST DEST_STATE_ABR # 1             12478    JFK               NY           12892  LAX             CA # 2             12478    JFK               NY           12892  LAX             CA # 3             12478    JFK               NY           12892  LAX             CA # 4             12478    JFK               NY           12892  LAX             CA # 5             12478    JFK               NY           12892  LAX             CA # 6             12478    JFK               NY           12892  LAX             CA #   CRS_DEP_TIME DEP_TIME DEP_DELAY DEP_DELAY_NEW DEP_DEL15 DEP_DELAY_GROUP TAXI_OUT # 1          900      901         1             1         0               0          # 2          900      901         1             1         0               0          # 3          900      859        -1             0         0              -1          # 4          900      900         0             0         0               0          # 5          900      902         2             2         0               0          # 6          900      900         0             0         0               0          #   WHEELS_OFF WHEELS_ON TAXI_IN CRS_ARR_TIME ARR_TIME ARR_DELAY ARR_DELAY_NEW # 1                                      1152     1117       -35             0 # 2                                      1152     1137       -15             0 # 3                                      1152     1111       -41             0 # 4                                      1152     1116       -36             0 # 5                                      1152     1119       -33             0 # 6                                      1152       NA        NA            NA #   ARR_DEL15 ARR_DELAY_GROUP CANCELLED CANCELLATION_CODE DIVERTED CRS_ELAPSED_TIME # 1         0              -2         0                          0              352 # 2         0              -1         0                          0              352 # 3         0              -2         0                          0              352 # 4         0              -2         0                          0              352 # 5         0              -2         0                          0              352 # 6        NA              NA         0                          1              352 #   ACTUAL_ELAPSED_TIME AIR_TIME FLIGHTS DISTANCE DISTANCE_GROUP CARRIER_DELAY # 1                 316                1     2475             10               # 2                 336                1     2475             10               # 3                 312                1     2475             10               # 4                 316                1     2475             10               # 5                 317                1     2475             10               # 6                  NA                1     2475             10               #   WEATHER_DELAY NAS_DELAY SECURITY_DELAY LATE_AIRCRAFT_DELAY  # 1                                                             # 2                                                             # 3                                                             # 4                                                             # 5                                                             # 6                                                             #  # [[2]] #   Visibility DryBulbCelsius DewPointCelsius RelativeHumidity WindSpeed Altimeter # 1   4.000000       0.000000       -1.000000        92.000000  0.000000 29.690000 # 2  10.000000       7.000000       -3.000000        49.000000 11.000000 29.790000 # 3   3.000000       1.000000        0.000000        92.000000  3.000000 29.710000 # 4  10.000000       7.000000       -5.000000        42.000000 18.000000 29.710000 # 5   1.250000       3.000000        0.000000        82.000000  6.000000 29.720000 # 6  10.000000       8.000000       -4.000000        44.000000 14.000000 29.770000 #   AdjustedYear AdjustedMonth AdjustedDay AdjustedHour AirportID # 1         2007             5           4           18     15177 # 2         2007             5           4            6     15177 # 3         2007             5           4           17     15177 # 4         2007             5           4            9     15177 # 5         2007             5           4           10     15177 # 6         2007             5           4            7     15177   "],
["data-manipulation-with-sparkr.html", "Chapter 5 Data Manipulation with SparkR 5.1 Data Aggregations", " Chapter 5 Data Manipulation with SparkR Now that we have our two datasets saved as Spark DataFrames, we can conduct standard data manipulation techniques to visualize and explore our data. First, we’ll use the rename function to rename our columns, and the select function to select the columns we need. We’ll also transform the These SparkR functions look just like the verbs from teh dplyr package for data manipulation, but are designed to work with Spark DataFrames. system.time(airDF &lt;- rename(airDF,                 ArrDel15 = airDF$ARR_DEL15,                 Year = airDF$YEAR,                 Month = airDF$MONTH,                 DayofMonth = airDF$DAY_OF_MONTH,                 DayOfWeek = airDF$DAY_OF_WEEK,                 Carrier = airDF$UNIQUE_CARRIER,                 OriginAirportID = airDF$ORIGIN_AIRPORT_ID,                 DestAirportID = airDF$DEST_AIRPORT_ID,                 CRSDepTime = airDF$CRS_DEP_TIME,                 CRSArrTime =  airDF$CRS_ARR_TIME,                 Distance = airDF$DISTANCE,                 DepDelay = airDF$DEP_DELAY,                 ArrDelay = airDF$ARR_DELAY                 )             )   #  user  system elapsed    # 0.136   0.000   0.242   # Select desired columns from the flight data.  varsToKeep &lt;- c(&quot;ArrDel15&quot;, &quot;Year&quot;, &quot;Month&quot;, &quot;DayofMonth&quot;, &quot;DayOfWeek&quot;, &quot;Carrier&quot;,                 &quot;OriginAirportID&quot;, &quot;DestAirportID&quot;, &quot;CRSDepTime&quot;, &quot;CRSArrTime&quot;,                 &quot;Distance&quot;, &quot;DepDelay&quot;, &quot;ArrDelay&quot;) system.time(airDF &lt;- select(airDF, varsToKeep))   #  user  system elapsed    # 0.064   0.000   0.112   # Round down scheduled departure time to full hour. system.time(airDF$CRSDepTime &lt;- floor(airDF$CRSDepTime / 100))   # user  system elapsed    #  0.00    0.00    0.06   5.1 Data Aggregations SparkR is great at merges, and data aggregation. For instance, suppose we want to see the average departure delay for each carrier and arrange it in descending order. The following syntax makes that very easy. sum_df &lt;- airDF %&gt;% select(&quot;Carrier&quot;, &quot;DepDelay&quot;) %&gt;%    groupBy(airDF$Carrier) %&gt;%    summarize(count = n(airDF$Carrier),              ave_delay = mean(airDF$DepDelay))    # user  system elapsed    # 0.024   0.000   0.055  The syntax is almost exactly like the syntax from the dplyr package, and the %&gt;% operator makes chaining the additive methods exceptionally simple. Note that the above operation will not be run until we call upon the sum_df. It is for now, just a promise for deferred evaluation. In order to evaluate and bring the summarized data into an R data.frame, we can use the collect statement. sum_local &lt;- sum_df %&gt;% collect() library(dplyr) sum_local %&gt;% arrange(desc(ave_delay))   #  user  system elapsed    # 0.616   0.536 337.758  Now that our data resides as a local data.frame, we can plot it using any R plotting library. load(&quot;local_df.RData&quot;) library(rcdimple) sum_local %&gt;%    dimple(x =&quot;Carrier&quot;, y = &quot;ave_delay&quot;, z =  &quot;count&quot;, type = &quot;bar&quot;) %&gt;%   add_title(html = &quot;&lt;h4&gt;Average Delay in Minutes by Carrier&lt;/h4&gt;&quot; ) %&gt;%    zAxis(outputFormat = &quot;#,### &quot;)  {\"x\":{\"options\":{\"chart\":[],\"xAxis\":{\"type\":\"addCategoryAxis\"},\"yAxis\":{\"type\":\"addMeasureAxis\"},\"zAxis\":{\"type\":\"addMeasureAxis\",\"outputFormat\":\"#,### \"},\"colorAxis\":[],\"defaultColors\":[],\"layers\":[],\"legend\":[],\"x\":\"Carrier\",\"y\":\"ave_delay\",\"type\":\"bar\",\"z\":\"count\",\"title\":{\"text\":null,\"html\":\"Average Delay in Minutes by Carrier\\u003c/h4>\"}},\"data\":{\"Carrier\":[\"AA\",\"PA (1)\",\"TW\",\"TZ\",\"HA\",\"AS\",\"UA\",\"B6\",\"NW\",\"HP\",\"US\",\"OH\",\"OO\",\"PI\",\"VX\",\"CO\",\"ML (1)\",\"PS\",\"WN\",\"DH\",\"DL\",\"KH\",\"XE\",\"EA\",\"EV\",\"F9\",\"9E\",\"YV\",\"FL\",\"MQ\"],\"count\":[17140606,316167,3757747,208420,555683,3443588,14862404,1652137,10585760,3636682,15709733,1765828,5443169,873957,54742,8888536,70622,83617,20529039,693047,19168060,154381,3459389,919785,3384793,670653,1045396,1563254,2232262,5750198],\"ave_delay\":[8.09216330413803,5.53244244289068,7.65825114221727,5.55423481294241,-0.470471755160245,7.24013290246557,9.6410715967209,10.9892686446028,6.02139368193511,8.10779026658562,6.99896030027832,9.52695744411818,6.95087777743635,9.56033602798461,10.0396614090817,7.81891288846895,6.2296766743649,8.92810370334441,9.32814933847665,9.61263938968893,7.55365844885708,1.59931768991184,8.55590331293135,8.67405056543554,12.7600643883231,6.82652221551061,6.8708289697235,9.63959007239046,8.28258416166276,8.69911067862048]}},\"evals\":[],\"jsHooks\":[]}   "]
]
