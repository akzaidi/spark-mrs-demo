<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Scalable Machine Learning and Data Science with R and Spark</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This book is a guide for doing data analysis with Spark and R. It is organized as a collection of examples of data science problems analyzed with these two technologies, and tries to highlight programming paradigms that make the best of both worlds. It starts with a discussion of using R for data science and machine learning, and describes how to create functional programming pipelines that are data source invariant, easy to write, easy to debug, and easy to deploy! In order to scale out typical R workflows, we showcase how to use Spark and Microsoft R Server to bring your favorite R functions to large datasets.">
  <meta name="generator" content="bookdown 0.0.79 and GitBook 2.6.7">

  <meta property="og:title" content="Scalable Machine Learning and Data Science with R and Spark" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/scalemlcover.png" />
  <meta property="og:description" content="This book is a guide for doing data analysis with Spark and R. It is organized as a collection of examples of data science problems analyzed with these two technologies, and tries to highlight programming paradigms that make the best of both worlds. It starts with a discussion of using R for data science and machine learning, and describes how to create functional programming pipelines that are data source invariant, easy to write, easy to debug, and easy to deploy! In order to scale out typical R workflows, we showcase how to use Spark and Microsoft R Server to bring your favorite R functions to large datasets." />
  <meta name="github-repo" content="akzaidi/spark-mrs-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Scalable Machine Learning and Data Science with R and Spark" />
  
  <meta name="twitter:description" content="This book is a guide for doing data analysis with Spark and R. It is organized as a collection of examples of data science problems analyzed with these two technologies, and tries to highlight programming paradigms that make the best of both worlds. It starts with a discussion of using R for data science and machine learning, and describes how to create functional programming pipelines that are data source invariant, easy to write, easy to debug, and easy to deploy! In order to scale out typical R workflows, we showcase how to use Spark and Microsoft R Server to bring your favorite R functions to large datasets." />
  <meta name="twitter:image" content="images/scalemlcover.png" />

<meta name="author" content="Ali Zaidi">

<meta name="date" content="2016-07-10">


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="data-manipulation-with-sparkr.html">
<link rel="next" href="spark-backend-for-dplyr.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.6/htmlwidgets.js"></script>
<link href="libs/plotlyjs-1.10.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.10.1/plotly-latest.min.js"></script>
<script src="libs/plotly-binding-3.6.5/plotly.js"></script>
<script src="libs/d3-3.5.5/d3.min.js"></script>
<script src="libs/d3-grid-0.1.0/d3-grid.js"></script>
<script src="libs/dimple-2.1.6/dimple.min.js"></script>
<script src="libs/dimple-binding-0.1/dimple.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#scalable-machine-learning-and-data-science"><i class="fa fa-check"></i>Scalable Machine Learning and Data Science</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#useful-resources"><i class="fa fa-check"></i>Useful Resources</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#spark"><i class="fa fa-check"></i>Spark</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#microsoft-r-server"><i class="fa fa-check"></i>Microsoft R Server</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#azure-hdinsight"><i class="fa fa-check"></i>Azure HDInsight</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Overview</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#why-r"><i class="fa fa-check"></i><b>1.1</b> Why R?</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#microsoft-r-server-ftw"><i class="fa fa-check"></i><b>1.2</b> Microsoft R Server FTW</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#apache-spark"><i class="fa fa-check"></i><b>1.3</b> Apache Spark</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#sparkr"><i class="fa fa-check"></i><b>1.4</b> SparkR</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#azure-hdinsight-1"><i class="fa fa-check"></i><b>1.5</b> Azure HDInsight</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#prerequisites---what-youll-need"><i class="fa fa-check"></i><b>1.6</b> Prerequisites - What Youâ€™ll Need</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#data-used"><i class="fa fa-check"></i><b>1.7</b> Data Used</a></li>
<li class="chapter" data-level="1.8" data-path="introduction.html"><a href="introduction.html#versioning"><i class="fa fa-check"></i><b>1.8</b> Versioning</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="apache-spark-1.html"><a href="apache-spark-1.html"><i class="fa fa-check"></i><b>2</b> Apache Spark</a><ul>
<li class="chapter" data-level="2.1" data-path="apache-spark-1.html"><a href="apache-spark-1.html#lazyevalfp"><i class="fa fa-check"></i><b>2.1</b> Functional Programming and Lazy Evaluation</a></li>
<li class="chapter" data-level="2.2" data-path="apache-spark-1.html"><a href="apache-spark-1.html#distributed-programming-abstractions"><i class="fa fa-check"></i><b>2.2</b> Distributed Programming Abstractions</a></li>
<li class="chapter" data-level="2.3" data-path="apache-spark-1.html"><a href="apache-spark-1.html#rdds"><i class="fa fa-check"></i><b>2.3</b> RDDs</a><ul>
<li class="chapter" data-level="2.3.1" data-path="apache-spark-1.html"><a href="apache-spark-1.html#common-transformations-and-actions"><i class="fa fa-check"></i><b>2.3.1</b> Common Transformations and Actions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="apache-spark-1.html"><a href="apache-spark-1.html#dataframes"><i class="fa fa-check"></i><b>2.4</b> DataFrames</a></li>
<li class="chapter" data-level="2.5" data-path="apache-spark-1.html"><a href="apache-spark-1.html#datasets"><i class="fa fa-check"></i><b>2.5</b> Datasets</a></li>
<li class="chapter" data-level="2.6" data-path="apache-spark-1.html"><a href="apache-spark-1.html#mllib"><i class="fa fa-check"></i><b>2.6</b> MLlib</a></li>
<li class="chapter" data-level="2.7" data-path="apache-spark-1.html"><a href="apache-spark-1.html#spark-apis"><i class="fa fa-check"></i><b>2.7</b> Spark APIs</a><ul>
<li class="chapter" data-level="2.7.1" data-path="apache-spark-1.html"><a href="apache-spark-1.html#scala"><i class="fa fa-check"></i><b>2.7.1</b> Scala</a></li>
<li class="chapter" data-level="2.7.2" data-path="apache-spark-1.html"><a href="apache-spark-1.html#pyspark"><i class="fa fa-check"></i><b>2.7.2</b> <a href="https://spark.apache.org/docs/latest/api/python/index.html">PySpark</a></a></li>
<li class="chapter" data-level="2.7.3" data-path="apache-spark-1.html"><a href="apache-spark-1.html#sparkr-1"><i class="fa fa-check"></i><b>2.7.3</b> SparkR</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="rserver.html"><a href="rserver.html"><i class="fa fa-check"></i><b>3</b> R &amp; Microsoft R Server - todo</a><ul>
<li class="chapter" data-level="3.1" data-path="rserver.html"><a href="rserver.html#functional-programming-and-lazy-evaluation-in-r"><i class="fa fa-check"></i><b>3.1</b> Functional Programming and Lazy Evaluation in R</a></li>
<li class="chapter" data-level="3.2" data-path="rserver.html"><a href="rserver.html#pema"><i class="fa fa-check"></i><b>3.2</b> PEMA Algorithms and the RevoScaleR Package</a></li>
<li class="chapter" data-level="3.3" data-path="rserver.html"><a href="rserver.html#xdf"><i class="fa fa-check"></i><b>3.3</b> eXternal Data Frames (XDFs)</a></li>
<li class="chapter" data-level="3.4" data-path="rserver.html"><a href="rserver.html#computecontext"><i class="fa fa-check"></i><b>3.4</b> Compute Contexts</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><i class="fa fa-check"></i><b>4</b> Azure HDInsight â€“ Managed Hadoop in the Cloud - todo</a><ul>
<li class="chapter" data-level="4.1" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html#hdinsight-premium-spark-clusters-with-r-server"><i class="fa fa-check"></i><b>4.1</b> HDInsight Premium Spark Clusters with R Server</a></li>
<li class="chapter" data-level="4.2" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html#dashboards-for-management"><i class="fa fa-check"></i><b>4.2</b> Dashboards for Management</a></li>
<li class="chapter" data-level="4.3" data-path="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html"><a href="azure-hdinsight-managed-hadoop-in-the-cloud-todo.html#jupyter-and-rstudio-server"><i class="fa fa-check"></i><b>4.3</b> Jupyter and RStudio Server</a></li>
</ul></li>
<li class="part"><span><b>Provisioning and Ingesting Data</b></span></li>
<li class="chapter" data-level="5" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html"><i class="fa fa-check"></i><b>5</b> Provisioning Instructions</a><ul>
<li class="chapter" data-level="5.1" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html#provision-cluster-from-azure-portal"><i class="fa fa-check"></i><b>5.1</b> Provision Cluster from Azure Portal</a></li>
<li class="chapter" data-level="5.2" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html#installing-packages"><i class="fa fa-check"></i><b>5.2</b> Installing Packages</a><ul>
<li class="chapter" data-level="5.2.1" data-path="provisioning-instructions.html"><a href="provisioning-instructions.html#todo---install-packages-demo"><i class="fa fa-check"></i><b>5.2.1</b> todo - install packages demo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ingestion.html"><a href="ingestion.html"><i class="fa fa-check"></i><b>6</b> Ingesting Data into Azure Blob Storage - todo</a><ul>
<li class="chapter" data-level="6.1" data-path="ingestion.html"><a href="ingestion.html#azcopy"><i class="fa fa-check"></i><b>6.1</b> AzCopy</a></li>
<li class="chapter" data-level="6.2" data-path="ingestion.html"><a href="ingestion.html#azure-storage-explorer"><i class="fa fa-check"></i><b>6.2</b> Azure Storage Explorer</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="rprofile.html"><a href="rprofile.html"><i class="fa fa-check"></i><b>7</b> Setting Your R Profile</a></li>
<li class="part"><span><b>Data Manipulation and Data Aggregation</b></span></li>
<li class="chapter" data-level="8" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html"><i class="fa fa-check"></i><b>8</b> Starting Your Machine Learning Pipeline</a><ul>
<li class="chapter" data-level="8.1" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#finding-the-sparkr-library"><i class="fa fa-check"></i><b>8.1</b> Finding the SparkR Library</a></li>
<li class="chapter" data-level="8.2" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#creating-a-spark-context"><i class="fa fa-check"></i><b>8.2</b> Creating a Spark Context</a></li>
<li class="chapter" data-level="8.3" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#creating-dataframes"><i class="fa fa-check"></i><b>8.3</b> Creating DataFrames</a><ul>
<li class="chapter" data-level="8.3.1" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#from-local-r-data.frames"><i class="fa fa-check"></i><b>8.3.1</b> From Local R data.frames</a></li>
<li class="chapter" data-level="8.3.2" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#code-reusability-and-non-standard-evaluation"><i class="fa fa-check"></i><b>8.3.2</b> Code Reusability and Non-Standard Evaluation</a></li>
<li class="chapter" data-level="8.3.3" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#creating-dataframes-from-csv-files"><i class="fa fa-check"></i><b>8.3.3</b> Creating DataFrames from CSV Files</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="starting-your-machine-learning-pipeline.html"><a href="starting-your-machine-learning-pipeline.html#caching-dataframes"><i class="fa fa-check"></i><b>8.4</b> Caching DataFrames</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html"><i class="fa fa-check"></i><b>9</b> Exploratory Data Analysis with SparkR</a><ul>
<li class="chapter" data-level="9.1" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#sparkr-the-explorer"><i class="fa fa-check"></i><b>9.1</b> SparkR the Explorer</a></li>
<li class="chapter" data-level="9.2" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#doing-data-aggregations-with-sparkr-efficiently"><i class="fa fa-check"></i><b>9.2</b> Doing Data Aggregations with SparkR Efficiently</a></li>
<li class="chapter" data-level="9.3" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#from-spark-dataframes-to-local-dataframes"><i class="fa fa-check"></i><b>9.3</b> From Spark DataFrames to Local Dataframes</a></li>
<li class="chapter" data-level="9.4" data-path="exploratory-data-analysis-with-sparkr.html"><a href="exploratory-data-analysis-with-sparkr.html#plotting-results"><i class="fa fa-check"></i><b>9.4</b> Plotting Results</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html"><i class="fa fa-check"></i><b>10</b> Data Manipulation with SparkR</a><ul>
<li class="chapter" data-level="10.1" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#data-aggregations"><i class="fa fa-check"></i><b>10.1</b> Data Aggregations</a></li>
<li class="chapter" data-level="10.2" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#collecting-results-to-local-dataframes"><i class="fa fa-check"></i><b>10.2</b> Collecting Results to Local Dataframes</a></li>
<li class="chapter" data-level="10.3" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#dimple-bar-charts"><i class="fa fa-check"></i><b>10.3</b> Dimple Bar Charts</a></li>
<li class="chapter" data-level="10.4" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#merging-data"><i class="fa fa-check"></i><b>10.4</b> Merging Data</a></li>
<li class="chapter" data-level="10.5" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#exploring-credit-scores-for-mortgage-borrowers"><i class="fa fa-check"></i><b>10.5</b> Exploring Credit Scores for Mortgage Borrowers</a><ul>
<li class="chapter" data-level="10.5.1" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#ingesting-originations-data-into-spark-dataframes"><i class="fa fa-check"></i><b>10.5.1</b> Ingesting Originations Data into Spark DataFrames</a></li>
<li class="chapter" data-level="10.5.2" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#calculting-state-level-credit-attributes"><i class="fa fa-check"></i><b>10.5.2</b> Calculting State Level Credit Attributes</a></li>
<li class="chapter" data-level="10.5.3" data-path="data-manipulation-with-sparkr.html"><a href="data-manipulation-with-sparkr.html#credit-attribute-choropleths"><i class="fa fa-check"></i><b>10.5.3</b> Credit Attribute Choropleths</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Modeling and Prediction with Microsoft R Server</b></span></li>
<li class="chapter" data-level="11" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html"><i class="fa fa-check"></i><b>11</b> Modeling with Microsoft R Server</a><ul>
<li class="chapter" data-level="11.1" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#import-csv-to-xdf"><i class="fa fa-check"></i><b>11.1</b> Import CSV to XDF</a></li>
<li class="chapter" data-level="11.2" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#splitting-xdf-into-train-and-test-tests"><i class="fa fa-check"></i><b>11.2</b> Splitting XDF into Train and Test Tests</a></li>
<li class="chapter" data-level="11.3" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#training-binary-classification-models"><i class="fa fa-check"></i><b>11.3</b> Training Binary Classification Models</a><ul>
<li class="chapter" data-level="11.3.1" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#logistic-regression-models"><i class="fa fa-check"></i><b>11.3.1</b> Logistic Regression Models</a></li>
<li class="chapter" data-level="11.3.2" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#tree-and-ensemble-classifiers"><i class="fa fa-check"></i><b>11.3.2</b> Tree and Ensemble Classifiers</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="modeling-with-microsoft-r-server.html"><a href="modeling-with-microsoft-r-server.html#testing-models"><i class="fa fa-check"></i><b>11.4</b> Testing Models</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html"><i class="fa fa-check"></i><b>12</b> Spark Backend for dplyr</a><ul>
<li class="chapter" data-level="12.1" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html#sparkrapi"><i class="fa fa-check"></i><b>12.1</b> sparkrapi</a><ul>
<li class="chapter" data-level="12.1.1" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html#a-proper-dplyr-backend-for-spark-dataframes"><i class="fa fa-check"></i><b>12.1.1</b> A Proper <code>dplyr</code> backend for Spark DataFrames</a></li>
<li class="chapter" data-level="12.1.2" data-path="spark-backend-for-dplyr.html"><a href="spark-backend-for-dplyr.html#aggregating-data"><i class="fa fa-check"></i><b>12.1.2</b> Aggregating Data</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Scalable Machine Learning and Data Science with R and Spark</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modeling-with-microsoft-r-server" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Modeling with Microsoft R Server</h1>
<div id="import-csv-to-xdf" class="section level2">
<h2><span class="header-section-number">11.1</span> Import CSV to XDF</h2>
<p>To take full advantage of the PEMA algorithms provided by MRS, we will import the merged data, currently saved as csv in blob storage, into an xdf.</p>
<p>We first have some housekeeping items to take care. We need to specify the spark compute context for the <code>RevoScaleR</code> package to properly utlize the Spark cluster. Saving a text file to HDFS creates blocks of the data and saves them in separate directories, and also saves an additional directory entitled â€œ_SUCCESS&quot; to indicate the import operation was successful. We need to remove this file before importing to xdf, as it has no value for the final data.</p>
<p>Further, in order to make sure the MRS modeling functions respect the data types of the columns in our merged dataset, we need to provide it with some column metadata. This can be provided with the <code>colInfo</code> argument inside of <code>rxImport</code>.</p>
<p>Lastly, we need to provide MRS with pointers to the HDFS store we will be saving our XDF to.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rxOptions</span>(<span class="dt">fileSystem =</span> <span class="kw">RxHdfsFileSystem</span>(),
          <span class="dt">reportProgress =</span> <span class="dv">0</span>)

dataDir &lt;-<span class="st"> &quot;/user/RevoShare/alizaidi/&quot;</span>

if(<span class="kw">rxOptions</span>()$hdfsHost ==<span class="st"> &quot;default&quot;</span>) {
 fullDataDir &lt;-<span class="st"> </span>dataDir
} else {
 fullDataDir &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">rxOptions</span>()$hdfsHost, dataDir)
}  

computeContext &lt;-<span class="st"> </span><span class="kw">RxSpark</span>(<span class="dt">consoleOutput =</span> <span class="ot">TRUE</span>)

<span class="co"># there&#39;s a folder called SUCCESS_ that we need to delete manually</span>
file_to_delete &lt;-<span class="st"> </span><span class="kw">file.path</span>(data_dir, <span class="st">&quot;delayDataLarge&quot;</span>, <span class="st">&quot;JoinAirWeatherDelay&quot;</span>, <span class="st">&quot;_SUCCESS&quot;</span>)
delete_command &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;fs -rm&quot;</span>, file_to_delete)
<span class="kw">rxHadoopCommand</span>(delete_command)


colInfo &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="dt">ArrDel15 =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">Year =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;factor&quot;</span>),
  <span class="dt">Month =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;factor&quot;</span>),
  <span class="dt">DayofMonth =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;factor&quot;</span>),
  <span class="dt">DayOfWeek =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;factor&quot;</span>),
  <span class="dt">Carrier =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;factor&quot;</span>),
  <span class="dt">OriginAirportID =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;factor&quot;</span>),
  <span class="dt">DestAirportID =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;factor&quot;</span>),
  <span class="dt">RelativeHumidityOrigin =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">AltimeterOrigin =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">DryBulbCelsiusOrigin =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">WindSpeedOrigin =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">VisibilityOrigin =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">DewPointCelsiusOrigin =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">RelativeHumidityDest =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">AltimeterDest =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">DryBulbCelsiusDest =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">WindSpeedDest =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">VisibilityDest =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">DewPointCelsiusDest =</span> <span class="kw">list</span>(<span class="dt">type=</span><span class="st">&quot;numeric&quot;</span>),
  <span class="dt">CRSDepTime =</span> <span class="kw">list</span>(<span class="dt">type =</span> <span class="st">&quot;numeric&quot;</span>),
  <span class="dt">CRSArrTime =</span> <span class="kw">list</span>(<span class="dt">type =</span> <span class="st">&quot;numeric&quot;</span>),
  <span class="dt">DepDelay =</span> <span class="kw">list</span>(<span class="dt">type =</span> <span class="st">&quot;numeric&quot;</span>),
  <span class="dt">ArrDelay =</span> <span class="kw">list</span>(<span class="dt">type =</span> <span class="st">&quot;numeric&quot;</span>)
)

myNameNode &lt;-<span class="st"> &quot;default&quot;</span>
myPort &lt;-<span class="st"> </span><span class="dv">0</span>
hdfsFS &lt;-<span class="st"> </span><span class="kw">RxHdfsFileSystem</span>(<span class="dt">hostName =</span> myNameNode, 
                           <span class="dt">port =</span> myPort)

joined_txt &lt;-<span class="st"> </span><span class="kw">RxTextData</span>(<span class="kw">file.path</span>(data_dir, <span class="st">&quot;delayDataLarge&quot;</span>, <span class="st">&quot;JoinAirWeatherDelay&quot;</span>),
                           <span class="dt">colInfo =</span> colInfo,
                           <span class="dt">fileSystem =</span> hdfsFS)

dest_xdf &lt;-<span class="st"> </span><span class="kw">RxXdfData</span>(<span class="kw">file.path</span>(data_dir, <span class="st">&quot;delayDataLarge&quot;</span>, <span class="st">&quot;joinedAirWeatherXdf&quot;</span>),
                      <span class="dt">fileSystem =</span> hdfsFS)



<span class="kw">rxImport</span>(<span class="dt">inData =</span> joined_txt, dest_xdf, <span class="dt">overwrite =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>Now that we have imported our data to an XDF, we can get some information about the variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rxGetInfo</span>(<span class="kw">RxXdfData</span>(<span class="kw">file.path</span>(data_dir, <span class="st">&quot;delayDataLarge&quot;</span>, <span class="st">&quot;joinedAirWeatherXdf&quot;</span>),
                      <span class="dt">fileSystem =</span> hdfsFS), <span class="dt">getVarInfo =</span> T, <span class="dt">numRows =</span> <span class="dv">2</span>)</code></pre></div>
</div>
<div id="splitting-xdf-into-train-and-test-tests" class="section level2">
<h2><span class="header-section-number">11.2</span> Splitting XDF into Train and Test Tests</h2>
<p>Prior to estimating our predictive models, we need to split our dataset into a training set, which weâ€™ll use for estimation, and a test set that weâ€™ll use for validating our results.</p>
<p>Since we have time series data (data ordered by time), we will split our data by time. Weâ€™ll use the data prior to 2012 for training, and the data in 2012 for testing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trainDS &lt;-<span class="st"> </span><span class="kw">RxXdfData</span>( <span class="kw">file.path</span>(dataDir, <span class="st">&quot;finalDataTrain&quot;</span> ),
                      <span class="dt">fileSystem =</span> hdfsFS)

<span class="kw">rxDataStep</span>( <span class="dt">inData =</span> dest_xdf, <span class="dt">outFile =</span> trainDS,
            <span class="dt">rowSelection =</span> ( Year !=<span class="st"> </span><span class="dv">2012</span> ), <span class="dt">overwrite =</span> T )

testDS &lt;-<span class="st"> </span><span class="kw">RxXdfData</span>( <span class="kw">file.path</span>(dataDir, <span class="st">&quot;finalDataTest&quot;</span> ),
                     <span class="dt">fileSystem =</span> hdfsFS)

<span class="kw">rxDataStep</span>( <span class="dt">inData =</span> dest_xdf, <span class="dt">outFile =</span> testDS,
            <span class="dt">rowSelection =</span> ( Year ==<span class="st"> </span><span class="dv">2012</span> ), <span class="dt">overwrite =</span> T )</code></pre></div>
</div>
<div id="training-binary-classification-models" class="section level2">
<h2><span class="header-section-number">11.3</span> Training Binary Classification Models</h2>
<p>Now that we have our train and test sets, we can estimate our predictive model. Letâ€™s try to predict the probability that a flight will be delayed as a function of other variables.</p>
<div id="logistic-regression-models" class="section level3">
<h3><span class="header-section-number">11.3.1</span> Logistic Regression Models</h3>
<p>RevoScaleR provides a highly optimized logistic regression model based on the Iteratively Reweighted Least Squares (IRLS) algorithm, which can be called using the <code>rxLogit</code> function. The <code>rxLogit</code> function looks nearly identical to the standard logistic regression function provided by the <code>glm</code> function in the base <code>stats</code> package, taking a formula as itâ€™s first argument, and the data as itâ€™s second argument.</p>
<p>We create a handy function <code>make_formula</code> for creating formula objects based on the variables in the <code>all_vars</code> argument of the function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">make_formula &lt;-<span class="st"> </span>function(resp_var,
                         vars_exclude,
                         all_vars) {
  
  features &lt;-<span class="st"> </span>all_vars[!(all_vars %in%<span class="st"> </span><span class="kw">c</span>(resp_var, vars_exclude))]
  form &lt;-<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(resp_var, <span class="kw">paste0</span>(features, <span class="dt">collapse =</span> <span class="st">&quot; + &quot;</span>),
                           <span class="dt">sep  =</span> <span class="st">&quot; ~ &quot;</span>))
  
  <span class="kw">return</span>(form)
}

data_names &lt;-<span class="st"> </span><span class="kw">rxGetVarNames</span>(trainDS)

form &lt;-<span class="st"> </span><span class="kw">make_formula</span>(<span class="st">&quot;ArrDel15&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;DepDelay&quot;</span>, <span class="st">&quot;ArrDelay&quot;</span>), data_names)

<span class="kw">system.time</span>(logitModel &lt;-<span class="st"> </span><span class="kw">rxLogit</span>(form, <span class="dt">data =</span> trainDS))
 <span class="co">#   user  system elapsed </span>
 <span class="co"># 15.916  17.068 302.806 </span>

base::<span class="kw">summary</span>(logitModel)</code></pre></div>
</div>
<div id="tree-and-ensemble-classifiers" class="section level3">
<h3><span class="header-section-number">11.3.2</span> Tree and Ensemble Classifiers</h3>
<p>Training the logistic regression model on the full training set took about five minutes. Logistic regression models are frequently used for classification problems due to their interpability and extensibility. However, without adequeate feature engineering, logistic regression models tend to lack the expressiveness and predictive power of ensemble methods, such as boosted trees, or random forests.</p>
<p>Using the same methodology as above, we could estimate decision trees and decision forests (random forests) just as easily with the same formula:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system.time</span>(dTreeModel &lt;-<span class="st"> </span><span class="kw">rxDTree</span>(form, <span class="dt">data =</span> trainDS,
                                  <span class="dt">maxDepth =</span> <span class="dv">6</span>, <span class="dt">pruneCp =</span> <span class="st">&quot;auto&quot;</span>))
  <span class="co">#   user   system  elapsed </span>
  <span class="co"># 29.088   67.940 1265.633</span>

<span class="kw">save</span>(dTreeModel, <span class="dt">file =</span> <span class="st">&quot;dTreeModel.RData&quot;</span>)</code></pre></div>
</div>
</div>
<div id="testing-models" class="section level2">
<h2><span class="header-section-number">11.4</span> Testing Models</h2>
<p>Now that we have estimated our models, we can test them on the unseen dataset, which is the dataset from the year 2012. The <code>RevoScaleR</code> package is equipped with a predict function that allows you to score a <code>rx</code> model on a new dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;testModels.RData&quot;</span>)

treePredict &lt;-<span class="st"> </span><span class="kw">RxXdfData</span>(<span class="kw">file.path</span>(dataDir, <span class="st">&quot;treePredict&quot;</span>),
                         <span class="dt">fileSystem =</span> hdfsFS)

<span class="kw">system.time</span>(<span class="kw">rxPredict</span>(dTreeModel, <span class="dt">data =</span> testDS, <span class="dt">outData =</span> treePredict, 
                      <span class="dt">extraVarsToWrite =</span> <span class="kw">c</span>(<span class="st">&quot;ArrDel15&quot;</span>), <span class="dt">overwrite =</span> <span class="ot">TRUE</span>))


<span class="co"># user  system elapsed </span>
<span class="co"># 13.436   3.616 142.326</span>


<span class="co"># logitPredict &lt;- RxXdfData(file.path(dataDir, &quot;logitPredict&quot;),</span>
                          <span class="co"># fileSystem = hdfsFS)</span>

<span class="co"># rxPredict(logitModel, data = testDS, outData = logitPredict,</span>
          <span class="co"># extraVarsToWrite = c(&quot;ArrDel15&quot;),</span>
          <span class="co"># type = &#39;response&#39;, overwrite = TRUE)</span></code></pre></div>
<p>Using our predicted results, we can calculate the accuracy and recall of our model. A very simple way of viewing modelâ€™s accuracy is by way of a ROC curve, which plots the recall and accuracy. The metric from this chart, AUC, is a useful validation metric for viewing your moelsâ€™ accuracy.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-manipulation-with-sparkr.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="spark-backend-for-dplyr.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
